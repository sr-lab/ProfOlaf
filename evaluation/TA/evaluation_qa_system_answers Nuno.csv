title,summary,"Factual Accuracy / Faithfulness: “The summary’s claims are correct and do not invent facts or misattribute statements.”
","Coverage / Salience: The summary captures the article’s main points and most important details, not just trivia.",Coherence / Structure: The summary is well-organized and easy to follow; ideas flow logically,Conciseness: The summary conveys the key points with minimal redundancy or fluff
PatchView__Multi-modality_detection_of_security_patches.pdf,"PatchView tackles the problem of detecting concealed security patches in Git commits by using a multi-modal approach. It leverages three data modalities: developer behavioral time-series data from the repository, commit messages, and code patches, each processed by its own sub-model (behavioral, text, and code). The outputs of these sub-models are fused in an end-to-end multi-modal model to produce the final prediction, making the system language-agnostic and language-oblivious. The dataset combines CVE-linked security patches (2015–2021) with non-security patches, totaling about 27,130 commits from 1,679 repositories, and includes rich behavioral features. PatchView achieves state-of-the-art performance with 94.52% accuracy and a 95.12% F1-score in detecting undisclosed security patches, with ablation studies showing the importance of end-to-end training and multi-modal fusion. The authors also make their data and code publicly available (GitHub and Kaggle) to support reproducibility and future work.",5,4,5,5
SUMLLAMA__Efficient_Contrastive_Representations_and_Fine-Tuned_Adapters_for_Bug_Report_Summarization.pdf,"SumLLaMA is a large-language-model–based approach for automated bug report summarization that addresses the challenges of capturing contextual relationships among bug report components and overfitting on small datasets. It uses a contrastive learning pre-training objective on CodeLLaMA, generating positive samples by masking different components and employing a Siamese network with a contrastive loss to learn sequence-level contextual representations. For fine-tuning, SumLLaMA adopts parameter-efficient tuning (LoRA), training a small summarization adapter (~1/1500 of the parameters) while keeping the base model fixed, and trains to generate the Summary from four remaining components. The model is pre-trained on a Bugzilla corpus (~270k reports) with an 80/10/10 train/val/test split and evaluated against four strong baselines and several LLM-based approaches using ROUGE and composite BLEU. Results show SumLLaMA achieves state-of-the-art performance, outperforming baselines by up to 24.01 ROUGE-L points and exceeding LLM-based methods by 2.9–3.1 ROUGE points, with ablations confirming the benefits of contrastive pre-training and PEFT. The authors note threats to validity due to Bugzilla-only data and suggest transfer learning and dataset adaptation for broader generalization in future work.",5,4,4,5
Evaluating_Large_Language_Model_Application_Impacts_on_Evasive_Spectre_Attack_Detection_.pdf,"This article investigates how applying large language models (DeepSeek, Kimi, Doubao) to text, image, and code tasks affects the detection of evasive Spectre attacks. It builds a realistic dataset by collecting hardware performance counters (HPCs) during attack and non-attack runs and uses DBSCAN to prune noise and select representative samples for testing. Three models (MLP, Random Forest, and RNN) are compared for attack detection, with Random Forest delivering the best overall performance and proposed as the primary detector. The results show the attack detection success rates follow the order code > text > image, with code tasks providing the strongest signals; this pattern persists across tested hardware platforms. Cross-architecture experiments on RTX 2080 Ti and RTX 3060 reveal that architectural differences shift HPC patterns, causing code-task detection to decline while text and image detection rise on the 3060 due to cache hierarchy changes. The study notes limitations (limited data types and attack types) and suggests defense directions such as adaptive cache management and hardware-based mitigations.",5,4,4,5
Can_llms_replace_manual_annotation_of_software_engineering_artifacts_.pdf,"The paper investigates whether large language models (LLMs) can safely replace costly human annotations in software engineering artifact evaluations. It applies six state-of-the-art LLMs to ten annotation tasks across five datasets, comparing LLM outputs to human ratings using Krippendorff’s alpha for human-human, human-model, and model-model agreement. The authors find that, for many tasks, human-model agreement is similar to human-human agreement and model-model agreement correlates with human-model agreement, suggesting when LLMs are suitable. Some tasks (e.g., code summarization and semantic similarity) show high agreement across all pairings, while others (e.g., causality and static analysis warnings) exhibit low model-model agreement, making them risky for substitution. They propose a two-step approach: first assess model-model agreement on few-shot prompts; if above a threshold, replace one human rating per sample (with selective replacements based on model confidence), yielding substantial reductions in human effort (up to about 33%–33% of overall effort for certain tasks, and up to 50–100% for some ratings). Overall, the study demonstrates a promising route toward mixed human-LLM evaluations, indicating potential to augment—rather than entirely replace—human annotators in many cases, with task- and sample-dependent considerations.",5,5,5,5
Retrieval_Augmented_Generation_Fine-Tuned_LLM_Model_for_Code_Recommendations_to_Mitigate_Lock_Contention.pdf,"- The article tackles lock-contention performance faults in Java, noting that existing tools identify issues but don’t provide actionable refactoring recommendations. 
- It proposes a Retrieval Augmented Generation (RAG) framework that combines a fine-tuned LLama model (via LoRA PEFT) with a domain knowledge base to generate context-grounded refactored code recommendations. 
- A Java lock contention code-smell dataset (six types) with 575 snippets was created and used to fine-tune LLama 3.1-8B-instruct in a supervised setting (1 training epoch, small hyperparameters). 
- The RAG pipeline uses BM25 and vector embeddings (ChromaDB) to retrieve relevant documents, then augments the LLM prompt to ground outputs and reduce hallucinations. 
- Evaluation shows the RAG+Fine-Tuned LLama achieves about BLEU 0.92 and CodeBLEU 0.90 (Pass@K ~0.52/0.80), outperforming baseline LLama (BLEU 0.31, CodeBLEU 0.24) and the fine-tuned model alone (BLEU 0.75, CodeBLEU 0.68). 
- External testing on Apache Kafka smells yielded substantial fix rates across several code-smell types, indicating practical effectiveness. 
- The paper concludes that RAG-grounded fine-tuning improves accuracy and reduces hallucinations, but notes limitations (reliance on external RAG content and no runtime performance verification) and suggests future work on multi-LLM ensembles and agent-based RAG.",5,4,2,4
Benchmarking_Long-Context_Language_Models_on_Long_Code_Understanding.pdf,"- The authors introduce LONG CODEU, a benchmark to rigorously evaluate long-context language models on long code understanding, covering four aspects (code unit perception, intra-code unit understanding, inter-code unit relations, and long code documentation) across eight tasks. 
- The benchmark uses extra-long, real-world Python code contexts up to 128K tokens from 116 GitHub repositories, with measures to reduce data contamination and preserve realistic dependencies. 
- Nine LCLMs are evaluated, including six general models (e.g., GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Flash, Mistral-v0.3, Phi-3.5, DeepSeek-V2.5) and three code models (DeepSeek-Coder-V2, Qwen2.5-Coder, CodeLlama). 
- Results show significant performance gaps among models, and a dramatic drop when long code length exceeds 32K tokens, far below the claimed 128K–1M context windows; inter-code unit relation understanding is the most challenging task. 
- The authors propose automatic evaluation metrics (EM-R/EM-P, LCS-R/LCS-P, CB-R/CB-P, and BLEU for documentation) that correlate well with human judgments (Kendall tau around 0.75). 
- Practical guidance is offered: smaller models can handle code-unit perception and understanding for lengths under ~16K, while long documentation benefits from GPT-4o or Gemini-1.5-Flash; inter-unit relation tasks require the strongest models, though performance remains limited at very long contexts. 
- Overall, LONG CODEU provides a comprehensive framework and reveals current LCLMs’ limits in long-code understanding, highlighting areas for model optimization and future multilingual and longer-context research.",5,5,2,3
Generating_vulnerability_security_fixes_with_Code_Language_Models.pdf,"PatchLM is a Code Language Model fine-tuned on code blocks from CVE-linked commit hunks to automatically generate security patches across multiple programming languages. It builds on pre-trained CodeT5 and CodeLlama models, fine-tuning them with vulnerability-fix pairs from the FixMe and RepairLlama datasets; CodeLlama is quantized to 4-bit for efficiency. The authors use structured prompt engineering to guide patch generation and evaluate results with CodeBLEU and ROUGE metrics, incorporating syntax, data flow, AST, and token-matching aspects. Experimental results show PatchLM outperforms the baselines CodeT5 and CodeLlama, achieving improvements up to 48.35% in CodeBLEU and 28.9% in ROUGE on vulnerability patches. The datasets comprise about 1495 vulnerability-fix pairs at the hunk level in FixMe and approximately 65,000 pairs in RepairLlama, enabling cross-language evaluation. The study demonstrates PatchLM’s practicality for automated vulnerability repair and suggests it can assist security analysts, while outlining future directions and limitations.",5,3,4,4
Enhancing_the_code_debugging_ability_of_llms_via_communicative_agent_based_data_refinement.pdf,"- The paper introduces DEBUG EVAL, a multilingual, multi-task benchmark to evaluate LLM code debugging across four tasks (Bug Localization, Bug Identification, Code Review, Code Repair) in Python, C++, and Java, using real user bugs and GPT-4 generated bugs.  
- It also proposes MASTER, a Communicative Agent-Based Data Refinement framework with three roles—Code Quizzer, Code Learner, and Code Teacher—that synthesize high-quality SFT data and provide Chain-of-Thought solutions to guide learning.  
- In zero-shot experiments with 13 LLMs, they find that 70B+ models generally perform better than 7B models, and that NeuDebugger variants can achieve strong results when trained with MASTER-synthesized data.  
- The authors observe that Chain-of-Thought (CoT) helps the Code Localization, Bug Identification, and Code Review tasks but can hurt Code Repair due to added noise and disruption of code structure.  
- They show that MASTER data refinement significantly boosts 7B-scale models, with NeuDebugger-DS-6.7B and NeuDebugger-Llama3-8B attaining notable improvements (about 27.7% and 4.1%, respectively) when fine-tuned on MASTER-generated data.  
- All data and code are released publicly (GitHub), enabling replication and further research in code debugging for LLMs.",5,3,4,4
PATCH__Empowering_Large_Language_Model_with_Programmer-Intent_Guidance_and_Collaborative-Behavior_Simulation_for_Automatic_Bug_Fixing.pdf,"This article presents PATCH, a stage-wise framework that enhances large language models (LLMs) for automatic bug fixing by simulating programmer collaboration and using richer inputs. It argues that existing LLM approaches struggle because they treat bug fixing as a single-step task and rely only on buggy code, lacking repository-level context and programmer intent. PATCH decomposes bug fixing into four interactive stages—bug reporting, bug diagnosis, patch generation, and patch verification—employing three ChatGPT agents (tester, developer, reviewer) to mirror real-world debugging roles. It augments the buggy snippet with dependence context from class and repository levels and incorporates programmer intent via commit messages, plus retrieved similar bug-fixing demonstrations to guide generation. Patch generation is iterative, with feedback from the reviewer guiding refinements until the patch meets the desired goal. Evaluations on the BFP benchmark show PATCH outperforms state-of-the-art LLM approaches, and the authors release replicate artifacts publicly to support reproducibility.",5,4,5,4
Hierarchical_repository-level_code_summarization_for_business_applications_using_local_llms.pdf,"- The authors propose a two-step hierarchical approach for repository-level code summarization tailored to business applications, aiming to handle large codebases effectively. 
- They segment Java files into cohesive units (functions, variables, constructors, enums, interfaces) using an AST, summarize each unit with local LLMs, and then aggregate these into file-level and package-level summaries. 
- To ground the summaries in business context, they use domain- and problem-context prompts focused on the telecom BSS domain, improving domain relevance and alignment with business intent. 
- The evaluation uses a small public GitHub repository mirroring their telecom BSS codebase, with local models (Llama-3, Starchat2, Codestral) for segment-level summarization and GPT-4 to generate ground-truth references; for long files, they employ Llama-3.2 with a 128K context window. 
- Results indicate that the hierarchical, segment-level approach improves coverage, and grounding further enhances domain specificity and relevance compared to direct file-level summarization. 
- The study concludes that the approach yields concise, coherent, and domain-aware summaries and suggests future work on agentic/self-reflective models, broader domains, and multi-modal data integration.",5,4,4,4
When_Fine-Tuning_LLMs_Meets_Data_Privacy__An_Empirical_Study_of_Federated_Learning_in_LLM-Based_Program_Repair.pdf,"- The paper investigates privacy-preserving collaboration for LLM-based automated program repair (APR) by applying federated learning to fine-tune models on private, decentralized code data. 
- It uses a private industrial dataset (TutorCode) for fine-tuning and the EvalRepair-Java benchmark for evaluation, comparing federated fine-tuning against centralized learning and baselines. 
- The framework employs parameter-efficient methods (QLoRA/LoRA) with 4-bit NF4 quantization, sharing only adapters and aggregating them via FedAvg to protect data privacy and reduce communication costs. 
- The study explores data heterogeneity (Non-IID feature skew in code) and multiple federated algorithms, including client-side, server-side, and personalized learning approaches. 
- Results show federated fine-tuning can significantly boost program repair performance (up to 16.57% Top@10 and 18.44% Pass@10), approaching centralized fine-tuning performance, with code heterogeneity having negligible negative impact and sometimes yielding substantial gains (18.41% Top@10, 21.46% Pass@10). 
- Among federated algorithms, FedAvg provides the best overall performance, personalized learning remains challenging for LLM fine-tuning, and the authors release an open artifact to support replication and practical adoption.",5,4,4,3
"Ai-powered,_but_power-hungry__energy_efficiency_of_llm-generated_code.pdf","The paper evaluates energy efficiency and performance of LLM-generated code across Python, Java, and C++, on macOS and Ubuntu, using GitHub Copilot, GPT-4o, and OpenAI o1-mini, over 53 hard LeetCode problems with 159 human baseline solutions and 477 LLM-generated solutions. Energy, runtime, and correctness are measured by synchronized runs using powermetrics and perf, with multiple repetitions to ensure reliable data. The key findings show LLMs perform best in Python, achieving higher pass@1 accuracy and often comparable or better energy usage than human-written baselines, while Java and especially C++ results generally show increased energy consumption. Among models, o1-mini improves accuracy across tasks but tends to consume more energy than GPT-4o and Copilot; Copilot is typically the most energy-efficient for Python and Java, whereas GPT-4o shines for C++. Algorithmic techniques and data structures matter: strings and trees are typically efficient and accurate, while sorting, graphs, and greedy approaches pose more challenges and higher energy demands, with recursion also increasing energy in some languages. The energy results are highly correlated across platforms, indicating portability of findings, and the authors provide a replication package to enable further study and emphasize that energy efficiency should be a consideration in AI-assisted code generation.",5,3,5,4
Leveraging_Generative_AI_to_Enhance_Automated_Vulnerability_Scoring.pdf,"The paper addresses automated vulnerability scoring by leveraging Generative AI to improve the quality of vulnerability descriptions, which strongly affects ML accuracy in predicting CVSS base metrics. It proposes a novel pipeline (FNT-BERT-CNN) that first uses a fine-tuned GPT-3.5Turbo to generate template-adherent vulnerability descriptions, then applies a fine-tuned BERT tokenizer per metric followed by a CNN classifier to predict each CVSS base metric and compute the overall severity. The authors create a template-based synthetic dataset from the original NVD descriptions (6,370 items) by fine-tuning GPT-3.5Turbo on a small set of CVE-template samples. Experimental results show that FNT-BERT-CNN outperforms baseline models (BERT-LSTM, BERT-TextRNN-ATT, BERT-Text-CNN) on both original and template-generated descriptions, with near-perfect accuracies on many metrics and strong precision across parameters. They find that template-based (GPT-generated) descriptions generally improve performance, though some metrics on original data are better for certain models, indicating the model-architecture and data quality both influence results. The paper concludes that standardizing descriptions with Generative AI can significantly boost vulnerability classification accuracy, while noting limitations such as data quality, computational requirements, and dependence on templates, and suggests future work on richer templates and broader parameter coverage.",5,4,5,5
Bridging_HCI_and_AI_Research_for_the_Evaluation_of_Conversational_SE_Assistants.pdf,"The paper argues that evaluating LLM-based conversational software engineering (SE) assistants requires automatic, human-centered methods due to the cost and scalability limits of traditional user studies. It proposes bridging HCI and AI by combining simulated users (for realistic, diverse, multi-turn interactions and qualitative feedback) with LLM-as-a-Judge (for scalable quantitative metrics). The authors outline four requirements for automatic evaluation (realistic conversations, diversity, quantitative metrics, and qualitative insights) and discuss how simulated users and LLM-as-a-Judge can address them. Their method involves creating representative personas, generating simulated-user interactions with the assistant per persona, collecting qualitative feedback, and then using LLM-as-a-Judge to produce quantitative assessments, all to complement—not replace—human studies. They acknowledge challenges such as creating representative personas, mitigating biases, and handling context with retrieval-augmented generation, plus the influence of design choices like prompts and temperature. The authors conclude that the combined approach can expand the scale, scope, and frequency of evaluation while remaining aligned with human judgments, though it requires validation against real user studies.",5,5,5,5
Automated_Vulnerability_Score_Prediction_through_Lightweight_Generative_AI.pdf,"The article presents an architecture for automated vulnerability scoring that combines generative AI with lightweight transformers to deliver an efficient CVSS predictor. It uses a two-phase approach: first, LLMs (GPT-3.5-Turbo, Claude3 Haiku, Claude1.2 Instant) augment NVD vulnerability descriptions to strictly follow the MITRE CVE template, improving data quality and robustness for modeling; second, a suite of fine-tuned, lightweight BERT models (TinyBERT, DistilBERT, BERTsmall, MediumBERT, BERT) predict CVSS base metrics on the augmented data, with per-metric training via a probabilistic learning approach. The study evaluates on 137,546 original descriptions and three LLM-generated augmented datasets, assessing accuracy, precision, recall, F1, Cohen’s Kappa, MAE, and MSE across metrics such as AV, AC, PR, UI, S, C, I, and A. Results indicate GPT-3.5-Turbo provides the strongest descriptive quality, Claude3 Haiku offers higher recall, and BERT variants generally outperform DistilBERT and TinyBERT, with BERT and BERTsmall achieving strong overall accuracy. The proposed pipeline achieves a favorable accuracy-time trade-off, performing well against state-of-the-art approaches that use many data sources while relying on a single public dataset and a lightweight model stack. The authors claim novelty in addressing out-of-template vulnerability descriptions with LLMs and highlight a scalable, data-efficient workflow for automated vulnerability scoring.",5,4,4,3
Learning_Game-Playing_Agents_with_Generative_Code_Optimization.pdf,"- The article presents a generative optimization framework where Atari game-playing policies are coded as modular Python programs and refined by large language models via the Trace framework. 
- Policies consist of trainable functions annotated with traces, enabling execution-grounded optimization and human-interpretable behavior, with object-centric representations used for the game state. 
- An LLM optimizer (OptoPrime) iteratively updates the policy code using execution traces and natural-language feedback, treating the policy itself as the object of optimization. 
- To tackle long-horizon, sparse-reward tasks, the approach uses staged, game-aware feedback and longer evaluation rollouts to improve credit assignment. 
- Empirically, the method achieves performance competitive with open-source deep RL baselines on Pong, Breakout, and Space Invaders while requiring significantly less training time and environment interactions. 
- Limitations include occasional unstable edits from the LLM and sensitivity to prompts and context window constraints, but the work demonstrates the potential of programmatic policy representations for efficient, interpretable, long-horizon agents.",5,4,4,5
Autopresent__Designing_structured_visuals_from_scratch.pdf,"Autopresent introduces SLIDES BENCH, the first benchmark for NL-to-slide generation, with 7k training and 585 test examples drawn from 310 slide decks across 10 domains, plus reference-based and reference-free evaluation schemes. The authors advocate generating slides via program synthesis: an NL instruction is converted into a Python program (using python-pptx), with a SLIDES LIB that provides modular functions to reduce program length and improve controllability. They present AUTOPRESENT, an 8B open-source model fine-tuned on SLIDES BENCH to produce editable PPTX slides, and show that iterative refinement can further improve results. Experiments reveal that end-to-end image-generation approaches struggle to produce truly structured slides, whereas code-generation methods with SLIDES LIB—especially AUTOPRESENT—achieve quality comparable to GPT-4o, with small models lagging far behind. Evaluation combines reference-based and reference-free metrics along with executability to assess both content accuracy and design quality. The work acknowledges limitations (focus on single slides, not full decks) and outlines future directions like interactive deck generation and animations, while releasing code, data, and demos publicly.",5,4,3,4
Correctness_Assessment_of_Code_Generated_by_Large_Language_Models_Using_Internal_Representations.pdf,"OPENIA is a white-box framework that uses in-process internal representations from code LLMs during generation to assess the correctness of the produced code. It extracts hidden states from the final generated token across layers and trains a lightweight probing classifier to predict whether the code passes predefined tests. In experiments on HumanEval, MBPP, and DevEval with several open-source code LLMs, OPENIA consistently outperforms post-hoc classifiers and LLM-as-a-judge baselines, delivering up to 2x higher accuracy in standalone tasks and up to 3x higher F1 in repository-level tasks. The study shows that the last-layer representations of the final token provide strong signals, and that shorter code is easier to assess while longer code can benefit from richer context. OPENIA enables early, in-process correctness checks and supports correctness-guided code generation (CG2), offering potential reductions in debugging costs and improvements in QA for AI-assisted coding. However, it yields binary correctness without error localization, and some dynamic issues (e.g., index-out-of-range) remain challenging; results may also depend on language and task. Overall, the work demonstrates that internal signals in LLMs encode reliability information that can be leveraged for proactive, robust code quality assurance.",5,5,5,5
Leveraging_Human_Insights_for_Enhanced_LLM-based_Code_Repair.pdf,"The article proposes a framework to improve LLM-based code repair by injecting human-inspired mechanisms: mining commit histories for reusable fix patterns, using dynamic feedback loops with automated tests and static analysis plus optional human guidance, and reasoning over historical context with structured representations. It leverages multi-view code interpretations (AST, CFG, PDG) embedded into a Code Property Graph (CPG) to capture structural and semantic information, storing repair experiences as vectors in a knowledge base V that link code structures to patches and outcomes. For a new buggy code, the system retrieves the most similar past experiences based on the CPG embedding, builds a temporary state graph from structural relations, and uses structure-guided reasoning to derive an initial patch p0 inspired by historical fixes. An iterative refinement loop then updates patches using automated feedback f(p) and optional human critiques h, stopping when the patch passes criteria or after a maximum number of iterations. The authors plan evaluation on benchmarks like SVEN and PrimeVul, focusing on correctness, quality, and efficiency, with future work including empirical validation, broader bug types, dynamic analysis feedback, and scalability improvements. Overall, the approach aims to combine human-like reasoning, historical context, and iterative refinement to produce higher-quality patches more efficiently.",4,4,4,4
Leveraging_Open-Source_LLMs_for_Zero-Shot_Vulnerability_Detection__A_Comparative_Analysis.pdf,"- The article evaluates open-source LLMs for zero-shot vulnerability detection in C/C++ and compares them with traditional SAST tools and deep-learning methods using the MVFSC dataset and a newly introduced MVFSC Test Set of real-world vulnerabilities to test generalization. 
- Three open LLMs (CodeGeex 9B, QwenCoder 2.5 14B, CodeLlama 34B) are prompted in a zero-shot setup to classify code as vulnerable or not, without any task-specific fine-tuning. 
- Deep learning models (VulBERTa-MLP/TextCNN and VulDetectionGNN) are fine-tuned on MVFSC, and the MVFSC Test Set is used to assess generalization to unseen, realistic cases. 
- Results show traditional SAST tools perform poorly on the test set; DL models struggle to generalize, though graph-based approaches like VulDetectionGNN fare relatively better, and larger LLMs (CodeLlama 34B) provide improved recall and F1 in zero-shot. 
- The study concludes vulnerability detection remains intrinsically challenging in realistic contexts, highlighting the need for robust benchmarks and hybrid approaches that combine graph-based representations, DL, and LLM prompting, along with better-quality data and training strategies. 
- Their contributions include a systematic comparison across approaches, zero-shot LLM evaluation, and the introduction of a real-world MVFSC Test Set for benchmarking.",4,4,4,4
Fea-bench__A_benchmark_for_evaluating_repository-level_code_generation_for_feature_implementation.pdf,"FEA-Bench is a new benchmark designed to evaluate large language models on repository-level incremental code development, specifically feature implementation by adding new components, with unit tests, drawn from 83 Python GitHub repos (1,401 task instances). Each task includes a feature request, signatures of new components, environment setup, a patch, and unit tests, requiring models to both generate code for new components and edit existing repository code. Tasks are filtered to ensure true feature-implementation aims (new components occupy a substantial portion of edits) and are validated by intent classification and test success after applying the gold patch. Compared with SWE-bench, FEA-Bench tasks are more complex, longer, and emphasize introducing new functionality rather than bug fixes. Experiments show that larger models and advanced bases outperform smaller ones, with Oracle retrieval generally superior to BM25, and natural edits (as opposed to direct patch generation) yielding higher success rates; however, the best-resolved rate is only about 10%, indicating substantial challenges in repository-level feature development. The study acknowledges limitations (Python-only data, data quality constraints, high compute cost, single-round evaluation) and calls for improved retrieval, editing formats, and broader model support, with data and code to be publicly released.",5,4,5,5
Towards_detecting_prompt_knowledge_gaps_for_improved_llm-guided_issue_resolution.pdf,"The paper analyzes 433 developer-ChatGPT GitHub issue conversations to understand how prompt knowledge gaps and conversation styles affect issue resolution. It identifies four prompt gaps (Missing Context, Missing Specification, Unclear Instructions, Multiple Context) and seven conversation styles, refined to six styles plus Directive Prompting, with Missing Context being the most common gap. Open issues show more gaps (44.6%) than closed issues (12.6%), and gaps persist across styles, though closed issues tend to progress more with added context and iterative prompting. The authors propose three heuristics—Specificity, Contextual Richness, and Clarity—based on textual and code-related metrics to automatically measure prompt gaps, and they demonstrate their association with successful resolution via logistic regression. They also provide a lightweight browser-extension prototype that flags prompt gaps and offers structured templates, achieving about 62% cross-validated accuracy and highlighting features like misspellings, readability, mean code snippet size, and entailment as influential. Overall, the work contributes a labeled dataset and taxonomies, validates practical heuristics for gap detection, and points to tooling that can help developers craft clearer, more context-rich prompts to improve LLM-guided issue resolution.",5,5,4,4
Measuring_and_Improving_the_Efficiency_of_Python_Code_Generated_by_LLMs_Using_CoT_Prompting_and_Fine-Tuning.pdf,"The study evaluates the efficiency of Python code generated by three OpenAI LLMs (GPT-4o-Mini, GPT-3.5-Turbo, GPT-4-Turbo) on greedy problems from the EffiBench dataset, running experiments across multiple Google Vertex AI VM configurations and measuring average execution time (AET), average memory usage (AMU), and average max memory usage (AMMU) while checking correctness. It compares two prompting strategies (Standard vs Chain-of-Thought, CoT) and also investigates the effect of a fixed seed, plus a fine-tuning experiment on GPT-4o-Mini using a 243-problem subset. The results show CoT prompting improves efficiency for GPT-4o-Mini and GPT-3.5-Turbo (lower AET and memory) without sacrificing correctness, but GPT-4-Turbo either gains little or reduces correctness with CoT. The fixed seed generally helps reduce memory and time in many configurations, though not universally. Fine-tuning GPT-4o-Mini unexpectedly degrades both accuracy and efficiency, suggesting fine-tuning may be detrimental for this task. The authors conclude that pairing high-CPU VM configurations with GPT-4o-Mini and CoT prompting yields the most efficient and accurate LLM-generated Python code for compute-intensive scenarios.",5,4,4,5
SelfCodeAlign__Self-Alignment_for_Code_Generation.pdf,"SelfCodeAlign is a fully transparent, permissive pipeline for self-aligning code LLMs without human annotation or distillation. It begins by collecting high-quality seed Python functions from The Stack V1, extracting coding concepts, and using the base model to self-generate diverse tasks through in-context learning (Self-OSS-Instruct). For each instruction, the model then produces multiple responses paired with test cases, which are executed in a sandbox to filter out failures and retain only passing instruction–response pairs for fine-tuning. Trained on a 74k synthetic dataset, CodeQwen1.5-7B with SelfCodeAlign achieves 67.1 pass@1 on HumanEval+, outperforming larger models and prior distillation-based methods. The approach outperforms GPT-3.5/Turbo- and GPT-4o-based distillation methods (OSS-Instruct, Evol-Instruct) and direct distillation, and generalizes across model sizes from 3B to 33B. Ablation studies show the importance of seed quality, concept generation, and execution filtering, with findings that models learn best from data in their own distribution rather than from a stronger but shifted teacher. The authors also release the generated datasets publicly and demonstrate cross-model effectiveness, including contributing to StarCoder2-Instruct.",5,5,5,4
Coast__Enhancing_the_code_debugging_ability_of_llms_through_communicative_agent_based_data_synthesis.pdf,"This paper presents DEBUG EVAL, a comprehensive benchmark for evaluating LLM-driven code debugging across four tasks—Bug Localization, Bug Identification, Code Repair, and Code Recognition—using buggy code from humans and GPT-4 in Python, C++, and Java. It finds that 7B-scale models consistently underperform larger models, highlighting challenges in mastering code semantics and multi-stage debugging. To improve data quality for fine-tuning, it introduces COAST (Communicative Agent-based data Synthesis), a three-agent framework with Code Quizzer, Code Learner, and Code Teacher that generates, critiques, and explains debugging problems to produce high-quality SFT data. Experiments show COAST-synthesized data substantially boost 7B-model debugging performance, enabling them to reach roughly GPT-3.5 levels, with NeuDebugger variants achieving the best overall results. The study also notes that Code Teacher’s chain-of-thought aids localization/identification/recognition but can hurt repair due to added noise, and that synthesized data benefits code-focused LLMs more than general ones. Limitations include reliance on the quality of the agents, and the authors provide open data and code to spur future improvements.",5,4,5,5
MORepair__Teaching_LLMs_to_Repair_Code_via_Multi-Objective_Fine-Tuning.pdf,"- The paper addresses limitations of prior LLM fine-tuning for program repair, which focus too much on syntax, require large datasets, and incur high compute costs. 
- It introduces MORepair, a multi-objective fine-tuning framework that trains models to both generate repaired code and explain the repair logic in natural language, teaching the model the underlying reasoning and enabling language-independent repair. 
- They assemble TutorLLMCode and use GPT-4 to generate detailed, structured repair guidance, augmenting human data, which allows effective learning from relatively small datasets. 
- The approach uses QLoRA adapters with LoRA and NEFTune, trains on TutorLLMCode, and evaluates on EvalRepair-C++/EvalRepair-Java plus repo benchmarks D4J-Repair and SWE-Bench, with a 2048-token context window. 
- Results show MORepair improves Top-1/5/10 repair performance by 11.4%–56.0% across open-source LLMs, e.g., CodeLlama-13B-instruct gains of 18.8% (C++) and 11.4% (Java), outperforming StdFT, Fine-tune-CoT, and RepairLLaMA. 
- Ablation studies indicate that jointly optimizing both objectives is superior, LLM-generated guidance provides richer repair reasoning than human-written guidance, and MORepair narrows the gap between small and large models; the authors also publish benchmarks and code.",5,5,3,5
Template-guided_program_repair_in_the_era_of_large_language_models.pdf,"The article proposes Neural Template Repair (NTR), a two-stage framework that combines template-based repair with large-language-model (LLM) code generation to improve automated program repair (APR). It splits the task into template selection (a multiclass classification problem) and patch generation (an NMT-style generation guided by the chosen templates), and introduces a special OtherTemplate to cover beyond-template and multi-hunk repairs. Template selection is trained with CodeT5-220M, using bug-fix pairs from the Transfer dataset, while patch generation uses billion-scale LLMs (StarCoder-15B or CodeLlama-70B) fine-tuned with an NMT workflow and a committable input format that includes bug code, the template, and the fixed code. Inference employs template prioritization and iterative patch generation to explore the patch space efficiently under memory constraints, rather than relying on very large beam sizes. Empirical results on Defects4J V1.2 and HumanEval-Java show that NTR substantially outperforms baselines, with StarCoder-based NTR achieving 128/129 fixes and CodeLlama-based NTR achieving 139/136 fixes, beating top APR tools and pure LLM baselines by notable margins. Ablation studies indicate the benefits of the two-stage design, the presence of OtherTemplate, and the ability to handle multi-hunk repairs, highlighting the value of template-guided generation. The authors also release fine-tuned models (StarCoder-15B and CodeLlama-70B) to promote open science and further research in template-guided APR.",5,5,5,5
Swe-bench_multimodal__Do_ai_systems_generalize_to_visual_software_domains_.pdf,"SWE-bench Multimodal (SWE-bench M) is introduced as the first benchmark to evaluate AI systems on visual, user-facing JavaScript software tasks, expanding beyond the Python-only SWE-bench. It contains 619 task instances from 17 open-source JavaScript repositories, featuring images or videos in problem statements or tests, with 83.5% of tasks deemed visually necessary to solve. The study finds that existing systems that perform well on SWE-bench struggle on SWE-bench M, largely due to visual reasoning requirements and JavaScript language diversity, and many Python-centric localization tools don’t transfer well. Across baselines, the best results are around 12% of tasks resolved, with multimodal, language-agnostic SWE-agent variants performing slightly better than others in some configurations, but overall margins are small and highly dependent on image content. The authors argue that generalizable, language- and modality-agnostic agent systems—emphasizing interaction and environment navigation over rigid pipelines—are needed to tackle visual software domains. SWE-bench M thus highlights the limitations of current approaches and motivates developing more flexible, cross-language, multimodal tools for real-world front-end and visualization tasks.",5,5,5,5
CodeV__Issue_Resolving_with_Visual_Data.pdf,"CODEV is a method that enhances GitHub issue resolution by leveraging visual data (images and videos) in issues, addressing a gap where prior work focused only on textual content. It uses a two-phase approach: first, data processing, which includes fine-grained descriptions of each visual element and a structured, multi-field summary of the issue; second, patch generation, which feeds this processed information into LLMs to produce a code patch. To evaluate visual reasoning, the authors create Visual SWE-bench, a 133-task benchmark across 11 open-source repositories that includes accompanying visuals. Experiments show CODEV achieves state-of-the-art performance, delivering the best Resolved% and about a 60% relative improvement over the Agentless baseline, with further gains when combined with Agentless. Ablation studies reveal that removing any component (independent/contextual descriptions or structured summarization) degrades performance, with structured summarization being particularly important. The results are robust across different vision-language models and coding LLMs, indicating broad applicability, though the work notes limitations such as reliance on a self-constructed benchmark and costs of deploying large models.",4,5,5,5
"Unlimited_Practice_Opportunities__Automated_Generation_of_Comprehensive,_Personalized_Programming_Tasks.pdf","The article introduces a new Tutor Kai feature that automatically generates comprehensive programming tasks—complete with problem descriptions, code skeletons, unit tests, and model solutions—while letting students choose programming concepts and contexts for personalized practice. It reports a two-phase mixed-methods evaluation: Phase 1 with expert/automated assessment of 200 tasks, and Phase 2 with 26 CS students solving and rating tasks. Phase 1 found that 89.5% of tasks were functional and 92.5% solvable, with context integration at 100%, but the ability to implement all requested concepts dropped from 94% for single-concept tasks to 40% for three-concept tasks. Unit tests were correct for 80% of solvable tasks and model solutions were correct in 85.4%, with iterative reflection helping to repair non-functional tasks over iterations. In Phase 2, students reported high satisfaction with personalization, found contextual relevance and problem descriptions solid, and perceived unlimited personalized tasks as beneficial for learning. The authors conclude the feature shows strong potential to provide unlimited, personalized practice and reduce instructor workload, while acknowledging challenges like misalignment between task descriptions and assessments, prompt-injection risks, and the need for educator-in-the-loop validation; future work includes improving task quality and adaptive difficulty.",5,5,5,5
Improving_Examples_in_Web_API_Specifications_using_Iterated-Calls_In-Context_Learning.pdf,"The article presents ICICL (Iterated-Calls In-Context Learning) as a method to automatically generate diverse and correct API parameter examples for OpenAPI specifications by combining retrieval-based prompting, multiple prompt contexts, and post-processing. It builds a parameter bank from thousands of OpenAPI specs, uses BM25 to retrieve similar parameters, generates a greedy (likely correct) example, and then creates multiple prompt contexts (five-shot each) to produce diverse candidates via higher-temperature iterative calls, finally selecting three examples that are type-correct and similar to the greedy one. Intrinsic evaluation shows high type correctness (near 99%), improved uniqueness, and better overall correctness as each component is added. Extrinsic evaluation demonstrates that adding these examples to fuzzers increases 2xx responses and branch coverage, while also improving API chatbots (SeqATIS and SGD) and human API understanding; in some cases, synthetic examples nearly match or outperform hand-written ones. The authors conclude that adding examples via ICICL benefits testing, understanding, and chatbot tasks across APIs, and they release their prompting code and artifacts for reproducibility.",,,,
The_Sustainability_Face_of_Automated_Program_Repair_Tools.pdf,"- The paper investigates the energy consumption of Automated Program Repair (APR) tools, arguing that software sustainability should accompany repair accuracy. 
- It measures energy usage for ten traditional APR tools and eleven fine-tuned LLM-based models repairing real Defects4J bugs, analyzing the trade-off between energy and repairability. 
- Findings show that some high-energy traditional tools (e.g., TBar) fix more bugs but incur large energy costs, while others (e.g., SimFix, CodeT5-Large) achieve a better energy–repair balance. 
- For LLMs, larger models consume more energy due to GPU requirements, and a portion of energy is spent on overfitting patches; the study introduces energy metrics (A0, E5, E52) to quantify energy to first plausible and first correct patches. 
- The results indicate a positive but imperfect correlation between energy spent and the number of bugs correctly repaired, with energy use varying by bug difficulty and project, complicating cross-family comparisons. 
- The authors offer guidelines for greener APR and make the data publicly available to support replication.",,,,
Speed_Up_Your_Code__Progressive_Code_Acceleration_Through_Bidirectional_Tree_Editing.pdf,"The article addresses code acceleration (CA) with LLMs, noting that existing training data often obscure acceleration patterns and ignore hierarchical relationships among optimizations. It presents BITE, a framework combining bidirectional tree editing (a code optimizer and a code degrader to generate optimized and degraded variants, using optimization and degradation trees with pruning) with progressive code acceleration learning (PKAC) that uses multi-level data to gradually master increasingly complex CA patterns. It also introduces a new CA benchmark spanning five languages, up-to-date Codeforces data, and a new Acceleration Progress (AP) metric to gauge how close generated code is to the optimal, alongside traditional OPT, SP, and COR metrics. Experimental results show BITE improves CA performance across LLMs, with Qwen 1.5B outperforming prompt-enhanced GPT-4 on many tasks, and multilingual training providing consistent gains. Ablation studies indicate larger tree degrees and multi-stage training enhance exploration and consolidation, supporting the effectiveness of progressive learning. Limitations include optimizing only runtime (not memory) and relying on open-source LLMs due to cost, suggesting future work on multi-objective optimization and broader code types.",,,,
LLM_program_optimization_via_retrieval_augmented_search.pdf,"The authors introduce Retrieval Augmented Search (RAS), a black-box method that optimizes code by beam-searching over candidate edits guided by in-context examples retrieved from a training set of slow-fast programs. Unlike prior work, RAS uses contextual retrieval based on an LLM-generated natural language description of the program, and iteratively retrieves, applies, and evaluates edits to progressively improve performance. On the PIE benchmark for C++, RAS achieves state-of-the-art results, delivering around 8x mean speedup compared with about 4.4x for previous dynamic retrieval baselines. To improve interpretability, they propose Atomic Edit Guided Search (AEGIS), which decomposes training pairs into atomic edits and uses RAS to search over these incremental edits. AEGIS yields about 6x average speedup with smaller edits (lower edit distance) than RAS, though it generally trails full RAS in raw speedup; ablations show the value of contextual retrieval and search. They note limitations such as higher computational cost due to beam search and additional training-time compute for AEGIS, but conclude that RAS and AEGIS are promising for blackbox LLM adaptation to code optimization and potentially other tasks.",,,,
SV-TrustEval-C__Evaluating_Structure_and_Semantic_Reasoning_in_Large_Language_Models_for_Source_Code_Vulnerability_Analysis.pdf,"The article presents SV-TrustEval-C, a benchmark designed to evaluate LLMs’ reliability in vulnerability analysis of C code, focusing on two core capabilities: structure reasoning (data and control flow) and semantic reasoning (counterfactual, goal-driven, and predictive scenarios). It introduces a Structure-Oriented Variants Generator that creates safe, unsafe, and impaired code variants by analyzing data/control flow graphs and increasing structural complexity. Built on the Juliet Test Suite, SV-TrustEval-C includes 377 base files across 82 CWEs, producing 1,297 unsafe and 1,286 safe variants and a total of 9,401 questions spanning both structural and semantic tasks. The study applies label masking to avoid leakage and evaluates 11 popular LLMs in zero-shot and in-context learning settings. Results show that current LLMs struggle to understand complex code relationships and largely rely on pattern matching rather than robust reasoning, with performance improving for larger or instruction-tuned models but remaining unsatisfactory overall. The authors provide the dataset publicly on HuggingFace and advocate for further work to enhance LLM reasoning for trustworthy real-world vulnerability analysis.",,,,
Enhancing_code_llm_training_with_programmer_attention.pdf,"The authors introduce HumanLLM, a two-part approach that integrates programmer eye-tracking signals into CodeT5 training to improve code summarization (also tested on completion and translation). They collect real eye-tracking data from professional Java developers and augment code tokens with semantic AST labels, adjacency-based expansions, k-gram patterns, and positional reading order. A reward-guided fine-tuning loop periodically uses these human-informed signals to adjust training, adding a reward term to the standard cross-entropy loss to align outputs with human fixations. On CodeXGlue code summarization, they report notable gains: BLEU +4.96, ROUGE-L +4.11, METEOR +5.66, and CodeBLEU +7.16 with Syntax +2.67 and Dataflow +14.97. They find that adjacency window size and training progress influence performance, with larger windows generally helping up to a point. However, for code completion and Java-to-C# translation, the fixation-based cues yield limited gains, suggesting task-specific cues are needed. They acknowledge threats to validity (Java-heavy data, sampling) and outline future work to extend the approach to other tasks and richer human signals.",,,,
Acecoder__Acing_coder_rl_via_automated_test-case_synthesis.pdf,"ACECODER presents a fully automated pipeline that synthesizes large-scale test cases to train reward models and perform reinforcement learning for code generation. The authors build ACECODE-87K by turning seed Python datasets into structured LeetCode-style questions, generating around 20 imagined tests per prompt, and filtering them with a stronger coder model, resulting in 87.1K questions and 1.38M clean test cases. They train reward models ACECODE-RM-7B and ACECODE-RM-32B using Bradley-Terry loss on approximately 300K preference pairs, and show strong Best-of-N improvements across multiple backbones (e.g., Llama-3.1, Qwen-2.5-Coder-7B-Instruct). In reinforcement learning, they fine-tune policies with Reinforcement++ using either the reward model or test-case pass rewards, achieving consistent gains on HumanEval, MBPP, BigCodeBench, and LiveCodeBench, including up to 25% improvement on HumanEval-Plus when starting from a base model with only 80 RL steps. ACECODE-RM also outperforms other open-source reward models on RM-Bench and can match or exceed results of much larger models on select tasks, suggesting reliable test-case signals are especially valuable for code generation. The authors acknowledge limitations around test-case quality and potential reward hacking, and emphasize that RL gains can vary, indicating room for further improvement. Overall, they claim this is the first work to jointly train reward models and perform RL for coding with a fully automated test-case synthesis pipeline, providing ACECODE-RM and ACECODE-87K as resources for the community.",,,,
On_Pretraining_For_Project-Level_Code_Completion.pdf,"- The paper studies repository-level pretraining for project-level code completion using OpenCoder 1.5B, extending the context window from 4K to 16K tokens by training on about 1B curated repository tokens. 
- On the Long Code Arena’s Project-Level Code Completion (LCA-large), the model achieves state-of-the-art results with far less data than competing models, demonstrating the effectiveness of repository-level pretraining at a smaller scale. 
- A key finding is that the choice of context composer (how repository files are aggregated into context) has only marginal impact, with gains of roughly +3 points in some settings, while adapting to a longer RoPE-based context window provides the main performance boost (approximately +19 to +22 EM points in certain comparisons). 
- The results also show that a simpler file-level training approach at the original sequence length remains effective, making repository-level research accessible with limited data and compute. 
- The authors acknowledge that the study is limited to OpenCoder and may not generalize to other LLMs, and they call for testing broader Code LLMs in future work. 
- Overall, the work argues that improving long-context attention (RoPE scaling) is the dominant factor in long-context code completion, with repository-processing strategies playing a secondary role.",,,,
DeCOS__Data-Efficient_Reinforcement_Learning_for_Compiler_Optimization_Selection_Ignited_by_LLM.pdf,"The article presents DeCOS, a data-efficient reinforcement learning framework for selecting compiler optimization sequences, boosted by large language model (LLM) guidance. To improve data efficiency, DeCOS uses a training-data synthesizer for hyperparameter tuning and incorporates simulation results to refine profiling information, reducing the need for costly real-data collection. It employs an augmented code representation that fuses lightweight IR embeddings (IR2vec reduced to 8 dimensions) with runtime performance counters and additional signals such as past optimization choices and time-consuming instructions. An LLM (GPT-4o-mini) accelerates the start-up phase by suggesting promising initial optimizations and interacting with the RL agent to refine exploration. To handle noise in profiling, DeCOS combines real profiling with Cachegrind-based simulations for intermediate observations while using real data to validate final performance gains. The system optimizes function-level LLVM IR by iteratively building optimization sequences up to length 16 and updating its policy every three completed sequences, achieving performance that matches or surpasses Opentuner on SPEC benchmarks and demonstrating portability across apps and hardware.",,,,
AnnCoder__A_Mti-Agent-Based_Code_Generation_and_Optimization_Model.pdf,"AnnCoder is a multi-agent framework for code generation that fuses simulated annealing with evolutionary genetics in a closed-loop to escape local optima and improve solution quality. It employs a four-dimensional, dynamically weighted scoring system (functional correctness, efficiency, symmetry, readability) with adaptive temperature control to balance exploration and convergence. The system features Case Retrieval, Code Synthesis, and Defect Diagnosis agents plus a closed-loop feedback mechanism that updates semantic weights and defensive templates based on failures. Its Evolutionary Annealing Optimization module combines elitism and tournament selection, uses defensive crossover with templates, adjusts temperature according to population diversity, and enforces a token budget. Defensive Programming Templates (input validation, error handling, boundary checks) are integrated to boost robustness and reduce debugging needs. On four benchmarks (HumanEval, HumanEval-ET, EvalPlus, MBPP) across multiple LLMs, AnnCoder achieves leading performance (e.g., about 90% on HumanEval and MBPP, with strong results on the other datasets), outperforming baselines such as Reflexion, Self-Planning, MapCoder, and CodeSim.",,,,
Evaluating_the_Generalizability_of_LLMs_in_Automated_Program_Repair.pdf,"The authors evaluate 11 top-performing LLMs on automated program repair (APR) using both Defects4J and a newly created transformed dataset, DEFECTS4J-TRANS, to study generalizability. DEFECTS4J-TRANS is produced by five AST-based transformations (variable renaming, loop changes, switch-to-if, dead code insertion, and boolean negation) that preserve semantics but alter content. Results show all LLMs experience a substantial drop on the transformed data: on average, correct patches fall by about 49.5% and plausible patches by about 42.9%. They also test adding repair-relevant information to prompts; for four LLMs with weaker generalizability, these prompts boost correct patches by 29.4%–136.7% and plausible patches by 9.8%–121.8%, with bug-report information yielding the strongest gains, yet overall performance remains below the original Defects4J results. The study observes that larger models sometimes generalize worse, potentially due to overfitting, and notes that many patches still target injected dead code or transformed predicates rather than real errors. The authors conclude that prompt engineering alone is insufficient to substantially improve generalizability and propose future directions such as code normalization, integrating traditional APR techniques, broader evaluations, and sharing data.",,,,
Benchmarking_and_categorizing_the_performance_of_neural_program_repair_systems_for_Java.pdf,"- The article argues that evaluating Neural Program Repair (NPR) systems is biased because systems are trained on different data and are often judged only by total bugs fixed; it introduces NPR4J, a benchmark and framework for fair training and evaluation of NPR for Java. 
- They retrain seven state-of-the-art NPR systems on a unified dataset and evaluate them across three sources (Defects4J, QuixBugs, Bears, plus others), yielding 9,070 plausible patches with 1,849 confirmed correct patches after manual review. 
- A key contribution is a lightweight bug taxonomy (ErrorState: Redundant, Incorrect, Missing; ErrorElement: Token, PartialStmt, WholeStmt, MultiStmt) to assess NPR repairability by defect class. 
- The study investigates both effectiveness (which bug types NPRs can repair) and efficiency (how performance changes with beam size and with evaluating more plausible patches). 
- They also compare NPRs to non-learning APR approaches by measuring overlap/uniqueness of fixes on Defects4J to understand the distinct value of neural methods. 
- NPR4J provides a modular pipeline (DataManager, Preprocessor, Trainer) and a curated, leakage-conscious dataset (NPR4J-Benchmark) to enable reproducible, fair evaluation and guidance for practitioners and researchers. 
- In conclusion, retraining on a unified dataset reveals varied repairability across bug types and shows that efficiency depends on configuration, underscoring the need for standardized benchmarks to meaningfully advance NPR.",,,,
CODEMORPH__Mitigating_Data_Leakage_in_Large_Language_Model_Assessment.pdf,"The paper tackles data leakage in Code LLM evaluation by introducing CODEMORPH, a framework that perturbs code across multiple languages (C/C++, Python, Java, Rust, Go) while preserving functionality and cross-file dependencies. CODEMORPH has two components: (1) a perturbation/verification module that uses 26 semantic-preserving transformations to produce diverse, compilable variants, and (2) PESO, a genetic-algorithm-based method that selects effective perturbations by optimizing a similarity-based objective. Similarity is computed from surface-level and semantic differences, and Boltzmann selection is used to balance exploration and exploitation during method choice. The authors evaluate on StarCoder with The Stack dataset, constructing 100 original samples per language and three code-completion task types, with human accuracy as the evaluation metric due to lack of unit tests. They report CODEMORPH reduces perturbed-code LLM accuracy by an average of 24.67% (Python up to 45%), and PESO achieves a 7.01% average reduction in code similarity (max 42.86%) and an average 15% drop in code-completion accuracy (max 25%) compared with random perturbations. The work provides a scalable, multi-language perturbation framework that preserves repository dependencies and offers an artifact for reproducibility.",,,,
Ecosystem_of_large_language_models_for_code.pdf,"The article presents an ecosystem view of LLMs for code (LLM4Code) on Hugging Face, compiling 429 models, 73 datasets, and 539 dependencies to study how code-oriented models and data interact. It finds that company-owned accounts dominate popularity (likes) and have high impact, though they do not necessarily contribute more models, and the network shows a few hub-like nodes with power-law degree distributions. The authors develop a nine-category taxonomy of model reuse (fine-tune, architecture sharing, quantization, continue training, model conversion, distillation, adapters, instruction-tuning, adversarial tuning) with fine-tuning and architecture sharing being the most common. They report publishing practices as lacking in documentation and license information (over 60% of models with no license), and while licenses tend to be permissive or AI-specific, they detect no incompatibilities due to missing licensing data. The paper also demonstrates the potential of using LLMs to automate ecosystem analysis, achieving 98% accuracy in identifying LLM4Code, 87% for base-model inference, and 89% for reuse-type prediction, and expanding the ecosystem to about 6,000+ items consistent with manual results. Finally, the authors discuss implications and recommendations to foster healthy growth, emphasizing improved documentation and clearer licensing.",,,,
Hybrid_automated_program_repair_by_combining_large_language_models_and_program_analysis.pdf,"GiantRepair is a hybrid automated program repair approach that uses patches generated by large language models (LLMs) as guidance to produce high-quality, program-specific patches through patch skeletons and context-aware instantiation. It tackles two main limitations of prior LLM-based APR: that LLM patches may be incorrect or domain-specific, and that evaluations often assume perfect fault localization. The method extracts concrete modifications from LLM patches via AST-level differencing, then abstracts these into skeletons with rules that preserve structure while omitting concrete details. It then instantiates these skeletons by static analysis to fill in usable and context-appropriate program elements, guided by three optimizations: element usability, similarity to both the buggy code and the LLM patch, and an adaptive application strategy that limits changes to a small, focused set. Patches are ranked and validated by running tests to identify plausible patches, with correctness defined as semantic equivalence to the developer patch. In Defects4J v1.2 and v2.0, GiantRepair improves correct fixes by about 28% and 23% over raw LLM patches and outperforms state-of-the-art APRs by at least 42 more bugs under perfect localization and at least 7 more under automated localization, demonstrating strong generality and complementarity across LLMs. The authors also open-source their implementation.",,,,
RAG_or_Fine-tuning__A_Comparative_Study_on_LCMs-based_Code_Completion_in_Industry.pdf,"- The paper compares Retrieval-Augmented Generation (RAG) and Fine-Tuning (FT) for industrial code completion using Tencent WXG’s large C++ codebase (about 160k files; 120k for retrieval/FT, 20k test). 
- It evaluates six large code models (DSC 1.3B/6.7B; QC 0.5B/1.5B/3B/7B) and several retrieval strategies (BM25, neural embeddings like CodeBERT/UniXcoder/CodeT5/CoCoSoDa, plus dependency-based retrieval). 
- Findings show a substantial domain gap for industrial code; both RAG and FT improve performance, but RAG can reach a higher performance ceiling, with BM25 delivering the strongest results among RAG methods. 
- FT and RAG are complementary, and combining them yields additional gains beyond either approach alone. 
- In terms of efficiency, FT is resource-intensive in preparation but incurs no runtime overhead, while RAG has lighter prep but adds runtime retrieval costs and longer input sequences, though it scales better with larger codebases. 
- The study provides practical guidance, suggesting using RAG for generalization, considering FT when resources permit, and being cautious with neural embeddings that may underperform; FT can also cause some forgetting on general tasks.",,,,
SWE-Dev__Building_Software_Engineering_Agents_with_Training_and_Inference_Scaling.pdf,"- SWE-Dev is an open-source software engineering agent framework focused on training and inference scaling to tackle real-world SWE tasks.  
- It introduces a scalable test-case generation pipeline that uses LLMs to create Gherkin-style descriptions and corresponding code patches, producing about 2,000 executable test cases from 38,000 issues across 4,000 repos, with an LLM-based filter to keep high-quality trajectories.  
- For inference, it uses iteration scaling (more interaction rounds per evaluation); larger models benefit more, e.g., SWE-Dev-32B reaches 36.6% resolve rate on SWE-bench-Verified, approaching GPT-4o performance.  
- Training-data scaling shows model performance increases roughly linearly with log data, with bigger models (32B) gaining more from additional trajectories (7B example: 574 to 16,639 trajectories from 13.0% to 22.8%).  
- Post-training, Rejection Sampling Fine-Tuning (RFT) provides the strongest gains, while offline RL methods like KTO and OREO offer smaller, more limited improvements.  
- SWE-Dev achieves state-of-the-art results among open-source SWE agents on SWE-bench-Verified and is publicly available on GitHub, offering a scalable foundation for future enhancements including online RL and adaptive iteration strategies.",,,,
FastFixer__an_efficient_and_effective_approach_for_repairing_programming_assignments.pdf,"FastFixer is an automated program repair (APR) approach for advanced programming assignments that combines a repair-oriented fine-tuning strategy with an inference acceleration method to produce accurate patches quickly. It introduces a modification-focused masking strategy during fine-tuning to bias the model toward learning actual patch Bank patterns and their necessary context, addressing the problem of models reproducing buggy code. The method runs in three stages: Prompt Generation (rich educational context and bug-type hints), Repair-oriented Fine-tuning (weighted loss over only the modified parts), and Program Repair (an inference acceleration technique that uses the buggy code as a draft for parallel verification). The acceleration mechanism builds a high-quality draft from the buggy code, performs a single autoregressive step to obtain a verified segment, and then accepts the longest matching prefix to speed up decoding, achieving substantial reductions in inference steps. Evaluated on Defects4DS and the expanded Defects4DS-L dataset, FastFixer achieves 312 fixed programs and a 20.46% improvement over PaR-ChatGPT, while delivering up to a 16.67× speedup compared to autoregressive decoding and outperforming baselines like ChatRepair and RepairLlama, especially on semantic errors. The results demonstrate FastFixer’s robustness and effectiveness for repairing complex student code and highlight the novelty of applying inference acceleration to LLM-based APR.",,,,
Question_Selection_for_Multi-Modal_Code_Search_Synthesis_using_Probabilistic_Version_Spaces.pdf,"EXCALIBUR is a multi-modal, interactive synthesizer for code search that combines an input-output example with a natural language description to address overfitting and misalignment with user intent. It guarantees soundness (all consistent programs pass the example) and bounded completeness (the target program is included within a bounded search space) and supports QA-based disambiguation. Candidate programs are encoded in a version space using a lightweight ExPath DSL, which avoids brute-force enumeration. A Probabilistic Version Space (ProbVS) uses an LLM to score code fragments against the NL description and propagates these scores to assign a user-aligned probability distribution over programs to guide question selection. Disambiguation is performed via a QA loop that employs a minimax-branch strategy to choose clusters of questions that minimize the worst-case number of interaction rounds. In experiments on 44 Java tasks, EXCALIBUR solves all tasks in 4.5 rounds on average without ProbVS (with about 68% top-ranked targets) and reduces to 3.68 rounds when ProbVS (with GPT-4o) is used, outperforming a SPORQ baseline.",,,,
When_Faster_Isn't_Greener__The_Hidden_Costs_of_LLM-Based_Code_Optimization.pdf,"Coignion, Quinton, and Rouvoy study the environmental trade-offs of using LLMs to optimize code. They evaluate eight optimization methods across five LLMs on 118 EvalPerf tasks, introducing the Break-Even Point (BEP) as the number of executions needed for energy savings to offset the optimization energy cost. They find that while some configurations can reduce energy usage by up to about 49%, these gains often require hundreds of thousands of executions to become profitable, and in many cases the optimization itself consumes more energy than it saves. Energy costs are dominated by the LLM generation phase, and larger models tend to incur higher per-optimization energy, sometimes by large factors. There is a weak negative correlation between runtime performance gains and actual energy savings, with some methods improving accuracy but others trading correctness for speed. The authors conclude that LLM-based optimization can be energy-efficient in the right contexts but requires careful model-method selection and energy monitoring across the code life cycle, prioritizing high-frequency workloads. They provide practical metrics (BEP, Efficient@k, Pass@1/Pass@result, DPS norm) and a replication package for energy-aware evaluation.",,,,
CodeContrast__A_Contrastive_Learning_Approach_for_Generating_Coherent_Programming_Exercises.pdf,"CodeContrast is a contrastive-learning model that generates coherent programming exercises by mapping problem descriptions, test cases, and code solutions into a shared feature space using three encoders (BERT for problems, BiLSTMs for test cases, and RoBERTa for code). It trains on positive triplets (matching problem–test case–solution) and negative triplets with NT-Xent loss, aided by hard negative mining, data augmentation (back-translation for descriptions and semantic-preserving code transformations), and curriculum learning. Automatic evaluation across Python, Java, and C++ shows high code correctness (average 92.3% of test cases passed) and strong problem–solution alignment (BLEU and BERTScore scores well above 0.8 and 0.85 respectively). Test-case coverage is robust, reported at about 85.7% statement, 79.4% branch, and 92.1% function coverage. Diversity metrics indicate substantial variety, with roughly 8.6k–8.9k unique problems, 8.9k–9.1k unique test cases, 8.8k–8.9k unique solutions, and text entropy around 4.18–4.21 bits. The authors implemented a software prototype and released the code on GitHub, validating the approach on GPU-backed infrastructure and arguing that CodeContrast effectively captures cross-component relationships to produce pedagogically valuable, coherent exercises.",,,,
Epicoder__Encompassing_diversity_and_complexity_in_code_generation.pdf,"EpiCoder introduces a feature tree-based framework to synthesize diverse and complex code data beyond seed snippets, organizing high-level code features into a hierarchical tree and iteratively evolving it to improve coverage. Features are extracted from seed data (From The Stack v2) using an LLM (GPT-4o) and hierarchical clustering, with node frequencies recording knowledge distribution. The framework expands the feature tree by sampling subtrees and growing them in depth and breadth, estimating new feature frequencies from siblings to enhance diversity and efficiency. Code generation is guided by reweighted feature distributions and sampled subtrees, enabling task generation that spans from function-level to multi-file and repository-level scenarios, with iterative debugging and test execution to ensure correctness. Trained on 380k function-level and 53k file-level synthetic samples, EpiCoder-Series achieve state-of-the-art performance on multiple function-level benchmarks and strong results on file-level XFileDep, indicating strong cross-level capability and potential for repo-level synthesis. Analyses show the synthetic data exhibits higher complexity and diversity (Halstead, cyclomatic, and other metrics) than baselines. The authors release their code and data publicly, illustrating the practicality and scalability of feature-tree guided code synthesis.",,,,
Supersonic__Learning_to_generate_source_code_optimizations_in_C_C++.pdf,"SUPERSONIC is a seq2seq-based system that learns to generate small, diff-based source-code optimizations in C/C++. It is trained on pairs (xt, xt+1) mined from competitive programming submissions (Codeforces, AtCoder, AIZU) with strict filters to ensure limited edits and high similarity, and it uses canonicalization, diff-synthesis, and post-processing to apply patches to the original code. The model is initialized from CodeBERT-CPP and consists of 278 million parameters, trained on a large dataset of optimization pairs. At inference, SUPERSONIC outputs a patch-style diff rather than a full program, which improves stability and reduces output length. In evaluations against GPT-3.5-Turbo and GPT-4 on 559 test programs, SUPERSONIC outperforms the OpenAI models in running-time and memory optimizations and is roughly 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4, making it far more cost- and energy-efficient. An ablation shows the diff-based output representation yields at least a 2x improvement over full-program generation, with most outputs maintaining high similarity to the input (i.e., targeted edits rather than rewrites). The authors validate results on external contest sites and publicly release their code, data, and models on GitHub for open science.",,,,
DependEval__Benchmarking_LLMs_for_Repository_Dependency_Understanding.pdf,"DEPEND EVAL is a hierarchical, multilingual benchmark designed to assess LLMs’ ability to reason about repository-scale dependencies, built from 15,576 real GitHub repositories across eight languages. It targets three progressively challenging tasks: Dependency Recognition (inferring inter-file calls), Repository Construction (generating structured project layouts from natural language requirements), and Multi-file Editing (coordinated changes across multiple files with in-place or expansion edits). Test data are produced by constructing dependency snippets from import graphs, with GPT-4o and human review to generate natural-language requirements and file descriptions, using ground-truth dependencies for evaluation. Evaluation combines: Exact Match for dependency recognition, graph-based precision/recall/F1 for repository construction, and a multi-file editing score based on several LLM-judged criteria (with predefined weights). Findings show larger models generally perform better but gains diminish for complex tasks like multi-file editing; domain-specific models can outperform larger general models, and performance varies by language (Python/JavaScript easier than PHP/C#, C/C++). The study identifies key challenges in repository-level reasoning—dependency parsing, cross-file reasoning, and maintaining consistency—and calls for better training data and architectures to improve repository-wide code understanding. Limitations include the scope of languages, tasks, and models, with plans to broaden languages, add more tasks (e.g., debugging, refactoring), and expand model coverage, while open-sourcing the data and tools.",,,,
ECCO__Can_We_Improve_Model-Generated_Code_Efficiency_Without_Sacrificing_Functional_Correctness_.pdf,"ECCO is a reproducible benchmark for evaluating the efficiency of model-generated Python code, with two task formulations: history-based code editing and NL-based code generation, and it measures execution correctness, runtime, and memory using a cloud-based Judge0 platform. It compiles about 1.3k programming problems from CodeNet, with over 50k slow/fast solution pairs and both public and private test cases to assess correctness and efficiency. The study analyzes three approaches: in-context learning, iterative refinement with various feedback types (natural language feedback, raw execution feedback, and NL reflection on execution), and fine-tuning conditioned on execution and editing history. The results show that adding execution information helps preserve functional correctness, while NL-based feedback often improves efficiency, but no method consistently improves time/space without some correctness trade-off. History-based editing generally yields higher pass@1 than NL-instructed generation, though exec-refine (execution feedback) often provides the best balance of correctness and efficiency, and NL-based methods can degrade correctness in some settings. Fine-tuning can boost correctness in editing tasks (especially trajectory-conditioned tuning), but prompting-based methods tend to perform best overall, with model scaling yielding mixed effects depending on the paradigm. The authors release ECCO to spur future correctness-preserving program optimization research and note limitations, including focus on Python and competitive programming problems.",,,,
Optimizing_code_runtime_performance_through_context-aware_retrieval-augmented_generation.pdf,"A UTOPATCH is a context-aware framework that combines retrieval-augmented generation (RAG) with control-flow graph (CFG) analysis to automatically optimize code runtime. It mimics human programmer reasoning through an analogy-driven workflow, leverages historical optimization patterns, and uses in-context prompts to generate optimized code. Central to the method is CFG difference analysis ∆G between original and optimized code, represented as ∆S (structure), ∆F (flow), and ∆C (content), which are fed into LLM prompts together with a single retrieved example Er containing the transformation rationale. The approach retrieves CFG-embedded examples from a vector store and selects the most relevant patch using cosine similarity, formulating a prompt P = {∆G, Ropt, Er} to guide the LLM’s output. Experiments on 1,200 C++ code pairs from IBM CodeNet show improvements in lexical similarity and, notably, a 7.3% reduction in execution time over zero-shot GPT-4o, with strongest gains in code refactoring, memory optimization, and loop optimization. The authors discuss maintainability concerns and future directions (debugging, program repair, broader data), and provide access to data and code via a public repository.",,,,
SecureQwen__Leveraging_LLMs_for_vulnerability_detection_in_python_codebases.pdf,"SecureQwen is a vulnerability detection tool for Python codebases that uses a decoder-only transformer with a 64k token context to classify code sequences into 14 CWEs, including OS Command Injection and SQL Injection. It addresses scalability gaps in manual reviews and traditional static/dynamic analysis by leveraging large language models to capture complex relationships between code tokens. The model is fine-tuned from CodeQwen, featuring 32 decoder layers, 4096-dimensional embeddings, Rotary Positional Embeddings, and a 15-class CWE output. Training data comprise 1.875 million function-level Python snippets from GitHub, Codeparrot, and about 640k synthetic snippets generated by LLMs, covering both real and AI-generated code. Data labeling combined multiple static analysis tools (Bandit, Semgrep, SonarQube, CodeQL, PyLint) with manual review, resulting in the PythonVulnDB dataset with 15+ labels. Evaluation reports F1 scores ranging from 84% to 99%, demonstrating strong performance across both human-written and AI-generated code. The authors position SecureQwen as a robust benchmark for Python vulnerability detection and a tool with potential to significantly enhance software security practices.",,,,
Brite__Bootstrapping_reinforced_thinking_process_to_enhance_language_model_reasoning.pdf,"BRiTE introduces a unified probabilistic framework for LLM reasoning by modeling a latent thinking process Z and an evaluation signal O alongside prompts X and answers Y, allowing automatic generation of high-quality reasoning paths. It bootsraps reasoning in two steps: (i) RL-based generation of rationales that approximate the desired thinking process, and (ii) fine-tuning the base LLM to maximize the joint probability of the rationale and the final answer given X. The authors prove a convergence rate of 1/T under Assumption 3.2 (with a concave objective in RKHS), and they design a reward-shaping approach to convert posterior inference into RL optimization. BRiTE is shown to generalize and unify existing learning paradigms (SFT, RLHF, rejection sampling, RestEM, DPO) within its EM-like two-step procedure. Empirically, BRiTE consistently improves over rejection-sampling baselines on GSM8K, MATH, and code tasks and can match or exceed performance achieved with human-annotated thinking in some cases; BRiTE-DPO also enhances RLHF performance. In larger-scale experiments with bigger datasets, BRiTE yields substantial gains across diverse math benchmarks and exhibits faster, more stable training dynamics. Overall, BRiTE provides a provable, practical framework for automated generation of reasoning processes that enhances LLM reasoning across domains.",,,,
Codei_o__Condensing_reasoning_patterns_via_code_input-output_prediction.pdf,"The article tackles the lack of broad, high-quality supervision for diverse reasoning tasks beyond math and coding by proposing CODE I/O, which condenses reasoning patterns embedded in real code into a code input-output prediction task using natural-language chain-of-thoughts (CoTs). It transforms raw code into executable functions, generates multiple input-output pairs by running the code, and trains models to predict inputs or outputs in natural language, decoupling reasoning from code syntax; it also introduces CODE I/O++, which uses execution feedback for multi-turn revisions to improve reasoning quality. The authors assemble a large-scale dataset (~450k functions, ~3.5M I/O samples) from multiple sources, convert them to a unified format, and synthesize CoTs with a strong open-source model, DeepSeek-V2.5, including a dedicated input generator to ensure non-trivial cases. They employ a two-stage training pipeline (CODE I/O data stage followed by instruction-tuning) and evaluate across 14 benchmarks spanning symbolic, mathematical, scientific, logic, and commonsense reasoning, showing universal gains over baselines; CODE I/O++ further enhances performance through feedback-driven revisions. Ablation studies demonstrate the importance of predicting both inputs and outputs, the preferred data format (query plus reference code with CoT in the response), and the necessity of two-stage training, as well as the positive scaling effects of more data. The results indicate that CODE I/O provides robust, generalizable improvements across model sizes and domains, and the authors make their data and models available on GitHub for reproducibility and further research.",,,,
Can_test_generation_and_program_repair_inform_automated_assessment_of_programming_projects_.pdf,"- The study investigates whether automated test generation (EvoSuite and an open LLM, Qwen2.5) and program repair (ARJA-e and LLM-based repairs) can support automated assessment of a real-world, multi-file Java project with 296 incorrect submissions.  
- Results show generated tests are generally insufficient to detect bugs compared with educator-written tests, identifying bugs in over 50% of cases less often, with only about 2% of cases where generated tests are equivalent or superior.  
- For program repair, off-the-shelf traditional repair (ARJA-e) fully repairs only around 2–2.3% of submissions, partially repairs about 6%, and LLM-based repairs (Qwen2.5) yield similar or slightly better partial repair rates but far fewer fully repaired submissions.  
- When using a reference solution and an incremental, method-level repair approach, repair effectiveness improves somewhat but remains limited, with full repairs remaining a small fraction and partial repairs comprising a larger share.  
- The authors conclude that current test generation and program repair tools are not yet adequate for automated assessment of complex programming assignments, and they offer practical guidelines and considerations for educators to better leverage these tools.  
- The study highlights key limitations (multi-file, nuanced functionality, and cross-method interactions) and suggests that more advanced techniques are needed to meaningfully support automated grading and feedback for intermediate/advanced programming projects.",,,,
Can_LLMs_Reason_About_Program_Semantics__A_Comprehensive_Evaluation_of_LLMs_on_Formal_Specification_Inference.pdf,"FormalBench is introduced as a large-scale benchmark to evaluate LLMs on formal specification inference for Java/JML, comprising FormalBench-Base (700 manually validated programs) and FormalBench-Diverse (6,219 semantic-preserving variants) to test robustness. The task requires synthesizing complete and consistent formal specifications, with consistency checked by deductive verification and completeness proxied by mutation testing. Across eight LLMs, zero-shot performance is around 10% success, with few-shot prompts boosting success and reducing failures (up to ~7 percentage points in SR and ~17 points in FR), while least-to-most prompting helps some models but loops remain particularly challenging. The study finds substantial robustness issues: semantic-preserving transformations yield flip rates of 27.2%–39.2%, showing models rely heavily on syntax rather than true semantic understanding. Common failures include syntax errors, incorrect postconditions, faulty loop invariants, and weak arithmetic reasoning; self-repair prompts can raise verifiable specifications by about 25% and reduce failures by up to ~40%, with mutation-based repair offering additional but costly gains. The authors conclude that current LLMs have limited capability to reason about program semantics, especially for complex control flow, but identifying failure patterns and applying tailored prompting can help, and they release FormalBench as an Apache 2.0 library to spur future research.",,,,
SnipGen__A_Mining_Repository_Framework_for_Evaluating_LLMs_for_Code.pdf,"SnipGen is a framework for mining GitHub code to create robust, prompt-augmented testbeds for evaluating large language models on software engineering tasks, while aiming to reduce data contamination. It mined about 227,000 data points from 338,000 recent Python commits, extracting method-level snippets and enriching them with features from the AST, code metrics, documentation, vulnerabilities (via CodeQL), and allowed mutations. Each data point is paired with prompt templates that enable single-step and multi-step tasks across code completion, commit generation, and code summarization, using eight templates and combinations to form CoT-style prompts. The pipeline includes data collection with PyDriller, deduplication, AST parsing with Tree-sitter, vulnerability tagging, and prompt generation, with manual validation of docstrings and code to ensure meaningful context. SnipGen produces testbeds for six SE tasks, yielding thousands of prompts and data points, and has been used in case studies like Galeras, SyntaxEval, and ASTxplainer to assess prompt engineering and model behavior. The authors emphasize SnipGen as a means to create dynamic, less-contaminated benchmarks that complement existing datasets, and note future work to broaden language support, add more SE tasks, and rank data points by complexity or other relevance metrics.",,,,
SWIFTCODER__Enhancing_Code_Generation_in_Large_Language_Models_through_Efficiency-Aware_Fine-tuning.pdf,"EFFICODER is a framework that fine-tunes large language models to improve both the correctness and efficiency of code generation, using a new dataset called EFFIINSTRUCT of high-quality, efficient code. The dataset is built by collecting tasks from nine open-source sources, generating multiple candidate solutions with several LLMs, and selecting the most efficient solution via local execution measurements of time and memory. EFFIINSTRUCT contains 65,710 tasks across Python, C++, Java, Rust, and Go, and analyses show that efficient solutions achieve substantially lower execution time and memory usage than inefficient ones. The authors fine-tune eight LLMs on EFFIINSTRUCT and evaluate on multiple benchmarks (EffiBench, HumanEvalPlus/MBPPPlus, DS-1000, EvoEval, HumanEval-X), reporting significant gains in both efficiency (ET, MU, TMU) and correctness (pass@1). For example, Qwen2.5-Coder-7B-Instruct’s pass@1 on EffiBench rises from 44.8% to 57.7% and average execution time drops by about 48.4%. EFFICODER also outperforms baselines like PIE and Mercury, benefits from multilingual support and large-scale training data, and the authors plan to open-source models, data, and code to advance research and enable deployment in resource-constrained settings.",,,,
Investigating_Execution-Aware_Language_Models_for_Code_Optimization.pdf,"The paper investigates whether teaching language models to understand code execution at run-time (via line executions, line coverage, branch coverage, and variable states) can improve automatic code optimization. It introduces three training strategies (execution-aware pre-training + fine-tuning, execution-aware pre-training with masked LM, and execution-aware fine-tuning) and twelve models derived from CodeT5+ across four execution aspects, evaluated against a CodeT5+ baseline. The study uses the PIE dataset for optimization tasks and collects execution traces with gdb, encoding execution information into tokens for model learning. Evaluation on the PIE test set shows that execution-aware models generally underperform the baseline in correctness, speedup, and percent optimized; only execution-aware fine-tuning on line executions (LES3) offers a small, non-significant improvement. Line coverage occasionally yields the best results among execution-aware approaches, but still does not beat the baseline, while branch coverage and variable states consistently degrade performance. Overall, the findings suggest that incorporating run-time execution information, as implemented here, does not reliably enhance code optimization with current methods, though the authors provide replication data for future work.",,,,
Opencoder__The_open_cookbook_for_top-tier_code_large_language_models.pdf,"OpenCoder is an entirely open-source code LLM that provides weights, inference code, and a fully reproducible data and training pipeline to advance transparent research in code AI. It introduces RefineCode, a 960B-token pretraining corpus combining raw code and code-related web data, with aggressive file-level deduplication, multi-tier filtering, copyright/PII removal, and language balancing to yield about 730B pretraining tokens. During annealing, it adds Algorithmic Corpus and synthetic high-quality data to further refine capabilities while preserving the original data distribution (84%), including self-contained code snippets and code textbooks. The model family includes OpenCoder-1.5B (24 layers) and OpenCoder-8B (32 layers) with SwiGLU activations and RoPE embeddings, trained via Megatron-LM on large GPU clusters. The training workflow employs a two-stage instruction tuning (theoretical CS QA first, then practical code tasks) and strict decontamination to avoid test data leakage. Empirical results show OpenCoder-1.5B and 8B outperform prior fully open models at similar scales and achieve competitive performance with proprietary models on benchmarks such as HumanEval, MBPP, BigCodeBench, LiveCodeBench, and MultiPL-E, supported by a comprehensive open release that acts as a reproducible cookbook for code LLM research.",,,,
Syncmind__Measuring_agent_out-of-sync_recovery_in_collaborative_software_engineering.pdf,"SyncMind defines and measures agent out-of-sync recovery in collaborative software engineering, formalizing out-of-sync states (knowledge gaps, state mismatch, and task failure) and two recovery modes (independent and collaborative) plus resource-aware constraints. SyncBench provides a large, real-world benchmark: 24,332 out-of-sync instances from 21 GitHub repos, with an evaluation subset of 300 (150 Caller, 150 Callee) and executable tests. Experiments across seven LLMs show substantial capability gaps, with collaboration generally improving recovery but agents exhibiting low willingness to seek help; an oracle-like collaborator upper bound indicates much higher potential. Successful recovery hinges on accurate localization and efficient root-cause analysis, and while more complex tasks are tougher, collaboration yields larger gains on harder tasks. Resource awareness is limited, as extending recovery time or budgets yields only modest improvements and changing collaboration costs has minimal effect. The study concludes that advancing CSE requires stronger collaboration initiative, better communication strategies, and more adaptive, resource-aware recovery capabilities.",,,,
Investigating_Reproducibility_Challenges_in_LLM_Bugfixing_on_the_HumanEvalFix_Benchmark.pdf,"This study analyzes reproducibility challenges in LLM bugfixing on the HumanEvalFix benchmark by re-evaluating 12 models across four families (DeepSeekCoder, CodeGemma, CodeLlama, WizardCoder) in various sizes and tunings. They found 35 unique reported results across studies but could reproduce only 12, highlighting substantial inconsistencies likely due to differing evaluation setups. Key factors driving differences include confusing base vs. instruction-tuned variants, prompt templates, and maximum generation length; decoding strategy also matters, with 4-bit quantization significantly reducing performance, while precision and 8-bit quantization have smaller effects. Through a thorough grid search and analysis of GitHub issues, they show misconfigurations such as wrong prompts, incorrect model variants, or too short generation lengths explain many discrepancies. They propose a standardized evaluation checklist (consistent precision, avoid quantization, use author-recommended prompts, greedy decoding or enough samples, adequate generation length, exact model naming/version, and transparent evaluation details) to improve reproducibility. They conclude that reproducibility problems extend to other benchmarks like MBPP and stress careful reporting of evaluation settings and model variants for reliable model comparisons.",,,,
Ctibench__A_benchmark_for_evaluating_llms_in_cyber_threat_intelligence.pdf,"CTIBench is a CTI-focused benchmark designed to evaluate how well large language models handle cyber threat intelligence tasks. It comprises five tasks and datasets: CTI-MCQ (CTI knowledge questions), CTI-RCM (mapping CVE to CWE), CTI-VSP (predicting CVSS v3 base metrics), CTI-ATE (extracting MITRE ATT&CK techniques), and CTI-TAA (threat actor attribution). The study evaluates five LLMs—GPT-4, GPT-3.5, Gemini-1.5, LLAMA-70B, and LLAMA-8B—using zero-shot prompts. Results show GPT-4 generally outperforms others across tasks, Gemini-1.5 leads in CTI-VSP, and LLAMA-70B is competitive on several tasks while LLAMA-8B lags; knowledge cutoffs also reveal reliance on memorization for some tasks like CTI-TAA. The authors argue that CTIBench reveals strengths and weaknesses of current models in CTI and provides a public dataset and code base to accelerate CTI research and incident-response automation. Limitations include scope, language (English-only), and potential for misuse, with future work aiming to broaden tasks and multilingual evaluations.",,,,
SECURE__Benchmarking_Large_Language_Models_for_Cybersecurity.pdf,"Answer:
The paper presents SECURE, a cybersecurity-focused benchmark to evaluate LLMs’ reliability in ICS-relevant scenarios, assessing knowledge extraction, understanding, and reasoning across six datasets (MAET, CWET, KCV, VOOD, RERT, CPST). These datasets draw from MITRE ATT&CK, CWE, CVEs (2024), and CISA advisories to test memory-based retrieval, comprehension of contextual information, and complex risk/solution reasoning. Seven state-of-the-art LLMs (both open and closed) are evaluated across tasks using metrics like accuracy, ROUGE-L, and MAD, with a Retrieval-Augmented Generation variant also explored. Results show closed models (notably ChatGPT-4 and Gemini-Pro) generally outperform open models, especially on extraction and understanding tasks, while VOOD highlights the benefits of robust out-of-distribution detection. The study also finds that prompting for step-by-step reasoning can improve performance at the cost of higher inference time, indicating model-specific calibration needs. Finally, the authors validate data quality, discuss limitations, and release the benchmark datasets and framework publicly to promote domain-specific evaluation of cyber-advisory LLMs.",,,,
Kernelbench__Can_llms_write_efficient_gpu_kernels_.pdf,"KernelBench is an open-source framework that evaluates whether language models can generate fast and correct GPU kernels for 250 real-world PyTorch workloads, ranging from single operations to full architectures. It defines a fastp metric (the fraction of tasks that are correct and speed up execution over a PyTorch baseline by a threshold p), focusing on p = 1 to assess practical improvements. Across one-shot baselines, frontier models perform poorly, with less than 20% of tasks faster than the baseline and many kernels suffering execution or correctness errors. The study finds that while LMs can sometimes propose algorithmic optimizations (fusion, tiling, memory management), hardware-specific gains are rare and generalization across GPUs is limited. Importantly, leveraging execution and profiler feedback in iterative refinement dramatically boosts performance and correctness, e.g., certain models reach 72% fastp at Level 2 with feedback, and some tasks exceed 90% correctness within 10 refinement turns. Repeated sampling also helps, though results depend on the base model, and hardware-aware prompts provide limited consistent gains. Overall, KernelBench shows substantial room for improvement but demonstrates clear potential for LM-assisted kernel generation when combined with feedback, better data, and alternative tooling beyond CUDA.",,,,
Why_Personalizing_Deep_Learning-Based_Code_Completion_Tools_Matters.pdf,"The article investigates whether fine-tuning deep learning–based code completion models to an organization’s or a developer’s code base improves performance. It evaluates two model families (T5 small/large and Code Llama 7B) across two organizations (Apache and Spring) and top developers, using developer-specific and organization-specific datasets derived from real commits. Baselines are generic models trained on broad public code, and performance is measured with Exact Match and CrystalBLEU, with careful time-based data splits to avoid leakage. The study finds that organization-specific fine-tuning consistently boosts performance across models and organizations, while developer-specific gains are present but limited by data availability. Moreover, organization-specific models can achieve similar completion quality to much larger generic models, implying significant cost savings. A cost-effectiveness analysis shows that a small, personalized T5 model can match the performance of a tenfold larger generic model. Overall, the work concludes that personalization—especially at the organization level—meaningfully improves code-completion tools and can reduce deployment and inference costs.",,,,
Repairbench__Leaderboard_of_frontier_models_for_program_repair.pdf,"RepairBench is a leaderboard for AI-driven program repair that evaluates frontier models in a frequent, standardized, and execution-based manner. It uses two real-world benchmarks, Defects4J v2 and GitBug-Java, containing executable tests and ground-truth human patches to verify patches beyond syntactic matching. The evaluation metrics are Plausible@1 (test-suite-based correctness) and AST Match@1 (syntactic similarity), with Plausible@1 serving as the default ranking metric. The methodology standardizes prompts (zero-shot, single-function bugs, non-iterative) and requires the model to return the repaired function inside a code block extracted from the first code snippet. RepairBench targets frontier models accessible via APIs under a defined cost cap and publicly shares prompts, patches, and code to enable reproducibility, with plans to add SWE-Bench in a future update. The authors also address benchmark leakage and argue that execution-based evaluation provides a robust measure of repair capability, aiming to track progress in program repair over time.",,,,
Harnessing_Large_Language_Models_for_Curated_Code_Reviews.pdf,"The paper shows that large code review datasets are noisy, with uncivil, irrelevant, and unclear comments that hinder AI-driven code review. It introduces an evaluation framework in which a large language model acts as a judge to categorize comments by Type, Nature, and Civility and to score them on Relevance, Clarity, and Conciseness. The authors then build CuRev, a curated code-review dataset by filtering out low-relevance comments (threshold <4) and reformulating the rest with Llama-3.1-70B to improve civility, clarity, and conciseness, resulting in 170,718 samples that are 100% civil and more prescriptive and clearer than the original. They demonstrate that cu ration shifts comments toward more actionable guidance and improves data quality, with curated comments being more useful for downstream tasks. In downstream experiments, models fine-tuned on CuRev achieve a 46% improvement in BLEU for automated comment generation and a 22% improvement in CodeBLEU for automated code refinement compared to models trained on the original data. The paper concludes that automated data curation substantially enhances LLM performance for code-review tasks, and provides access to the dataset and replication materials.",,,,
Coding_Agents_with_Multimodal_Browsing_are_Generalist_Problem_Solvers.pdf,"OpenHands-Versa is a generalist AI agent built on the OpenHands framework, combining coding, multimodal web browsing, and information access as its core capabilities. It enhances OpenHands with a visual browser using Set-of-Marks prompts, a browsing condenser to trim observations, and API-based web search (Tavily/Exa/Brave) to access up-to-date information and avoid CAPTCHAs. The approach relies on a single agent with broad tools for writing and debugging code, browsing, searching, and processing multimodal files, without domain-specific sub-agents. Across three benchmarks—GAIA, SWE-Bench Multimodal, and The Agent Company—it achieves state-of-the-art or competitive results, outperforming prior open-source baselines by notable margins. The authors argue that simple, generalist tool sets can generalize better than complex multi-agent systems tailored to individual domains. They also discuss limitations such as CAPTCHA barriers, potential hallucinations from search summaries, loops, and incomplete tests, supported by CPU-only experiments with Claude backbones.",,,,
DeepCodeGraph__A_Language_Model_for_Compile-Time_Resource_Optimization_Using_Masked_Graph_Autoencoders.pdf,"DeepCodeGraph (DCG) is a graph-based language model designed to optimize compile-time resource decisions for heterogeneous hardware, pre-trained with a Masked Graph Autoencoder (MGAE). It leverages a large DCG dataset (~100k graph samples of OpenCL/C/C++/LLVM-IR code) and uses an extended ProGraML graph with BACK edges to capture data dependencies. The MGAE pre-training masks random nodes (with dynamic and complex masking) and learns to reconstruct them via a node-token classification objective, assisted by a momentum-based target network to stabilize training. After pre-training, DCG is fine-tuned on two tasks: heterogeneous device mapping and thread block size prediction, combining graph representations with small auxiliary inputs and passing them through an MLP classifier. DCG achieves state-of-the-art performance among graph-based models, with average DevMap accuracy around 0.87 across devices and LS-CAT top-3 accuracy around 0.19, outperforming previous methods by about 3%. The work demonstrates that MGAE-pretrained graph LMs provide transferable, effective representations for compile-time optimization, and it introduces a large, diverse LLVM-IR–based dataset to support this approach.",,,,
Search-based_llms_for_code_optimization.pdf,"SBLLM is a framework that treats code optimization as a search problem by integrating large language models (LLMs) with evolutionary search to iteratively discover more efficient optimization methods. It has three core modules: (1) execution-based representative sample selection using fitness (accuracy and speedup) to pilot generation; (2) adaptive optimization pattern retrieval that uses BM25-based similarity and fine-grained AST-diff patterns to provide targeted hints; and (3) GO-COT prompting that employs crossover and mutation-inspired reasoning to combine optimization methods. The process starts from initial seed optimizations and repeats sampling, pattern retrieval, and LLM prompting until convergence, guiding LLMs toward better solutions. Experiments on the PIE dataset (Python and C++) across CodeLlama, Gemini, ChatGPT, and GPT-4 show SBLLM outperforms baseline prompting methods in both correctness and speedup, with notable gains up to around 209% speedups on Python. Ablation studies indicate each component contributes meaningfully, with pattern retrieval and GO-COT prompting being especially impactful. Overall, SBLLM demonstrates that a search-based, iterative refinement approach can yield significantly faster and correct code compared to traditional one-shot prompting methods.",,,,
LLM-KG-Bench_3_0__A_compass_for_semantic_technology_capabilities_in_the_ocean_of_LLMs.pdf,"LLM-KG-Bench 3.0 is an open-source framework for automated evaluation of Large Language Models on Semantic Web and Knowledge Graph Engineering tasks, featuring a major update to its Task API and extensible task set. It includes RDF-focused tasks (RDF syntax repair, RDF analytics, fact extraction, and graph relations) and SPARQL tasks across multiple serializations (Turtle, JSON-LD, RDF/XML, N-Triples), with encrypted task data to prevent leakage. The framework provides modular task classes, a prompt-answer-evaluate loop, and multiple LLM connectors (OpenAI, Google Gemini, Anthropic Claude, vLLM) for broad model support. It was used to evaluate 30+ open and proprietary LLMs, generating a large dataset and model cards, and to visualize results with capability compass plots across dimensions like brevity, RDF syntax, RDF analytics, and SPARQL syntax/semantics. It also offers built-in analytics, score aggregation, and statistical comparisons (e.g., format preferences between Turtle and JSON-LD). The authors emphasize its focus on KG-related tasks as a complement to existing leaderboards and publish the code and results openly (GitHub and Zenodo). Future work targets adding new tasks and deeper analyses to broaden model coverage and evaluation tooling.",,,,
Context-aware_prompting_for_LLM-based_program_repair.pdf,"This paper tackles the limitations of traditional APR and DL-based methods by using context-aware prompting with large language models (LLMs) to improve repair effectiveness. The authors introduce CodeCorrector, which follows a developer-like Chain-of-Thought: it infers a repair direction from the failure message, selects repair-direction–relevant context (local and global), and generates patches via adaptive prompts. Local context completion enriches the buggy function, while global context selection uses an AST-derived candidate repository to pick direction-specific context and reduce noise. Patch generation employs iterative prompts and a validate loop that compiles and tests patches, feeding back errors and stopping when a plausible patch passes tests and is semantically examined. On Defects4J v1.2 and v2.0, CodeCorrector fixes 87 out of 255 bugs and 63 out of 228 bugs respectively, including 14 and 24 unique bugs that no baseline could fix, outperforming state-of-the-art baselines such as ChatRepair and GAMMA. Ablation studies show that repair directions and global context selection substantially contribute to its performance, with large improvements over base LLMs (GPT-3.5 and GPT-4). The work demonstrates that adaptive, context-aware prompting can significantly boost LLM-based automated program repair, and the authors provide public code for replication.",,,,
LogUpdater__Automated_Detection_and_Repair_of_Specific_Defects_in_Logging_Statements.pdf,"The article identifies four factual defects in logging statements—statement-code inconsistency, static-dynamic inconsistency, temporal relation inconsistency, and readability issues—and shows these defects account for a substantial portion of log-centric commits through a pilot study across 641 GitHub Java repositories. It proposes LogUpdater, a two-stage framework that offline trains a similarity-based detector on synthetic defective statements and online detects and automatically updates defective logs. Defective statements are synthesized using three mutation strategies: word-level edits for readability, tense changes for temporal issues, and LLM-driven semantic mutations for code-semantic mismatches. In the online phase, a defect-type classifier guides an LLM-based updater, with a checker vetting updates and a BM25-based retrieval of past log changes to provide context-aware, project-sensitive repairs. The framework is evaluated on three datasets, achieving a 0.625 F1 in defect detection, with notable improvements in static text and dynamic variable updates (48.12% and 24.90%), and a 61.49% success rate on new projects, with 25 of 40 proposed changes merged across 11 projects. The authors also open-source their code, prompts, and datasets to support reproducibility.",,,,
Just-in-time_detection_of_silent_security_patches.pdf,"- The paper tackles the problem of silent security patches in open-source software, which often lack advisories and can lead to n-day attacks if not detected promptly. 
- It introduces llmda, a multi-modal framework that augments patches with LLM-generated explanations and fuses code, descriptions, and instructions to improve security relevance detection. 
- The approach centers on three components: (1) LLM-generated explanations to provide semantic context, (2) PT-Former to align and fuse multi-modal embeddings across local and global contexts, and (3) stochastic batch contrastive learning (SBCL) to refine decision boundaries. 
- Patch representations are built from CodeT5+ for code and LLaMa-7b for text, with an instruction embedding and cross-attention to align modalities, producing a unified patch embedding for classification. 
- Evaluations on PatchDB and SPI-DB against strong baselines (e.g., GraphSPD, TwinRNN, GPT, CodeT5, VulFixMiner) show state-of-the-art performance, with LLmda achieving higher AUC, accuracy, F1, and recall, including up to a 42% improvement in F1 over GraphSPD. 
- Ablation studies and cross-dataset experiments demonstrate the contributions of the explanations, PT-Former, and SBCL, and indicate good generalization across diverse OSS ecosystems.",,,,
INDICT__Code_generation_with_internal_dialogues_of_critiques_for_both_security_and_helpfulness.pdf,"INDICT introduces a dual-critic framework for code generation, featuring a safety-driven critic and a helpfulness-driven critic that autonomously generate grounded critiques using external knowledge, code snippets, web search, and code interpreters. The two critics collaborate with the code-generating actor, revising outputs conditioned on the critiques to improve safety and usefulness. The system applies both preemptive feedback (during initial generation) and post-hoc feedback (after execution), integrating execution results to further refine the code. Across 8 tasks in 8 languages and models from 7B to 70B parameters, INDICT achieves consistent improvements in safety and helpfulness, reaching state-of-the-art safety on multiple benchmarks and strong gains in usefulness. Ablation studies show that both critics and external tools are essential, with preemptive safety contributing notably and the combination yielding the best overall performance. The approach generalizes to open-ended tasks beyond code, and the authors provide code and demonstrate robustness across varying model sizes.",,,,
ObscuraCoder__Powering_Efficient_Code_LM_Pre-Training_Via_Obfuscation_Grounding.pdf,"The authors argue that Code-LMs struggle to disentangle syntactic and semantic aspects of code and face a data bottleneck, motivating obfuscation-based grounding as a new pre-training objective. They introduce ObscuraX, a ~55 million pair dataset of source code and obfuscated code across seven languages, and ObscuraCoder, a family of Code-LMs trained with a bidirectional translation objective between source and obfuscated code plus standard language-modeling on unpaired data. Obfuscation renames identifiers to generic tokens (e.g., VAR_n, FUNC_n, CLASS_n) and, in about 25% of samples, obfuscates imports to improve API understanding, with two sentinel tokens facilitating translation in either direction. The models are evaluated on CodeXGLUE defect detection, ReCode robustness, BigCodeBench library-oriented generation, Multipl-E multilingual completion, and CommitChronicle summarization. Results show consistent gains over vanilla autoregressive LMs, with larger benefits as model size increases, and ObscuraCoder often matching or beating frontier models on several tasks, especially in zero-shot settings. Compared to DOBF, ObscuraCoder generally performs better across the board, underscoring the value of end-to-end obfuscation-grounded pre-training. Overall, the work demonstrates that obfuscation-based pre-training improves both syntactic and semantic code understanding and enhances multilingual and API-focused capabilities, offering a data-efficient path forward for Code-LMs.",,,,
ELABORATION__A_Comprehensive_Benchmark_on_Human-LLM_Competitive_Programming.pdf,"ELABORATION is a comprehensive benchmark for Human-LLM competitive programming that introduces a four-stage taxonomy of human feedback (problem comprehension, solution planning, code generation, and debugging) and ELABORATIONSET, a dataset of 8,320 Codeforces/AtCoder problems annotated to support large-scale simulated and real-human feedback. It enables evaluation of human-LLM collaboration across the entire programming process, using both LLM-based simulators and real participants. Findings show that LLMs alone struggle, especially on unseen or hard problems, but human feedback across stages substantially improves performance, with coding-stage guidance yielding the largest gains (though at higher token cost) and planning-stage feedback offering potential cost-effectiveness. A real-human debugging study shows that humans dramatically improve bug identification and resolution, boosting Pass@1 by a substantial margin compared to automatic debugging. The authors provide open-source resources (dataset and code) and discuss limitations, including prompt sensitivity and domain specificity to competitive programming. They conclude that ELABORATION offers a robust foundation for evaluating and guiding future improvements in human-LLM collaboration for programming tasks.",,,,
UCP__a_unified_framework_for_code_generation_with_pseudocode-based_multi-task_learning_and_reinforcement_alignment__Y__Wen_et_al_.pdf,"The article presents UCP, a unified framework that improves open-source language models for code generation by using pseudocode as a structured intermediate representation. It introduces PoT (Pseudocode-of-Thought), a set of tasks that generate pseudocode before code, bridging natural language and programming languages and enabling six aligned tasks: Text2Code, Text2Multilingual Code, PoT, Code2Pseudocode, Unit Test, and Code Completion. The framework combines a dynamic loss-balanced multi-task joint fine-tuning strategy with SimPO-based reinforcement learning and a cyclic self-filling inference to maintain alignment across pre-training, fine-tuning, RL, and inference. Datasets are built with pseudocode-augmented data and multi-granular code completion data, with pseudocode exposure enhancing interpretability for tools like AI-powered IDEs. Experiments on Qwen2.5-Coder-7B-Instruct show improvements over baselines, e.g., HumanEval pass@1 rising from 84.15 to 87.80 and LiveCodeBench pass@1 from 25.4 to 27.2. The framework aims to boost code generation capabilities of open-source LLMs and supports cross-language, multi-task coding with practical benefits for developers.",,,,
Synthesizing_Software_Engineering_Data_in_a_Test-Driven_Manner.pdf,"SWE-Flow presents a data synthesis framework grounded in Test-Driven Development (TDD) that automatically infers incremental development steps from unit tests using a Runtime Dependency Graph (RDG). For each development step, it generates three training artifacts: a partial codebase, the corresponding unit tests (requirements), and a ground-truth diff (patch or replace format) guiding the implementation. It constructs a development schedule that ensures incremental building from scratch and uses skeletonization to produce verifiable ground-truth solutions. The authors created SWE-Flow-Bench, collecting 16,061 training instances and 2,020 test instances from 74 open-source projects to evaluate LLMs on TDD-oriented tasks. Fine-tuning Qwen2.5-Coder-32B-Instruct on SWE-Flow data yielded SF-Coder-32B-Instruct, which showed improved performance on TDD-style tasks compared to baselines. All code, data, models, and Docker images are released to support further research, with future directions including scaling data and integrating reinforcement learning.",,,,
Mercury__A_code_efficiency_benchmark_for_code_large_language_models.pdf,"Mercury is the first benchmark dedicated to evaluating code efficiency in Code LLMs, measuring runtime performance across 1,889 Python tasks using task-specific runtime distributions. It introduces Beyond, a runtime percentile–weighted Pass score that normalizes absolute runtimes and reflects both functional correctness and efficiency, along with reporting Pass and the Gap between Beyond and Pass. The dataset includes Mercury-eval (256 tasks for evaluation) and Mercury-train for fine-tuning, with three LeetCode-based difficulty levels and per-task test-case generators to ensure coverage. Experiments on 10 Code LLMs show that Direct Preference Optimization (DPO) generally improves code efficiency and often preserves or enhances functional correctness, while Supervised Fine-Tuning (SFT) can degrade correctness for larger models. Results indicate larger models tend to achieve higher Pass scores, and DPO can narrow the Gap between Beyond and Pass, particularly for models above 15B parameters; simple prompt engineering also helps some models. The paper notes limitations like assuming uniform runtimes, potential data contamination, and sandboxed evaluation, but argues Mercury reveals substantial room for efficiency gains and provides an open framework for future work.",,,,
A_comparative_analysis_on_using_GPT_and_BERT_for_automated_vulnerability_scoring.pdf,"- The paper provides a comprehensive comparison of GPT and BERT for automated vulnerability scoring using vulnerability descriptions from public datasets (NVD/CVE). 
- It outlines the architectural differences: GPT is a decoder-only autoregressive model, while BERT is an encoder-only bidirectional model; both are Transformer-based and differ in attention masking and training objectives (causal language modeling vs masked language modeling). 
- The study investigates using these models for text classification to predict CVSS metrics and explores hybrid GPT–BERT architectures that combine generative and comprehension strengths. 
- Experiments evaluate standard metrics (accuracy, precision, recall, F1) and examine the impact of hyperparameters, pre-training strategies, and fine-tuning, including domain-specific continual pre-training. 
- The work situates itself among related research showing mixed results across tasks and notes the potential and challenges of LLMs in vulnerability assessment, including computational costs and deployment considerations. 
- The findings suggest GPT and BERT have complementary strengths and that hybrid approaches can outperform single-model setups, providing guidance on model choice and deployment for automated vulnerability scoring.",,,,
Repairllama__Efficient_representations_and_fine-tuned_adapters_for_program_repair.pdf,"RepairLLaMA is a program repair approach that combines parameter-efficient fine-tuning (LoRA) with APR-specific code representations to adapt large language models for bug fixing. It introduces a novel repair adapter (about 4M parameters) that plugs into an open-source CodeLlama-7B model and learns to transform rich buggy-code representations into correct patches, while reducing memory and overfitting. The key innovations include fault-localization signals modeled as regions and an input/output representation design (IR1–IR4 and OR1–OR4), with IR4xOR2 identified as the best pairing for leveraging both localized fault context and the in-fill learning objective. Trained on Megadiff data and evaluated on Defects4J v2, HumanEval-Java, and GitBug-Java, RepairLLaMA achieves state-of-the-art repair performance, correctly fixing 144 Defects4J v2 bugs, 109 HumanEval-Java bugs, and 20 GitBug-Java bugs, outperforming strong baselines including GPT-4. The study also shows that parameter-efficient fine-tuning can match or surpass full fine-tuning in this domain and that the approach generalizes beyond the tuning distribution (RQ2 and RQ3). The authors provide open-source code, models, and artifacts on GitHub to support reproducibility.",,,,
EDITLORD__Learning_Code_Transformation_Rules_for_Code_Editing.pdf,"- EDITLORD introduces a framework that makes code edits explicit by learning a concise, modular meta-rule set of transformations (R) from training data, enabling rule-based code editing while preserving original functionality. 
- It uses a three-step pipeline: data-driven inductive rule discovery to derive R per sample, functional specification discovery to capture intended input-output behavior, and rule-based editing to create an augmented training set for finetuning or prompting. 
- For each training sample, the model learns per-sample editing rules Ri and functional specs si, then predicts the post-edit code yi from the pre-edit xi conditioned on si and Ri (P(yi|xi) via si, Ri). 
- R is iteratively refined using ADD, MERGE, and PRUNE operations to ensure the rules are balanced, reusable, and not overly generic or specific. 
- Empirical results across performance optimization, decompilation readability, and security hardening show EDITLORD improves editing performance, robustness against semantics-preserving edits, and functional correctness over end-to-end baselines, with ablations highlighting the value of both rule learning and functional-spec learning. 
- The approach also supports zero-shot and iterative refinement editing modes, benefits from human augmentation, and, while promising, does not provide formal correctness guarantees.",,,,
ReAPR__Automatic_program_repair_via_retrieval-augmented_large_language_models.pdf,"ReAPR is a retrieval-augmented framework for Automatic Program Repair (APR) that combines Large Language Models with historical bug-fix data to reduce hallucinations and improve repair quality for Java functions. It builds a high-quality retrieval database of bug-fix pairs (364,063 after deduplication) and uses both BM25 (sparse) and Dense Passage Retrieval (DPR) to fetch relevant pairs, which are then incorporated into a prompt for decoder-only LLMs to generate fixes. The approach is evaluated on Defects4j 2.0 and GitBug-Java benchmarks, comparing against state-of-the-art baselines and varying model sizes. Results show that retrieval-augmented generation generally outperforms non-retrieval baselines, with dense retrieval providing larger gains, especially for larger models like CodeLlama and DeepSeekCoder; no-retrieval performance can still be competitive for smaller models. However, retrieval effectiveness depends on the retrieval corpus quality; on GitBug-Java, where related bug-fix data are scarce, retrieval can offer little or even negative benefits. The work demonstrates the potential of RAG to enhance APR performance and provides open-source code for replication.",,,,
On_the_Effectiveness_of_LLM-as-a-judge_for_Code_Generation_and_Summarization.pdf,"Crupi et al. study whether large language models (LLMs) can act as judges for two code-related tasks: code generation and code summarization. They evaluate eight LLMs (varying sizes) on code generation (1,405 Java and 1,281 Python functions) and on code summarization (about 1,163 summaries) by having the LLMs judge outputs and comparing those judgments to ground truth from unit tests or human ratings. The results show GPT-4-turbo is the best judge overall, but even it frequently misjudges: for Java it correctly flags many valid solutions yet misses about half of the incorrect ones, with Cohen’s kappa around 0.21 (Java) and 0.10 (Python), indicating only modest agreement with ground truth. In general, larger models outperform smaller ones, while several small models perform poorly or fail to output judgments. For code summarization, GPT-4-turbo again leads in agreement with human judgments (Krippendorff’s alpha up to 0.81 for content adequacy in Java and 0.56–0.69 for other criteria), whereas some models (e.g., DeepSeek Coder) struggle to provide reliable judgments. The study concludes that LLMs-as-a-judge show promise for scalable evaluation but cannot yet replace human judgments due to substantial misjudgments and biases, and it provides replication data and best-practice prompts for future work.",,,,
ProSec__Fortifying_Code_LLMs_with_Proactive_Security_Alignment.pdf,"PROSEC introduces a proactive security alignment approach for code LLMs that synthesizes vulnerability-inducing instructions from CWEs and corresponding fixes, creating a security-focused alignment dataset used in an offline preference-optimization stage after post-training to preserve utility. It addresses data sparsity and post-training integration by generating two data pools—Dsec (vulnerable instances) and Dnorm (normal utility-preserving instances)—and applying heuristic and training-dynamics data selection to curate high-quality samples. The alignment uses a Bradley-Terry preference objective and is designed to run after existing post-training, without altering earlier stages. Empirically, PROSEC-synthesized instructions trigger about 25 times more vulnerable code than standard datasets, and the resulting dataset is roughly 7x larger than SafeCoder. On evaluated models (e.g., Phi3-mini-Inst and CodeLlama-7B-Inst), PROSEC improves security by about 25.2% to 35.4% over SafeCoder without measurable utility loss, with results generalizing across languages and vulnerability types. Additional findings show that instruction clustering improves diversity, Dnorm sampling helps preserve utility, and the training-dynamics-based selection effectively balances security and usefulness. The paper concludes that PROSEC provides a scalable, model- and pipeline-friendly offline security alignment method, with future work exploring online reinforcement learning and multi-step reasoning.",,,,
Di-bench__Benchmarking_large_language_models_on_dependency_inference_with_testable_repositories_at_scale.pdf,"DI-B ENCH is the first large-scale benchmark focused on dependency inference for repository-level LLM evaluation, featuring 581 real-world repositories across Python, Rust, C#, and JavaScript. It couples textual accuracy with an automatic, CI-based execution evaluation by masking dependency sections in build files and re-running the project’s tests via GitHub Actions, enabling end-to-end assessment of executability. Key findings show that even the best models achieve only 42.9% executability on regular repos, with performance dropping for larger repos and more dependencies; hallucinations and erroneous dependency metadata are major bottlenecks. Providing oracle dependency metadata dramatically improves executability (e.g., Python +28.4%, Rust +246%, JavaScript +50%), highlighting the importance of accurate dependency information. The study also reveals that longer context and bigger codebases challenge current LLMs, and while GPT-4o leads performance, open-source models lag behind in overall executability. DI-B ENCH supplies a scalable dataset and a dual evaluation framework to advance robust, end-to-end repository-level software synthesis, while acknowledging limitations such as a limited model set, black-box results, and non-exhaustive test coverage.",,,,
Just-in-time_detection_of_silent_security_patche.pdf,"The article introduces llmda, a framework for just-in-time detection of silent security patches in open-source software, addressing patches that arrive without explicit advisories. llmda combines three key innovations: (1) LLM-generated explanations to enrich patches with semantic intent, (2) PT-Former to align and fuse multi-modal inputs (patch code, descriptions, explanations, and instructions) into a unified embedding, and (3) stochastic batch contrastive learning (SBCL) to sharpen classification boundaries. Patch representations are built from CodeT5+ for code and LLaMa-7b for text, with a task-specific instruction guiding the learning process. The approach trains a binary classifier on fused embeddings and uses SBCL alongside a BCE loss to improve robustness and precision. Evaluations on PatchDB and SPI-DB show state-of-the-art performance, with up to 42% improvement in F1 over GraphSPD and strong recall metrics. Ablation studies demonstrate that each component (LLM explanations, PT-Former, and SBCL) substantially contributes to performance. Overall, llmda provides a scalable, multi-modal, and interpretable solution for detecting silent security patches across diverse OSS ecosystems.",,,,
Do_LLMs_consider_security__an_empirical_study_on_responses_to_programming_questions.pdf,"This paper investigates whether three popular LLMs (GPT-4, Claude 3, and Llama 3) exhibit security awareness when prompted with vulnerable code from Stack Overflow. It creates two datasets—Mentions-Dataset (SO answers already mentioning vulnerabilities) and Transformed-Dataset (refactored code with no security mentions) to test detection capabilities and generalization. The study analyzes 900 model responses (300 questions × 3 models) to determine if the LLMs warn about vulnerabilities and whether their warnings cover causes, exploits, and fixes. Results show limited security warnings overall (12.6%–40%), with GPT-4 achieving the best performance, especially on Mentions-Dataset; performance declines on the Transformed-Dataset, suggesting training-data influence. When warnings appear, the models often provide more detailed information about causes, exploits, and fixes than typical SO responses, with common vulnerabilities including authentication bypass, XSS, and SQL injection. The authors also explore prompt-engineering strategies and introduce a CLI tool that integrates CodeQL to improve LLM security responses, arguing that while there is potential to raise developer awareness, consistent and thorough vulnerability warnings remain an area for improvement in SE.",,,,
RECoRD__A_Multi-Agent_LLM_Framework_for_Reverse_Engineering_Codebase_to_Relational_Diagram.pdf,"RECoRD is a multi-agent framework that converts production code into interpretable causal graphs (relational diagrams) by fusing deterministic program analysis with large-language models. It uses an entity extraction agent (AST and data-flow analysis with LLM refinement and optional human filtering) and a relation extraction agent (prompt-engineered inference enhanced by reinforcement fine-tuning). The relation extractor is trained with reinforcement fine-tuning (RFT) using Direct Preference Optimization and LoRA adapters, learning from subgraphs with ground-truth edges to prefer correct causal relations. Across use cases like NewsVendor, MiniSCOT, and Black-Scholes, RFT-trained models significantly outperform foundation models, with notable improvements in graph accuracy and generalization to new codebases. The approach highlights the importance of accurate entity recall and combines AST guidance with lightweight human checks to balance precision and recall. RECoRD enables scalable automation for building causal graphs from code, supporting debugging, optimization, and risk management, with future work aimed at scaling to larger repositories and more complex algorithmic structures.",,,,
An_empirical_study_on_capability_of_large_language_models_in_understanding_code_semantics.pdf,"The paper introduces EMPICA, a framework to systematically evaluate code LLMs’ understanding of code semantics by applying controlled semantic-preserving and semantic-non-preserving transformations to input code. It focuses on two key semantic relations—control dependence and data dependence—using eight transformation operators and assesses robustness (consistency) and sensitivity (change) across three code-understanding tasks: code summarization, method/function name prediction, and output prediction. The evaluation uses code generated by models on HumanEval seeds in Java and Python, testing several SOTA(code LLMs): DeepSeek-Coder, Code Llama, MagicCoder, and GPT-3.5. Results show that code summarization is highly robust to SP transformations (around 0.9) and only weakly sensitive to SNP changes (~0.1), while method-name and output predictions exhibit greater sensitivity and less robustness to SNP transformations (e.g., modest changes in predicted names and a substantial portion of differing outputs under SNP). Overall, the study suggests that current code LLMs rely more on surface patterns and general code structure than deep semantic understanding, indicating a need to improve their sensitivity to semantic changes. The authors also examine model size effects, correlations with correctness, and semantic-inference capabilities, and provide a reproducible resource for future research.",,,,
Codeultrafeedback__An_llm-as-a-judge_dataset_for_aligning_large_language_models_to_coding_preferences.pdf,"CodeUltraFeedback introduces a dataset designed to evaluate how well LLMs align with coding preferences using an LLM-as-a-Judge framework. It contains 10,000 coding instructions and 40,000 responses from 14 LLMs, with each response rated and explained by GPT-3.5-Turbo across five coding preferences: instruction following, code explanation, code complexity and efficiency, code readability, and coding style. CodeUltraFeedback-Bench provides a 500-sample subset for rigorous, single-answer evaluation of alignment across models. The study finds that GPT-3.5-Turbo and GPT-4-Turbo generally outperform open-weight models, highlighting alignment gaps in open models. They show that supervised fine-tuning (SFT) and direct preference optimization (DPO) on CodeLlama-7B-Instruct can substantially improve alignment—often surpassing larger models—and also improve functional correctness on HumanEval+ when combined with efficient training (e.g., QLoRA on a RTX A5000). The authors release their data and prompts on GitHub to spur further research in LLM alignment and reinforcement learning from AI feedback for code.",,,,
M2rc-eval__Massively_multilingual_repository-level_code_completion_evaluation.pdf,"The authors introduce M2RC-EVAL, the first massively multilingual repository-level code completion benchmark, spanning 18 programming languages and featuring two fine-grained annotation types (bucket-level and semantic-level) derived from ASTs, plus cross-file repository context. They also release M2RC-INSTRUCT, a massively multilingual instruction-tuning corpus across the same languages to boost repository-level code completion performance. Data are collected from The Stack v2 with quality filters, yielding 50k files per language for M2RC-INSTRUCT and 100 validation plus 500 test samples per language for M2RC-EVAL. The study evaluates three code LLMs (StarCoder-7B, DeepSeekCoder-6.7B, Code Llama-7B) under baselines, retrieval, and retrieval-plus-tuning settings using EM, ES, and CodeBLEU metrics. Results show that cross-file context substantially improves performance, and fine-tuning on M2RC-INSTRUCT further enhances results, with smaller models benefiting significantly from instruction data; Python-only fine-tuning can also transfer to multilingual performance. The findings reveal performance variations across languages, bucket depths, and semantic levels, highlighting challenges in shallow AST nodes and language-specific structures. Overall, M2RC-EVAL and M2RC-INSTRUCT provide a comprehensive framework for evaluating and advancing multilingual, repository-aware code intelligence.",,,,
Revisiting_Chain-of-Thought_in_Code_Generation__Do_Language_Models_Need_to_Learn_Reasoning_before_Coding_.pdf,"- The paper reevaluates Chain-of-Thought (CoT) prompting in code generation and asks whether LLMs need to learn reasoning before coding. 
- It builds a synthetic 50k-pair dataset and compares four supervised fine-tuning strategies: Seed, Code without CoT, Code follow CoT (CoT before code), and Code precede CoT (code first, then CoT). 
- Across multiple benchmarks (e.g., HumanEval, MBPP, LiveCodeBench, BigCodeBench, LeetCode) and base models, they observe that Code precede CoT yields the best performance, with about a 9.86% relative improvement over Code follow CoT. 
- The authors argue that high-quality code can serve as a practical CoT, and traditional CoT should be viewed as an explanation of the code rather than its reasoning steps. 
- The findings generalize across model sizes and data sources, indicating robust gains and suggesting that forcing CoT before code can hinder learning while Code-first CoT improves generalization and efficiency. 
- Additional analyses show the importance of code signatures, that mixed-data strategies can hurt performance, and that the approach scales to more difficult tasks, reinforcing the recommendation to place code before CoT in SFT for code generation.",,,,
Bitsai-cr__Automated_code_review_via_llm_in_practice.pdf,"BitsAI-CR is a two-stage, LLM-based code review framework that uses RuleChecker to detect issues based on a taxonomy of 219 rules and ReviewFilter to verify those findings, complemented by context preparation and comment aggregation. It employs a data flywheel to continuously improve through annotated feedback, rule updates, and an Outdated Rate metric that measures how often flagged code lines are actually modified, enabling scalable evaluation. Offline results show the taxonomy-guided model achieves substantially higher precision (57.03% overall) than a base version (16.83%), and ablation confirms the essential role of ReviewFilter in reducing hallucinations. Among ReviewFilter reasoning patterns, Conclusion-First offers a strong balance of precision and speed, achieving high precision with reasonable inference time. In production at ByteDance, BitsAI-CR serves over 12k weekly active users, with Go language achieving an Outdated Rate of 26.7% by week 18 and steadily improving precision over time. User surveys and expert interviews indicate strong positive reception and practical impact, with retention sustaining at a substantial level, demonstrating the framework’s scalability for enterprise code review.",,,,
Post-Incorporating_Code_Structural_Knowledge_into_Pretrained_Models_via_ICL_for_Code_Translation.pdf,"- The paper addresses code translation with large language models and notes that handling code syntax remains challenging, while traditional syntax-aware methods are heavy and training-intensive. 
- It proposes a training-free, model-agnostic approach that post-incorporates code structural knowledge into pre-trained LLMs at test time via in-context learning (ICL) using CAST retrieval. 
- CAST is a surrogate objective for information coverage based on Abstract Syntax Trees (ASTs): it quantifies the fraction of test-code AST nodes covered by the subtrees present in the selected exemplars, using fingerprints and a co-occurrence matrix. 
- Maximizing CAST is NP-hard, but the paper proves the objective is monotone submodular, enabling a greedy algorithm with a (1−1/e) approximation guarantee and polynomial time. 
- The framework includes CAST-F (fixed exemplar count) and CAST-A (adaptive threshold) variants and is designed as a plug-in module used with various state-of-the-art LLMs. 
- Experiments show CAST improves code translation (and code summarization) performance across benchmarks, supporting the claim that code structural knowledge can be effectively post-incorporated at inference time, and that simply scaling model size or data does not guarantee emergent code syntax knowledge. 
- The authors present CAST as the first training-free, model-agnostic method to inject code syntactic knowledge into LLMs through ICL for code translation.",,,,
Insights_into_resource_utilization_of_code_small_language_models_serving_with_runtime_engines_and_execution_providers.pdf,"The article analyzes how deep learning serving configurations—combinations of runtime engines and execution providers—affect energy consumption, execution time, and computing-resource utilization during inference of code-generation Small Language Models (SLMs). Using 12 code-generation SLMs and short prompt auto-complete tasks derived from HumanEval, the study tests seven configurations formed by TORCH, ONNX, OV, and JIT with CPU or CUDA (excluding OV+CUDA). Experiments run on an RTX 4090 and Ryzen 9 7950X, with EnergiBridge, nvidia-smi, and a wattmeter to measure energy, time, CPU/GPU usage, and a replication package for reproducibility. Results show that CUDA-backed configurations consistently outperform CPU-backed ones in energy and time, with TORCH+CUDA providing the best overall energy efficiency and fastest inferences. The study also finds that ONNX and OV can offer strong CPU-based efficiency, while JIT can be competitive only in certain tasks and generally requires refinement for code-generation scenarios. The authors provide practical guidelines for practitioners, emphasize monitoring to avoid bottlenecks, and present a reproducible pipeline to support greener DL serving research.",,,,
Towards_reliable_evaluation_of_neural_program_repair_with_natural_robustness_testing.pdf,"The article argues that evaluating Neural Program Repair (NPR) robustness with semantic-preserving transformations often uses unnatural, unrealistic code changes that can mislead conclusions about NPR effectiveness. To address this, the authors advocate natural robustness testing, focusing on transformations that naturally occur in real-world software and validated through human studies. They derive naturalness criteria from interviews (focusing on readability and coding convention) and build TransformedDefects4J, a dataset of 1,098 semantic-preserving transformations applied to 220 Defects4J bugs, labeled by 10 developers. They find only about 60% of transformations are natural, with 19% deemed unnatural—especially at the statement level—highlighting the risk of relying on unnatural transformations for robustness evaluation. Their analysis shows that natural vs. all transformations yield different NPR outcomes: natural transformations cause prediction changes and substantial declines in both plausible and correct patch rates, and can shift the relative rankings of NPR methods. They also introduce an automated approach for assessing naturalness using cross-entropy and large language models, proposing Relative Naturalness Changes (RNCs) with an AUC of 0.7 to scale naturalness evaluation. The paper concludes that natural robustness testing should be part of NPR evaluation to obtain reliable results, and it provides replication data and a path for future research.",,,,
"Kodcode__A_diverse,_challenging,_and_verifiable_synthetic_dataset_for_coding.pdf","KODCODE is a large-scale synthetic coding dataset (447K question–solution–test triplets) designed to train coding LLMs with both diversity and verifiable correctness. It follows a three-step pipeline: (1) Coding Question Synthesis across 12 subsets to ensure broad coverage and varied difficulty; (2) Solution & Test Generation using a self-verification loop that enforces 100% branch coverage, with extra attempts for hard questions; and (3) Post-training Data Synthesis that rewrites questions into diverse formats and creates SFT data via a reasoning model with test-based reject sampling. The self-verification pipeline shows high reliability on MBPP and LiveCodeBench, and more attempts (pass@k) significantly improve success on challenging subsets. Models fine-tuned on KODCODE-SFT and further trained with RL achieve state-of-the-art results on HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench, often surpassing larger baseline models. Analyses indicate strong diversity, low benchmark contamination, and beneficial ablations for style conversion and hard-question exposure. Limitations include weaker performance on LiveCodeBench-Hard, with future work to scale harder problems, optimize data selection, and explore repository-level synthetic data.",,,,
Comprehensive_Fine-Tuning_Large_Language_Models_of_Code_for_Automated_Program_Repair.pdf,"- The article investigates comprehensive fine-tuning of large language models (LLMs) for automated program repair (APR), extending previous work from million-level to billion-level LLMs under an NMT-like fine-tuning paradigm. 
- It conducts two studies: an initial MILLION-level study of CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder with design-choice analysis (code abstraction, code representations CR1–CR4, and checkpoint selection) across bugs, vulnerabilities, and programming errors, and an extended study on five BILLION-level families (InCoder, CodeGeeX, CodeGen, StarCoder, CodeLlama) using PEFT methods (LoRA, AdaLoRA, IA3) plus full fine-tuning. 
- Results show million-level LLMs can outperform many prior APR tools and can repair multi-hunk defects, with design choices significantly impacting repair effectiveness. 
- In the billion-level study, larger models generally yield better repair capabilities but incur higher memory and time costs; LoRA consistently emerges as the best PEFT option, while AdaLoRA and IA3 offer mixed or limited gains. 
- The study also evaluates three novel repair strategies (TENURE, ITER, KATANA) on LLMs, finding they can generalize to LLMs but do not always surpass the basic NMT fine-tuning baseline. 
- The authors release 22 fine-tuned LLMs and provide guidance on PEFT for resource-constrained APR, highlighting limitations and suggesting directions to improve patch search efficiency and explore decoder-only models.",,,,
Efficient_program_optimization_through_knowledge-enhanced_LoRA_fine-tuning_of_large_language_models.pdf,"CodeOPT is a knowledge-enhanced, LoRA-based fine-tuning framework for optimizing C/C++ code using a decoder-only LLM (CodeLLaMA / LLaMA-3). It fine-tunes only a small optimization adapter (about 4–5 million parameters, <0.1% of the full model) to achieve code optimization efficiently and scalably. It also incorporates instruction-based fine-tuning by injecting standard C/C++ optimization guidelines (e.g., loop unrolling, inline expansion, constant folding, dead code elimination) as input to guide the model. Trained on a large open-source dataset (~312k training examples) and evaluated on 559 test samples from Codeforces, AIZU, and AtCoder, CodeOPT is benchmarked against SUPERSONIC, GPT-3.5-turbo, and GPT-4. The results show CodeOPT consistently achieves higher optimization rates and greater performance improvements, with the LoRA-plus-instruction combination yielding the best results, while full-parameter fine-tuning under the same data performs worse. The authors emphasize that CodeOPT provides high-quality optimizations with minimal changes to the original programs and can enable real-time, resource-efficient code optimization in practice.",,,,
Combining_Large_Language_Models_with_Static_Analyzers_for_Code_Review_Generation.pdf,"- The paper proposes a hybrid approach to code review generation by combining static analyzers (KBS) with large language models (LBS), using three integration points: data augmentation (DAT), retrieval-augmented generation (RAG), and naive concatenation of outputs (NCO).  
- They build baselines with Java data (PMD and Checkstyle as KBS; CodeLlama-7b fine-tuned with QLoRA as LBS) and create an augmented dataset Da by merging KBS and LBS outputs, filtered for quality using Llama3-70B.  
- DAT retrains the LBS on Da; RAG injects KBS outputs into the LLM’s prompts during inference; NCO concatenates KBS and LBS outputs after inference.  
- Evaluation on 1,245 samples shows RAG provides the highest accuracy among hybrids (though still below pure KBS), while DAT and NCO are less impactful on accuracy; in terms of coverage, DAT and especially RAG offer the best results, with RAG delivering top-tier coverage.  
- Llama3-70B’s judgments align substantially with human judgments (kappa = 0.72), supporting its use as an evaluator for the broader test set.  
- The authors conclude that combining KBS and LBS bridges precision and coverage in code review comments, with RAG being the most effective strategy, though generalizability beyond Java remains a consideration.",,,,
Enhancing_automated_program_repair_with_solution_design.pdf,"The article presents DRCodePilot, a design-rationale–driven approach to Automated Program Repair (APR) that leverages design rationales (DR) mined from issue logs to guide patch generation. It combines five phases: (1) DR acquisition via DRMiner, (2) defective-segment localization and an initial patch by GPT-4, (3) project-specific reference patches generated by a fine-tuned CodeT5P-220M, (4) identifier recommendations to avoid context-incorrect names, and (5) a final refined patch produced by GPT-4 using feedback. The authors evaluate on a benchmark of 938 Jira-GitHub issue-patch pairs from Flink and Solr, using full-match patches and CodeBLEU as metrics rather than test-based evaluation. DRCodePilot achieves substantially more full-match patches than strong baselines (e.g., 109 vs 23 in Flink; 18 vs 5 in Solr) and higher CodeBLEU scores, demonstrating the effectiveness of DR-guided repair. Ablation studies show that design rationales contribute the most to performance, with identifier recommendations and reference patches providing additional gains, and they also show that DR can improve other LLMs and that hand-annotated DRs further boost results. The work argues for a human-in-the-loop, project-aware APR workflow while noting limitations, such as imperfect DR extraction, long issue logs, lack of executable evaluation due to evolving dependencies, and the focus on two large open-source projects.",,,,
Humaneval_pro_and_mbpp_pro__Evaluating_large_language_models_on_self-invoking_code_generation.pdf,"The article introduces self-invoking code generation, where an LLM must solve a base problem and then reuse its own generated code to solve a related, more complex one, to assess progressive reasoning in coding. It builds two benchmarks, HumanEval Pro and MBPP Pro, plus BigCodeBench-Lite Pro, by generating self-invoking variants from existing datasets using DeepSeek-V2.5 and applying an iterative process (solution generation, test-case creation, execution, and manual review) to ensure high-quality, testable problems. Across 20+ LLMs, the authors find a substantial drop (about 10–15 percentage points in pass@1) on self-invoking tasks compared with traditional benchmarks, with instruction-tuned models offering only marginal improvements. Chain-of-thought prompting can modestly boost performance on these tasks, and error analysis shows common failures like AssertionError and NameError as major bottlenecks. BigCodeBench-Lite Pro yields similar challenges, with limited gains from instruction-tuning, underscoring the difficulty of self-invoking code reasoning. The authors conclude that these benchmarks reveal gaps in current models and call for new training approaches and broader multilingual benchmarks to enhance self-invoking code generation capabilities.",,,,
Are_large_language_models_memorizing_bug_benchmarks_.pdf,"- The paper investigates whether large language models memorize widely used bug benchmarks, leading to data leakage that inflates evaluation results. 
- They evaluate open-source models (Codegen-multi, CodeLlama, LLaMa 3.1, StarCoder, Gemma, Mistral) on bug benchmarks (Defects4J, BugsInPy, BugsCpp, GitBug-Java, SWEBench-Lite) and unseen 2024 GitHub repos. 
- Leakage is assessed using benchmark membership in TheStack, negative log-likelihood (NLL), and 5-gram accuracy. 
- Defects4J shows the strongest leakage signals across models, especially for older models like codegen-multi 6B; newer models such as LLaMa 3.1 exhibit reduced memorization. 
- Newer benchmarks (GitBug-Java, BugsInPy, BugsCpp) demonstrate lower leakage risk, suggesting researchers should supplement evaluations with these datasets. 
- Regression analysis indicates that more parameters and larger training budgets correlate with more memorization, though some large models can still reveal memorization in certain cases, underscoring the need for careful benchmark selection and leakage monitoring.",,,,