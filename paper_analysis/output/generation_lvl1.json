{"id":"pdf_0","text":"RepairBench: Leaderboard of Frontier Models\nfor Program Repair\nAndr\u00e9 Silva and Martin Monperrus\nKTH Royal Institute of Technology, Sweden\n{andreans, monperrus}@kth.se\nhttps:\/\/repairbench.github.io\/\nAbstract\nAI-driven program repair uses AI models to repair buggy software by producing\npatches. Rapid advancements in AI surely impact state-of-the-art performance\nof program repair. Yet, grasping this progress requires frequent and standardized\nevaluations. We propose RepairBench, a novel leaderboard for AI-driven program\nrepair. The key characteristics of RepairBench are: 1) it is execution-based: all\npatches are compiled and executed against a test suite, 2) it assesses frontier models\nin a frequent and standardized way. RepairBench leverages two high-quality\nbenchmarks, Defects4J and GitBug-Java, to evaluate frontier models against real-\nworld program repair tasks. We publicly release the evaluation framework of\nRepairBench. We will update the leaderboard as new frontier models are released.\n1 Introduction\nIn recent years, AI-driven program repair [Zhang et al., 2023, 2024a] has emerged as a key application\nof AI in software engineering. Program repair is the task of automatically fixing software bugs, and\nAI-driven repair uses AI models to generate bug-fixing patches.\nExisting evaluation methodologies [Xu et al., 2022, Jiang et al., 2023] are inadequate for keeping\npace with the rapid evolution of AI. They fail to capture the longitudinal perspective required to track\nthe progress of AI-driven program repair over new generations of AI models. In this paper, we focus\non frontier models, those state-of-the-art models that push the boundaries of AI capabilities.\nWe propose RepairBench, a novel leaderboard aimed at a frequent, sound, and standardized evaluation\nof frontier models for program repair. RepairBench consistently evaluates frontier models on a high-\nquality set of program repair tasks. RepairBench employs carefully curated benchmarks: 1) Defects4J\nv2 [Just et al., 2014], a widely-adopted benchmark in the software engineering community, and 2)\nGitBug-Java [Silva et al., 2024], a benchmark of recent bugs from 2023, that has been designed to\naddress benchmark leakage. A key design decision is that all bugs in RepairBench are real-world bugs\ncoming from real-world programs. They also come with executable tests to verify the correctness of\npatches beyond syntactic match.\nRepairBench carefully selects evaluation metrics to ensure a meaningful comparison across different\nmodels: 1) AST Match@1, which captures syntactic correctness w.r.t. the reference patch written by\nthe human developer, and 2) Plausible@1, which captures correctness based on the execution of all\ntest cases. The latter is the default one used for ranking because it accounts for execution.\nTo sum up, our contributions are:\n\u2022Leaderboard : We publish RepairBench as a leaderboard on the web at https:\/\/\nrepairbench.github.io\/\n\u2022Data : We publicly share all prompts and patches at https:\/\/github.com\/ASSERT-KTH\/\nrepairbench\n\u2022Code : We open-source the code to produce the leaderboard at: https:\/\/github.com\/\nASSERT-KTH\/elle-elle-aimearXiv:2409.18952v1  [cs.SE]  27 Sep 2024\n2 Methodology\nWe devise the RepairBench methodology, a rigorous and standardized methodology to measure the\nperformance of frontier models in program repair. This section outlines the key components of\nRepairBench, including the benchmarks, models, prompts, and the evaluation process.\n2.1 Models\nRepairBench exclusively focuses on frontier models. Frontier models are models that, at the time of\ntheir release, stand out due to their performance across a wide-range of tasks when compared with\nthe state-of-the-art. Their capabilities are demonstrated in general-purpose [Hendrycks et al., 2020,\nLiang et al., 2022, Chiang et al., 2024] and code-specific tasks [Jain et al., 2024]. In other words,\nfrontier models lie at the border of what AI models are currently capable of doing.\nWe select frontier models based on the following criteria: 1) they must demonstrate state-of-the-art\ncapabilities (i.e., be frontier models) in other live evaluation systems (e.g., [Chiang et al., 2024, Jain\net al., 2024], 2) they must be instruction-tuned, due to our prompt setup (see subsection 2.3), 3) they\nmust be available through an API that is accessible to the RepairBench team, and 4) the estimated\ncost to evaluate each bug in RepairBench must not exceed a given price (see subsection 2.4).\n2.2 Benchmarks\nRepairBench selects benchmarks per the following criteria: 1) being real-world programs (no toy\nprograms, no competition programs), 2) being real-world bugs (no seeded or synthetic bugs), 3)\nincluding a variety of domains, 4) being executable, incl. at least one failing test case, 5) including a\nground-truth patch written by a human developer, and 6) being well-engineered so that they can be\nintegrated into the RepairBench framework with reasonable effort.\nRepairBench V1 includes the only two benchmarks that meet all those criteria:\nDefects4J v2 [Just et al., 2014], a widely-adopted benchmark in software engineering research,\ncontains 835 real-world bugs from 17 open-source Java projects. We identify 484 single-function\nbugs which are utilized in RepairBench.\nGitBug-Java [Silva et al., 2024], is a benchmark of Java bugs from 2023, containing 199 real-world\nbugs from 55 open-source Java projects, from which we identify 90 single-function bugs utilized in\nRepairBench.\nIn the next update of the leaderboard, we plan to introduce SWE-Bench [Jimenez et al., 2024].\n2.3 Prompts\nRepairBench employs the same prompt setup for all models, to ensure consistency. The prompt setup\nis zero-shot [Xia and Zhang, 2022], targets single-function bugs (i.e., bugs whose reference patch\nalters a single function), and is not iterative [Zhang et al., 2024b, Xia et al., 2024] (i.e., only a single\ncall to the model is made). These choices are made for scoping reasons: RepairBench aims to provide\na standardized evaluation of model capabilities, without accounting for additional approaches built\non top of these models.\nRepairBench\u2019s prompt template includes: 1) the buggy function, 2) the failing test cases\u2019 code,\nand 3) the failing test cases\u2019 error message (runtime information). This set of ingredients captures\nthe test-suite based program repair task [Parasaram et al., 2024]: the buggy function is the current\nprogram, and the failing test case\/error provides the difference between current and expected behavior\nas defined by the developers. All code snippets contain the original comments (e.g., inline comments,\njavadocs), and are surrounded by Markdown quotation marks. Finally, the model is prompted to\nreturn the repaired function inside quotation marks. Figure 1 shows an example prompt.\nThe answers generated by the models are expected to contain the fixed version of the buggy function\ninside quotation marks. However, models are known to return additional natural language responses\nor explanations. To retrieve the generated code with reasonable leeway for such text, we extract the\nfirst code block generated by the model using regular expressions.\n2\nYou are an automatic program repair tool . Your task is to fix the\nprovided buggy code .\nThe following code contains a buggy function :\n```java\n\/**\n* Puts all values of this record into the given Map .\n*\n* @param map The Map to populate .\n* @return the given map.\n*\/\n<M extends Map <String , String >> M putIn ( final M map) {\nfor ( final Entry <String , Integer > entry : mapping . entrySet ())\n{\nfinal int col = entry . getValue (). intValue ();\nmap .put( entry . getKey () , values [col ]);\n}\nreturn map;\n}\n```\nThe code fails the following tests .\nTest `org . apache . commons .csv . CSVRecordTest :: testToMapWithShortRecord `:\n```java\n@Test\npublic void testToMapWithShortRecord () throws Exception {\nfinal CSVParser parser = CSVParser . parse (\"a,b\",\nCSVFormat . DEFAULT . withHeader (\"A\", \"B\", \"C\"));\nfinal CSVRecord shortRec = parser . iterator (). next ();\nshortRec . toMap ();\n}\n```\nTest `org . apache . commons .csv . CSVRecordTest :: testToMapWithShortRecord `\nerror :\n```\njava . lang . ArrayIndexOutOfBoundsException : 2\n```\nPlease provide a fixed version of the buggy function , and only that\nfunction , inside a code block .\nFigure 1: Prompt for bug Csv-6 of Defects4J. The test case and runtime information guide frontier\nmodels in generating patches.\n2.4 Costs\nFrontier models are typically expensive to evaluate due to both the energy cost to operate them and\nthe provider\u2019s markup. RepairBench is, for the most part, supported by the RepairBench team, who\npay model providers for the patch generation jobs and who execute patches in local infrastructure.\nTo cap the amount of resources allocated to RepairBench, we define a maximum of 0.2 USD per\nevaluated bug, or approx. $115.1 for a total of 574 bugs. When a new frontier models is released, the\nRepairBench team estimates the cost to run RepairBench and proceeds only if the value is within the\nlimit. The cost to generate patches is calculated according to the pricing of each organization, or the\npricing of third-party model providers in case of open-weights models.\n3\nRepairBench is open to sponsorship from model providers, in which case the cost threshold is not\nconsidered.\nCost is also important for program repair per se. Automated program repair fundamentally comptes\nwith the costs of human developers. RepairBench provides a cost-aware [Hidv\u00e9gi et al., 2024] view\nof program repair, and the trade-off between repair cost and repair effectiveness.\n2.5 Metrics\nThe goal of program repair is to obtain a program that correctly fixes the bug without introducing any\nregression. Thus, evaluating models for program repair involves evaluating the multiple dimensions\nof the patches generated by the models.\nThe patch should parse, compile, and type checks (depending on the target language). Correctness is\nevaluated by running the repaired code against a set of test cases to ensure that the original issue is\nresolved without introducing new errors. This is why we select benchmarks with reasonably good\ntest suites.\nRepairBench evaluates and ranks models according to two metrics. Both of them are meant to be\nmaximized: the higher the metric, the stronger the model.\nPlausible@1 : the probability that the first generated patch passes all test cases. By running all test\ncases, we check if the original bug is resolved without new bugs being introduced. Note that this\nmetric does not guarantee that the patch is functionally equivalent to the reference implementation\nsince test suites typically do not cover the entire specification and input domains. To compute\nthepass@k metrics, we rely on Chen et al. [Chen et al., 2021]\u2019s numerically stable and unbiased\nestimator, generating 10 non-deterministically sampled patches per bug with the provider\u2019s default\nsettings and a temperature of 1.0.\nAST Match@1 : the probability that the first generated patch has the same abstract syntax tree (AST)\nas the reference patch provided by the benchmark. Unlike Plausible @1 ,AST-Match @1 is static and\ndoes not rely on the test suite. This metric is a strong indicator of correctness: if the ASTs are the\nsame, it means that the model was able to produce the exact same patch as the human developer.\nNote that the pass@k metrics are more reliable than simply computing the total number of correctly\nfixed bugs: 1) generating patches is not deterministic, even when using deterministic sampling\nalgorithms [Ouyang et al., 2023], 2) models are usually deployed with non-deterministic sampling\nalgorithms in practice. pass@k accounts for the non-determinism by representing the probability of\ngenerating a correct patch given a budget of kgenerations.\n3 Results\nThis section contains the RepairBench results, and is structured to be updated over time with new\nfrontier models. We plan to update the benchmarks for at least 3 years. Table 1 shows the leaderboard\nstatus as of September 30, 2024.\nOrganization ModelDefects4J v2 (484 bugs) GitBug-Java (90 bugs) Total (574 bugs)Ref.\nPlausible@1 AST Match@1 Cost ($) Plausible@1 AST Match@1 Cost ($) Plausible@11AST Match@1 Cost ($)\nAnthropic claude-3-5-sonnet-20240620 41.5% 12.3% $ 57.91 26 .1% 9.0% $ 30.20 39.1% 11.7% $ 88.11 [Anthropic, 2024]\nOpenAI gpt-4o-2024-08-06 34.1% 8.4% $ 20.74 18 .8% 8.1% $ 9.77 31 .7% 8.3% $ 30.51 [OpenAI, 2024a]\nGoogle gemini-1.5-pro-001 30.3% 13.0% $44.95 16 .7% 9.6% $ 33.70 28 .2% 12.5% $78.65 [Reid et al., 2024]\nMeta llama-3.1-405b-instruct 28.9% 7.7% $ 17.42 16 .7% 7.3% $ 11.86 27 .0% 7.6% $ 29.28 [Dubey et al., 2024]\nDeepSeek deepseek-v2.5 26.6% 6.4% $ 14.17 17 .6% 7.3% $ 5.55 25 .1% 6.5% $ 19.73 [Liu et al., 2024]\nAlibaba Cloud qwen-2.5-72b-instruct 25.5% 6.7% $ 2.46 17 .3% 5.9% $ 2.28 24 .2% 6.6% $ 4.74 [Team, 2024]\nMistral mistral-large-2407 24.5% 6.6% $ 27.17 15 .2% 6.6% $ 20.53 23 .0% 6.6% $ 47.70 [Mistral, 2024]\nOpenAI2o1-preview-2024-09-122\u2014 \u2014 \u2014 32.3% 12.1% $325 .71 \u2014 \u2014 \u2014 [OpenAI, 2024b]\n1Models are sorted by the total Plausible@1 score.\n2Only partial results available right now due to cost reasons.\nTable 1: Leaderboard of Frontier Models for Program Repair as of September 30, 2024\nThe leaderboard highlights a clear dominance of Anthropic\u2019s claude-3-5-sonnet-20240620 , which\nachieves the highest overall Plausible@1 score ( 39.1%). This means that this model captures the\nmost of the expected behavior specified in one shot prompt, coupled together with perfect mastering\nof the syntax of the programming language.\n4\nOpenAI\u2019s gpt-4o-2024-08-06 and Google\u2019s gemini-1.5-pro-001 achieve the second and third best\nscores, respectively. gemini-1.5-pro-001 is the best model according to AST Match@1 ( 12.5%).\nOpenAI\u2019s o1-preview-2024-09-12 results are currently incomplete due to its high cost. Yet, we note\nthat it achieves the best score on GitBug-Java with a 32.3%Plausible@1 score (as opposed to 26.1%\nforclaude-3-5-sonnet-20240620 ).\n20 40 60 80\nT otal Cost ($)0.240.260.280.300.320.340.360.38T otal Plausible@1\ngemini-1.5-pro-001\n(2024-05-24)gpt-4o-2024-08-06\n(2024-08-06)\nllama-3.1-405b-instruct\n(2024-07-23)\ndeepseek-v2.5\n(2024-09-05)\nmistral-large-2407\n(2024-07-24)qwen-2.5-72b-instruct\n(2024-09-19)claude-3-5-sonnet-20240620\n(2024-06-20)Performance (Plausible@1) vs Cost\nBest Linear Fit\nFigure 2: Performance (Plausible@1) in function of the total cost (in USD). The most performant\nmodels are also the most expensive ones.\nFigure 2 plots the performance (Plausible@1) as function of the total cost to run RepairBench\non each model. The most expensive frontier models are also the most performant. Anthropic\u2019s\nclaude-3-5-sonnet-20240620 costs a total of $88.11 for a score of 39.1%. Alibaba Cloud\u2019s qwen-2.5-\n72b-instruct model, the cheapest model (4.74$, approx. 20x less than claude-3-5-sonnet-20240620 )\nin RepairBench, achieves a score (24.2%) comparable with DeepSeek\u2019s deepseek-v2.5 (25.1%) and\nbetter than Mistral\u2019s mistral-large-2407 (23.0%).\n4 Discussion\nA critical concern when evaluating AI models is the issue of benchmark leakage [Dong et al., 2024,\nMatton et al., 2024]. Benchmark leakage occurs when models are exposed to test data during pre-\ntraining or post-training. Benchmark leakage, aka contamination, can lead to inflated performance\nresults, giving a misleading picture of the actual ability to generalize and solve novel problems. This\nconcern is particularly problematic for frontier models due to their training on huge amounts of data.\nWe believe there is some benchmark leakage for Defects4j, but the actual extent is unknown. However,\nthe low overall performance (approx. 30-40%) shows that the benchmark is not at all perfectly\nmemorized.\nIn RepairBench, we do mitigate benchmark leakage with the inclusion of GitBug-Java [Silva et al.,\n2024], a newly constructed benchmark with only recent bugs, from 2023 onwards. Moreover,\nRepairBench prioritizes an execution-based metric (Plausible@1) over purely static evaluations (AST\nMatch@1): this helps assess model capabilities beyond superficial memorization of code.\n5 Related Work\n5.1 General-Purpose Benchmarks\nThe evaluation of AI models typically relies on general-purpose benchmarks that assess performance\nacross diverse domains. Among these, MMLU [Hendrycks et al., 2020] stands out as a comprehen-\nsive benchmark, encompassing problems from a wide array of academic disciplines. HellaSwag\n5\n[Zellers et al., 2019] focuses on testing models\u2019 commonsense reasoning, while benchmarks like\nGSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] are designed to evaluate models\u2019\nmathematical problem-solving capabilities.\nIn parallel, several live evaluation platforms have emerged to continuously measure model perfor-\nmance. HEML [Liang et al., 2022] aims to provide a comprehensive evaluation of models across a\nrange of tasks. Other platforms, such as Vellum1, Open LLM Leaderboard2, and KLU.ai\u2019s leader-\nboard3also provide live updates of model performances. Notably, ChatBotArena [Chiang et al., 2024]\nmaintains real-time leaderboards based on battles between models and a crowd-sourced evaluation\nmethodology.\nWhile these benchmarks cover a broad range of capabilities and reasoning tasks, none of them address\nthe specificity of the program repair task.\n5.2 Code Benchmarks\nCode-related tasks, such as code generation and repair, require specialized benchmarks. Being code,\nexecution is a unique characteristic of the output and we claim that execution-based code benchmarks\nis crucial [Khan et al., 2024].\nOne of the most widely-used execution-based benchmarks in this area is HumanEval [Chen et al.,\n2021], which evaluates the ability of models to generate Python code for simple algorithmic problems.\nAlthough HumanEval has been an important tool for measuring the effectiveness of models in code\ngeneration, it is now exhausted, as frontier models achieve near-perfect scores. Also, it is not a\nprogram repair task.\nProgram repair benchmarks [Le Goues et al., 2015] provide a suitable testing ground for AI-driven\nprogram repair. Several program repair benchmarks have been proposed across languages and\ndomains [Csuvik and Vid\u00e1cs, 2022], [Gyimesi et al., 2019]. Some program repair benchmarks are\nexclusively static, without test cases available for execution [Avula et al., 2023]. RepairBench only\nfocuses on executable benchmarks.\nOther benchmarks, despite including test cases, are not fully reproducible due to missing third-party\ndependencies and other low level problems [Madeiral et al., 2019, Saha et al., 2018]. RepairBench\nonly focuses on reproducible benchmarks [Zhu and Rubio-Gonz\u00e1lez, 2023].\n5.3 Code Leaderboards\nBeyond sporadic evaluations, live evaluation platforms for code have been proposed.\nAider\u2019s leaderboard4evaluates LLMs on their capability to write code according to a given instruction.\nIn contrast, RepairBench focuses exclusively on program repair, which involves fixing real-world\nbugs in existing codebases.\nLiveCodeBench [Jain et al., 2024] offers a continuous evaluation of LLMs on a variety of code-related\ntasks, including self-repair [Fan et al., 2023], where the model is assessed based on its ability to\nfix code it has previously generated. While LiveCodeBench focuses on artificial tasks extracted\nfrom code competitions, RepairBench only evaluates models with real-world repair tasks that human\ndevelopers have encountered during software development. Finally, Shariffdeen et al. [Shariffdeen\net al., 2023] held a competition of program repair approaches, but do not include frontier models.\n6 Conclusion\nRepairBench introduces a standardized, execution-based evaluation framework for assessing frontier\nmodels in AI-driven program repair. RepairBench relies on real-world bug benchmarks and focuses on\nexecution for evaluating patches. As new frontier models will be released, RepairBench\u2019s leaderboard\nwill provide insights into the longitudinal evolution of AI-driven program repair.\n1https:\/\/www.vellum.ai\/llm-leaderboard\n2https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard\n3https:\/\/klu.ai\/llm-leaderboard\n4https:\/\/aider.chat\/docs\/leaderboards\/\n6\n7 Acknowledgments\nThis work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program\n(WASP) funded by the Knut and Alice Wallenberg Foundation. The computations\/data handling were\nenabled by the supercomputing resource Berzelius-2023-175 provided by National Supercomputer\nCentre at Link\u00f6ping University and the Knut and Alice Wallenberg foundation.\nReferences\nAnthropic. Claude 3.5 sonnet, June 2024. URL https:\/\/www.anthropic.com\/news\/\nclaude-3-5-sonnet .\nSai Krishna Avula, Venkatesh V obbilisetti, and Shouvick Mondal. Minecraft: Automated mining of\nsoftware bug fixes with precise code context. In Proceedings of the 38th IEEE\/ACM International\nConference on Automated Software Engineering , 2023.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng\nLi, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena:\nAn open platform for evaluating llms by human preference, 2024.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168 , 2021.\nViktor Csuvik and L\u00e1szl\u00f3 Vid\u00e1cs. Fixjs: a dataset of bug-fixing javascript commits. In Proceedings\nof the 19th International Conference on Mining Software Repositories , pages 712\u2013716, 2022.\nYihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, and Ge Li. Generalization or memorization: Data con-\ntamination and trustworthy evaluation for large language models. arXiv preprint arXiv:2402.15938 ,\n2024.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783 , 2024.\nZhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. Automated repair\nof programs from large language models. In 2023 IEEE\/ACM 45th International Conference on\nSoftware Engineering (ICSE) , pages 1469\u20131481. IEEE, 2023.\nP\u00e9ter Gyimesi, B\u00e9la Vancsics, Andrea Stocco, Davood Mazinanian, Arp\u00e1d Besz\u00e9des, Rudolf Ferenc,\nand Ali Mesbah. Bugsjs: a benchmark of javascript bugs. In 2019 12th IEEE Conference on\nSoftware Testing, Validation and Verification (ICST) , pages 90\u2013101. IEEE, 2019.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference on\nLearning Representations , 2020.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track\n(Round 2) , 2021.\nD\u00e1vid Hidv\u00e9gi, Khashayar Etemadi, Sofia Bobadilla, and Martin Monperrus. Cigar: Cost-efficient\nprogram repair with llms. arXiv preprint arXiv:2402.06598 , 2024.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free\nevaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\n7\nNan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. Impact of code language models on automated\nprogram repair. In 2023 IEEE\/ACM 45th International Conference on Software Engineering\n(ICSE) , pages 1430\u20131442. IEEE, 2023.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R\nNarasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth\nInternational Conference on Learning Representations , 2024.\nRen\u00e9 Just, Darioush Jalali, and Michael D Ernst. Defects4j: A database of existing faults to enable\ncontrolled testing studies for java programs. In Proceedings of the 2014 international symposium\non software testing and analysis , pages 437\u2013440, 2014.\nMohammad Abdullah Matin Khan, M Saiful Bari, Do Long, Weishi Wang, Md Rizwan Parvez, and\nShafiq Joty. Xcodeeval: An execution-based large scale multilingual multitask benchmark for code\nunderstanding, generation, translation and retrieval. In Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pages 6766\u20136805, 2024.\nClaire Le Goues, Neal Holtschulte, Edward K Smith, Yuriy Brun, Premkumar Devanbu, Stephanie\nForrest, and Westley Weimer. The manybugs and introclass benchmarks for automated repair of c\nprograms. IEEE Transactions on Software Engineering , 41(12):1236\u20131256, 2015.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110 , 2022.\nAixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong\nRuan, Damai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and efficient mixture-of-\nexperts language model. arXiv preprint arXiv:2405.04434 , 2024.\nFernanda Madeiral, Simon Urli, Marcelo Maia, and Martin Monperrus. Bears: An extensible java\nbug benchmark for automatic program repair studies. In 2019 IEEE 26th international conference\non software analysis, evolution and reengineering (SANER) , pages 468\u2013478. IEEE, 2019.\nAlexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He,\nRaymond Ma, Maxime V oisin, Ellen Gilsenan-McMahon, and Matthias Gall\u00e9. On leakage of code\ngeneration evaluation datasets. arXiv preprint arXiv:2407.07565 , 2024.\nMistral. Large enough, July 2024. URL https:\/\/mistral.ai\/news\/mistral-large-2407\/ .\nOpenAI. Hello gpt-4o, May 2024a. URL https:\/\/openai.com\/index\/hello-gpt-4o\/ .\nOpenAI. Openai o1 system card, September 2024b. URL https:\/\/openai.com\/index\/\nopenai-o1-system-card\/ .\nShuyin Ouyang, Jie M Zhang, Mark Harman, and Meng Wang. Llm is like a box of chocolates: the\nnon-determinism of chatgpt in code generation. arXiv preprint arXiv:2308.02828 , 2023.\nNikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr,\nand Sergey Mechtaev. The fact selection problem in llm-based program repair. arXiv preprint\narXiv:2404.05520 , 2024.\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini\n1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint\narXiv:2403.05530 , 2024.\nRipon K Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, and Mukul R Prasad. Bugs. jar: A\nlarge-scale, diverse dataset of real-world java bugs. In Proceedings of the 15th international\nconference on mining software repositories , pages 10\u201313, 2018.\nRidwan Shariffdeen, Martin Mirchev, and Abhik Roychoudhury. Program repair competition. In\n2023 IEEE\/ACM International Workshop on Automated Program Repair (APR) , pages 19\u201320.\nIEEE, 2023.\n8\nAndr\u00e9 Silva, Nuno Saavedra, and Martin Monperrus. Gitbug-java: A reproducible benchmark\nof recent java bugs. In 2024 IEEE\/ACM 21st International Conference on Mining Software\nRepositories (MSR) , pages 118\u2013122. IEEE, 2024.\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https:\/\/qwenlm.\ngithub.io\/blog\/qwen2.5\/ .\nChunqiu Steven Xia and Lingming Zhang. Less training, more repairing please: revisiting automated\nprogram repair via zero-shot learning. In Proceedings of the 30th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering , pages\n959\u2013971, 2022.\nChunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying\nllm-based software engineering agents. arXiv preprint arXiv:2407.01489 , 2024.\nFrank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of\nlarge language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium\non Machine Programming , pages 1\u201310, 2022.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 4791\u20134800, 2019.\nQuanjun Zhang, Chunrong Fang, Yuxiang Ma, Weisong Sun, and Zhenyu Chen. A survey of learning-\nbased automated program repair. ACM Transactions on Software Engineering and Methodology ,\n33(2):1\u201369, 2023.\nQuanjun Zhang, Chunrong Fang, Yang Xie, YuXiang Ma, Weisong Sun, and Yun Yang Zhenyu Chen.\nA systematic literature review on large language models for automated program repair. arXiv\npreprint arXiv:2405.01466 , 2024a.\nYuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous\nprogram improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis , pages 1592\u20131604, 2024b.\nHao-Nan Zhu and Cindy Rubio-Gonz\u00e1lez. On the reproducibility of software defect datasets. ICSE.\nIEEE , 2023.\n9","filename":"Repairbench__Leaderboard_of_frontier_models_for_program_repair.pdf","responses":"[1] Machine Learning For Code: Application of machine learning techniques to software code tasks such as program repair, code generation, analysis, and evaluation."}
{"id":"pdf_1","text":"Improving Examples in Web API Specifications\nusing Iterated-Calls In-Context Learning\nKush Jain\nCarnegie Mellon University\nUnited States\nkdjain@andrew.cmu.eduKiran Kate\nIBM Research\nUnited States\nkakate@us.ibm.comJason Tsay\nIBM Research\nUnited States\nJason.Tsay@ibm.com\nClaire Le Goues\nCarnegie Mellon University\nUnited States\nclegoues@cs.cmu.eduMartin Hirzel\nIBM Research\nUnited States\nhirzel@us.ibm.com\nAbstract \u2014Examples in web API specifications can be essential\nfor API testing, API understanding, and even building chat-bots\nfor APIs. Unfortunately, most API specifications lack human-\nwritten examples. This paper introduces a novel technique for\ngenerating examples for web API specifications. We start from\nin-context learning (I CL): given an API parameter, use a prompt\ncontext containing a few examples from other similar API\nparameters to call a model to generate new examples. However,\nwhile I CLtends to generate correct examples, those lack diversity,\nwhich is also important for most downstream tasks. Therefore,\nwe extend the technique to iterated-calls I CL(ICICL): use a\nfew different prompt contexts, each containing a few examples,\nto iteratively call the model with each context. Our intrinsic\nevaluation demonstrates that I CICLimproves both correctness\nand diversity of generated examples. More importantly, our\nextrinsic evaluation demonstrates that those generated examples\nsignificantly improve the performance of downstream tasks of\ntesting, understanding, and chat-bots for APIs.\nI. I NTRODUCTION\nWeb Application Programming Interfaces (APIs) enable\nsystems to communicate across a network [1], [2]. REpresen-\ntational State Transfer (REST) APIs have become the de facto\nstandard for modern web applications [3]. This style enables\nclients and services to exchange information over HTTP. Large\ncompanies like Google, Amazon, and Apple expose services\nthrough REST APIs, including large enterprise services like\nGoogle Drive and Apple Authentication, as well as simpler\nservices like REST Countries,1for querying information about\na country.\nREST APIs are commonly described using OpenAPI spec-\nifications [4]: one survey of communication service providers\nfound that 73% of companies and 75% of suppliers use\nOpenAPI to describe their APIs.2OpenAPI specifications\nformalize the contract between API developer and API user,\ndescribing the structure of API requests and responses. Tools\n1https:\/\/restcountries.com\n2https:\/\/inform.tmforum.org\/features-and-opinion\/\nthe-status-of-open-api-adoption\/such as Redoc3and SwaggerUI4can automatically convert\nOpenAPI specifications into human-readable webpages, allow-\ning developers to better understand these APIs. Additionally,\nspecifications are commonly used in input validation [5], [6]\nand testing [7], [8].\nCommon downstream clients of OpenAPI specifications\nleverage realistic examples of OpenAPI parameters (when\nthey exist) as a part of their workflow. Fuzzers [9], [10]\nuse examples to guide API testing, producing fewer invalid\nrequests and covering deeper code paths. Chat-bots [11], [12],\n[13] first build an underlying model of a system and then\nderive API calls from the natural language utterance. Recently-\ndeveloped large language models (LLMs), like ChatGPT [14]\nand GPT-4 [15], benefit from using API parameter examples,\nand other LLMs use them for fine-tuning, as evaluated on\ndialog benchmarks [16]. API parameter examples can also\nimprove human understanding, especially for novice users\n[17], [18].\nHowever, despite their widespread adoption, most OpenAPI\nspecifications lack API parameter examples (only 1,953 out\nof a dataset of 13,346 mined OpenAPI parameters have\nany examples). There has been some research on generating\nexamples for OpenAPI specifications. Prior work follows two\napproaches: (i) extracting examples from API descriptions [10]\nor (ii) mining examples from knowledge bases [9], [19]. The\ngoal of both approaches is to generate diverse and correct\nexamples. Example correctness is important, as these examples\nserve as input to software testing and dialog systems. Con-\nversely, example diversity is also important, as examples that\ndiffer from one another help testing increase its coverage and\nhelp chat-bots generalize their natural-language understanding.\nBoth approaches to example generation are limited: mining\nexamples only works for examples present in knowledge bases,\nwhile extracting examples from descriptions only works when\nthe description explicitly enumerates parameter examples.\n3https:\/\/github.com\/Redocly\/redoc\n4https:\/\/github.com\/swagger-api\/swagger-uiarXiv:2504.07250v1  [cs.SE]  9 Apr 2025\nWe present I CICL, which combines retrieval-based prompt-\ning [20] with iterated calls to in-context learning (I CL) to\ngenerate diverse and correct API parameter examples. I CICL\nleverages the ability of LLMs to generate realistic examples\nbased on their pretraining. Unlike knowledge bases, LLMs\nare pretrained on large swaths of the internet, and thus have a\nstrong prior of the world around them. We take as input the\nOpenAPI specification without examples and generate exam-\nples for all API parameters, regardless of whether examples\nexist on the internet or the descriptions specify example values.\nFor correctness, we use greedy decoding (taking the highest\nprobability token at each step) to generate one (likely) correct\nexample. We perform postprocessing to only keep examples\nthat are similar to our (likely) correct example. For diversity,\nwe both increase temperature, and, unlike vanilla I CL, use\niterated calls with multiple prompt contexts. One can increase\ntemperature (smoothing the distribution of next token proba-\nbilities) to generate different model outputs. Additionally, we\nobserve that the problem of example diversity is similar to\nthe challenge of generating different model outputs, which is\nsolved by ensembles [21] of different models. This observation\nleads us to use multiple prompt contexts, where each context\nconsists of a different set of few-shot examples.\nWe evaluate I CICL, finding that it generates diverse, cor-\nrectly typed examples. We further manually annotate a sample\nof 385 parameters and show that 75% of the generated\nexamples are correct. We then demonstrate the usefulness of\nthe generated examples in three downstream settings: fuzzing,\ndialogue benchmarks, and human API understanding, which\nwe assess via an exploratory developer pilot. Our examples\nsignificantly improve performance in these tasks, improving\nbranch coverage by 116%, dialog intent recognition by 3%,\nand dialog slot filling by 5%, compared to the original speci-\nfications.\nTo summarize, our core contributions are as follows:\n\u2022We identify adding examples as a single improvement to\nAPI specifications that benefits several downstream use\ncases (understanding, fuzzing, chat-bots).\n\u2022Inspired by how ensembles use multiple models to im-\nprove results, we introduce I CICL, a new technique for\nusing LLMs to generate API examples. We combine\nretrieval-based prompting, multiple prompt contexts, and\npost-processing to produce diverse yet correct examples.\n\u2022We include an extensive experimental evaluation that\nquantifies the value of the generated examples for several\nuse-cases. These include fuzz testing, chat-bots, and an\nexploratory study of developers\u2019 API understanding.\nOur prompting, intrinsic, fuzzing, and exploratory\nstudy evaluation and code are at https:\/\/figshare.com\/s\/\n8eec881ddf8e6573f43f, including detailed reproduction\ninstructions. We elide calls to internal company services\nin the prompting code, but release all other code. We are\nunfortunately unable to release our API parameter bank,\nintrinsic evaluation dataset, and SeqATIS dataset, as they\nare internal to the large technology company at which thisListing 1: Illustrative OpenAPI parameter from the Rest Countries\nAPI. Prior approaches struggle to generate correct examples for this\nAPI parameter; knowledge bases contain many false positives, and\nthe description contains no examples.\nname: currency\ndescription: Search by ISO 4217 currency code\nin: path\nrequired: true\nschema:\ntype: string\nwork was conducted, but hope that the other elements of the\nartifact are informative for subsequent research.\nII. M OTIVATING EXAMPLE AND OVERVIEW\nListing 1 shows an illustrative parameter for the\n\/currency endpoint5of the REST Countries API. As with\nmost OpenAPI parameters, this specification contains its name,\na short description, and a type. However, it does not contain\nany example values, nor can example values easily be ex-\ntracted from the description or name. To try the \/currency\nendpoint, a developer would either need domain knowledge\nof ISO 4217 currency codes or would need to search for an\nexample. Fuzzers also fail to cover deeper code paths for this\nendpoint, as they would start from a random sequence of bits\nand would only arrive at a valid ISO 4217 currency code by\nchance.\nTwo common approaches to generating example values,\nnamely mining them from a knowledge base such as DBPedia\nor extracting examples from the description, would also fail\nhere. While ISO 4217 is an entity in DBPedia (the knowledge\nbase used by the state-of-the-art example generation tool,\nARTE [9]), there are numerous other currency codes that\nare not ISO 4217, meaning that generated examples are\nsemantically incorrect. ARTE [9] circumvents this by calling\nthe API with examples to see if they are valid; however, this\nlimits applicability to cases like fuzzing, which can send a\nlarge volume of requests to the API. The description also\ndoes not enumerate examples of currency codes that could\nbe extracted.\nFigure 1 gives an overview of I CICL. It first retrieves\nparameters from the API parameter bank that are similar to\nthe parameter from the original API specification (step 1).\nThen it creates a prompt context by greedily selecting the\ntop-most similar retrieved parameters for in-context learn-\ning (step 2). Following this, it uses the LLM with greedy\ndecoding to obtain the greedy example, which has the highest\nconfidence (step 3). It then creates multiple diverse prompt\ncontexts, each of which includes the greedy example plus\nsome retrieved parameters for in-context learning, selected\nto be similar but with some randomization (step 4), and\nuses iterated calls to the LLM with a higher temperature\nto obtain multiple diverse examples, one from each of the\ndiverse prompt contexts (step 5). Then it creates a list\n5https:\/\/restcountries.com\/v2\/currency\nGreedy context\ncurrencyCode: EUR\norder_crncyCode: USD\nImproved API spec.\nname: currency\ndescription: ISO 4217\ntype: string\nexamples: USD,CAD,EUR\n\u2026Greedy example\nUSDOriginal API spec.\nname: currency\ndescription: ISO 4217\ntype: string\n(no examples)\n\u2026Retrieved parameters\n1. currencyCode: EUR\n2. order_crncyCode: USD\n3. local_tender: INR\n\u2026API\nchat-bot\nAPI\nunderstanding\nAPI\nfuzz-testingDiverse contexts\n\u2022currency: USD\ncurrencyCode: EUR\n\u2022currency: USD\nlocal_tender: INRDiverse examples\nUSD,GBP,USD,\nCAD,ZAR,CAD,\nINR,MXN,CNY,\nEUR\ninputs output intermediate results clientsParameter bank\nid: abc, def\ncurrencyCode: EUR\nstate: CA, CO, NY\n\u202612 3\n4 56Large\nlanguage modelFig. 1: Overview and running example of our approach. Circled numbers correspond to different steps in our approach.\nof filtered examples that include the greedy example and\nsome of the diverse examples, which it adds to the API\nspecification (step 6).\nOur approach performs well on the snippet in Listing 1,\ngenerating USD,CAD, and EUR, all valid currency examples.\nIII. I CICL\nICICLtakes an API specification without parameter exam-\nples as input and returns an improved specification with those\nexamples as output. Figure 1 outlines the approach. Offline, we\ncreate a parameter bank by mining parameters and examples,\nand pick an off-the-shelf LLM (Section III-A). Online, given\na parameter in an OpenAPI specification, we retrieve relevant\nparameters from the parameter bank (Section III-B), build\nprompt contexts (Section III-C), and finally postprocess model\noutput (Section III-D).\nA. Offline: Mining Examples, Model Selection\nWe mine 1,236 OpenAPI specifications from API Guru [22]\nand Wittern et. al. [23]. This collection has OpenAPI specifica-\ntions for popular enterprise applications such as Box, Google\nDrive, YouTube, and others. We parse the mined specifications\nto extract each API parameter and corresponding examples. Of\n13,346 parameters, 1,953 have examples. We use these mined\nexamples as our parameter bank (shown among the inputs on\nthe left of Figure 1).\nWe use Falcon,6a 40B parameter model trained on one\ntrillion tokens from the internet, for the LLM (middle of\nFigure 1). Falcon outperforms LLAMA, GPT-3, and MPT on\nthe OpenLLM leaderboard. Falcon has also been extensively\npretrained on code, which we hypothesize will help with type\ncorrectness. Using a large but not huge open-source model\nsuch as Falcon is representative of commercial settings that\nmust balance cost and data exposure regulatory concerns.\nWe model the task of example-generation as an instance of\nfew-shot prompting, varying the prompt context to generate\ndifferent examples.\n6https:\/\/huggingface.co\/tiiuae\/falcon-40bB. Retrieving Relevant Parameters\nGiven an API parameter, we first seek a set of relevant\nsimilar parameters from the parameter bank (Figure 1: 1). We\nfirst extract the initial 50 characters from the API parameter\ndescription (our dataset has a median description length of 54\ncharacters, with the first 50 characters concisely representing a\nparameter\u2019s purpose or function). At times, the full description\nis excessively verbose, with all other parameter information\noutside the description having a median length of 63 char-\nacters, thus truncating at 50 characters ensures that we do\nnot overwhelm other important information. We append the\nexact name of the API parameter to the description, ensuring\nthat the name of the API parameter is factored into any\nsimilarity computation. Lastly, we append the operation ID,\nwhich offers additional context about the operation associated\nwith the parameter. For example, the parameter \u2018 name \u2019 has\ndifferent meanings if the operation ID is \u2018 getCountries \u2019\nor \u2018getUserByUsername \u2019. We use the concatenated string\nof the parameter description, parameter name, and operation\nID as the query for retrieval.\nWe use BM25 [24] as the retrieval method, due to its\nhigh speed and accuracy [25]. BM25 calculates a weight\nof terms based on their frequency in both the query and\nthe target documents. It then considers the term\u2019s prevalence\nacross the entire parameter bank (intuitively infrequent terms\ndiscriminate better). When BM25 processes a parameter (such\nas \u2018currency\u2019 in the running example), it returns a similarity\nscore for each API parameter in the parameter bank. These\nscores measure how closely each parameter in the parameter\nbank matches the parameter we are generating examples for.\nWe leverage this distribution of similarity scores to craft\nprompt contexts (sets of few-shot examples for in-context\nlearning).\nC. Prompt Context Generation\nPrompt context generation consists of two phases: eliciting\nthe greedy example, and then constructing ten prompt con-\ntexts (of five shots each) to elicit diverse examples. We use a\ntwo-phase approach to improve both correctness and diversity\nof the generated examples.\nListing 2: LLM prompt for currency code. We provide the parameter\nthat is missing examples and five few-shot examples.\n# Given an OpenAPI parameter, generate a unique\nexample of the parameter.\ninput_0 = {\n\"param_name\": \"currencyCode\",\n\"type\": \"string\",\n\"operation_id\": \"contractInfo\",\n\"description\": \"The currency code (ISO 4217)\",\n\"api_name\": \"beezup\"\n}\n# must generate a unique currencyCode string\nexample_0 = \"EUR\"\n...\ninput_6 = {\n\"param_name\": \"currency\",\n\"type\": \"string\",\n\"operation_id\": \"v2Currency\",\n\"description\": \"Search by ISO 4217 currency code\n\",\n\"api_name\": \"rest-countries\"\n}\n# must generate a unique currency string\nexample_6 =\nThe first phase prompts the LLM with the top five re-\ntrieved parameters with the highest similarity to the query\n(Figure 1: 2) as returned by retrieval (Section III-B). Greedy\ndecoding in an LLM simply picks the most likely token at each\ngeneration step, thus deterministically yielding the sequence\nof most-probably tokens. By leveraging greedy decoding, this\nstep aims to produce a (likely) correct example 3. Our model\nyields USD as the greedy example (a correct currency code).\nBy providing the greedy example in all prompt contexts, we\nensure that the LLM, even at a higher temperature setting,\ngenerates examples that align with the original example.\nThe second phase improves example diversity by sampling\nfrom the distribution of similarity scores to generate 10 prompt\ncontexts of five examples each 4. We take inspiration from\nensembles [21], where multiple models produce different\noutputs that improve both the correctness and diversity of the\nresulting system. Our prompt contexts, each of which consists\nof a different set of API parameter examples (i.e., \u201cshots\u201d),\nare similar to the diverse models used in ensembles. We iter-\natively call the LLM with each prompt context with a higher\ntemperature of 0.5 to generate 10 example candidates 5. The\norder of these calls does not matter; they can be parallelized\nor batched. In the running example, the calls return USD,GPP,\nUSD,CAD,ZAR,CAD,INR,MXN,CNY, and EUR.\nD. Postprocessing\nWe perform postprocessing to narrow these 10 example\ncandidates down to 3 examples to add to the improved API\nspecification (Figure 1: 6). First, we filter out all examples\nthat do not match in type to the API parameter we are\ngenerating examples for. This is the earliest opportunity for\nthis filter, and we do it right away given the importance of\ntype compatibility. We then add the greedy example to the\n10 example candidates and perform deduplication. We alwaysinclude the greedy example in our set of three generated\nexamples, as it is likely to be correct. Following this, we add\nall examples that the model generates multiple times to our\nset of three, and return this set if it contains at least three\nexamples. For example, if the model generates the currency\nCAD twice, then we add it to the final set of three examples.\nIf, at this point, there are fewer than three examples, we use\nBERT [26] to encode each example and the greedy example.\nWe then select the most similar examples until we have three\nexamples (illustrated by adding EUR in Figure 1).\nThis ensures that the generated examples are similar in\nformat and content to the greedy example, improving their\nlikelihood of being correct. We choose to favor correct-\nness over diversity here, given its importance to downstream\ntasks (testing and chat-bots). Using BERT embeddings ensures\nthat we are comparing the semantic similarity of each example\nto the greedy example, rather than doing a simple text-\nbased match (which, in the case of currency codes, is less\nmeaningful).\nIV. I NTRINSIC EVALUATION\nWhile ultimately extrinsic evaluations (Section V) matter\nmost for downstream clients, they are laborious to measure, so\nwe used intrinsic evaluations for nimble iterative modeling. We\nevaluate examples generated by I CICLon intrinsic correctness\nand diversity. Specifically, we measure whether examples\ngenerated by I CICLare type correct, unique, and semantically\ndiverse. We also hand-evaluated a smaller subset of examples\nfor semantic correctness. We compare different components of\nICICLacross these metrics.\nA. Experimental Setup\n1) Dataset: We evaluate modeling approaches on a ran-\ndomly sampled dataset of 1,000 OpenAPI parameters mined\nfrom mainstream services including but not limited to Box,\nGoogle Drive, and Gmail. We remove all parameters in the\nparameter bank from our set of API parameters prior to\nsampling. We also remove all Boolean and enum parame-\nters (approximately 1,000 from the initial mined set) from our\nevaluation set, as predicting the values of these parameters\nis trivial. Due to computational cost, we do not run our\nintrinsic evaluation on the full final set of 13,346 parameters,\ninstead focusing on a likely-representative random sample\nof 1,000 examples (approximately 1\/13) of the dataset. This\nsampling is in line with prior work [27], [28], which sample a\nsimilar proportion of the dataset for evaluation. These include\n668 string, 129 array, 106 integer, 34 number, 14 object, and\n5 datetime types. The remaining 44 parameters come from a\nvariety of other types including color, tuples, and None types.\n2) Approach: We evaluate the efficacy of each component\nof our approach (adding retrieval, sampling from the distri-\nbution of similarity scores, and applying our postprocessing).\nThis is equivalent to an ablation study: the final setting is\nthe full approach, earlier settings remove components. We\nprompt the model as described in each settings and evaluate\nthe generated examples using the metrics described below.\nStatic: Static refers to a static prompt of five parameter and\nexample pairings for in-context learning. We also include the\ngreedy example as part of the prompt and use a temperature\nof 0.5. Temperature corresponds to the level of randomness\nin text generation - temperature of 0 refers to sampling the\nmost likely tokens, while higher temperature refers to sampling\nmore diversely. We prompt the LLM 10 times to generate\n10 examples and perform deduplication. Finally, we randomly\nselect three examples to return to the user.\nRetrieval: Retrieval refers to the greedy retrieval approach.\nRather than sampling 10 prompt contexts from the distribution\nof similarity scores, we only use a single prompt context\ncontaining the five most similar parameters for prompting.\nIn other words, this setting performs in-context learning with\nretrieval, but no iterated calls.\nRetrieval (w\/contexts) : Our retrieval with context approach\nadds iterated calls with context sampling. Rather than selecting\nthe five most similar examples for all 10 prompts, we build\nprompt contexts by randomly sampling from the distribution\nof similarity scores (similar parameters are more likely to be\nchosen than different parameters).\nRetrieval (w\/postprocessing) : This is our final approach\nused in extrinsic evaluations (fuzzing, dialog, and exploratory\nusability study). We apply our postprocessing that filters out\ntype-incorrect examples and selects examples that are similar\nto the greedy example. This helps ensure that our examples\nare correct, both in type and in semantic meaning (close to a\ngenerated example likely to be correct).\n3) Metrics: We define the following set of metrics to\nbenchmark various prompting approaches. The main factors\nwe consider are example correctness and example diversity.\nType Correctness: Type correctness adheres to the strict\ndefinition of all generations from the LLM being the same\ntype as the parameter. We use this strict definition to ensure\nall generations conform to the same example type. Recall that\nour intrinsic evaluation focuses on open-ended types (strings,\nnumbers, arrays, objects, etc.) but not Boolean or enums (as\ngenerating values for types with small closed sets is trivial).\nUniqueness: Uniqueness refers to the ability of the LLM\nto generate three case-insensitive unique examples from 10\ngenerations. Higher uniqueness values indicate more diverse\nLLM generated examples. For example, if all 10 generations\nare the same example, the uniqueness would be 0, otherwise\nif there are three unique examples it would be 1.\nDiversity: Diversity is 1 minus mean cosine similarity be-\ntween the BERT [26] embeddings of examples. We choose to\nuse BERT embeddings over TF-IDF or BM25 embeddings,\nas BERT embeddings detect semantic similarity, while other\napproaches only detect overlap of tokens (syntactic similarity).\nExample Correctness: Example correctness refers to gener-\nated examples matching the specification. We define correct-\nness as examples that both satisfy preconditions specified in\nthe natural language description of the parameter and have\nconsistent format between all generated examples. Correct\nexamples can be used in an API call to the API under\ntest without 4xx or input validation errors. Unlike the othermetrics, which are fully automated, this metric requires human\neffort. We manually annotate a randomly sampled subset of\n385 out of our 1,000 sampled examples across all four settings,\nfor 95% confidence in the correctness results.\nB. Intrinsic Evaluation Results\nTABLE I: Intrinsic evaluation metrics on 1,000 (columns Type,\nUnique, Both (type correct and unique), Div) and 385 (column\nCorrect) randomly sampled examples. Each approach component\nimproves type correctness, the proportion of unique examples, and\noverall correctness.\nSetting Type Unique Both Div Correct\nstatic 97% 48% 47% 0.22 70.4%\nretrieval 98% 55% 55% 0.20 73.2%\nw\/contexts 98% 66% 65% 0.23 65.7%\nw\/postprocessing 99% 67% 67% 0.19 74.3%\nTable I shows the results from running various components\nof I CICLon a selected OpenAPI parameters. We show how\neach component improves on the baseline.\nWe find that type correctness of generated examples is\nrelatively strong across all approaches (varying from 97% to\n99%). We hypothesize this is due to LLMs\u2019 extensive training\non code, where type is important in generating the next token.\nHowever, we do notice that type correctness does increase\nas we add retrieval-based prompting and our postprocessing,\nwhich improves type correctness to 99%.\nIn terms of generating unique examples, we find that each\nstep in our process improves upon the previous step. Retrieval\nand adding contexts see approximately a 10% improvement\nover the previous steps. Postprocessing improves uniqueness\nslightly, with 67% of examples generated having 3 examples.\nCosine similarity between examples remains relatively stable\nacross modes, with retrieval templating (third row) slightly\nimproving example diversity. We do want examples to have\nconsistent format, while still being diverse, likely resulting in\nlower diversity scores. The average Levenshtein edit distance\non our dataset is 15 characters, suggesting the examples are\nstill syntactically different from one another on average.\nExample correctness remains relatively stable across all four\nsettings (varying from 66% to 74%). The correctness of our\nfinal approach is higher than any intermediate approach. Note\nthat our evaluation of example correctness is conservative:\nin order for an example to be correct, all generations need\nto satisfy preconditions and have consistent format. Even\nexamples not labeled as correct can still be useful for de-\nvelopers (such as an example of a time parameter that is\nmissing the timezone), meaning that the 74% correctness rate\nis likely an underestimate of the true utility of the examples.\nOverall, our approach is often correct, showing the promise\nthat LLMs pose for usefully enhancing API specifications. The\nextrinsic evaluation in the following section shows that, despite\nnot always being correct, synthetic examples benefit all three\ndownstream clients we tried.\nListing 3: Fuzzing enhanced OpenAPI specification. Examples\ngenerated by I CICLare both diverse and correct.\nname: currency\ndescription: Search by ISO 4217 currency code\nrequired: true\nschema:\ntype: string\nenum:\n- USD\n- CAD\n- EUR\nexample: USD\n...\nname: currency\ndescription: Search by ISO 4217 currency code\nrequired: true\nschema:\ntype: string\nV. E XTRINSIC EVALUATION\nThis section evaluates I CICLon downstream tasks (clients\non the right-hand side of Figure 1), namely software testing\n(Section V-A), API chatbots (Section V-B), and, by means of\nan exploratory pilot, human API understanding (Section V-C).\nA. Software Testing\nThe goal of REST API testing is to find inputs that increase\ncode coverage (and, ultimately, find bugs). At a high level, API\nfuzzers encode the schemas present in OpenAPI specifications,\nand use them to generate values for API endpoints. Coverage\nserves as a feedback mechanism: calls that increase coverage\nare saved for further mutation, while calls that do not are\nthrown out.\n1) Dataset and fuzzers: We evaluate I CICLfor fuzzing\nusing a dataset from Kim et. al. [10] consisting of both small\nand large APIs. We exclude the OMDB and Spotify APIs\nfrom that dataset, due to changes in both that make usage\nmore challenging, and internal restrictions that block certain\nendpoints. This leaves seven widely-used REST API services\n\u2014 FDIC, REST Countries, ohsome, GenomeNexus, OCVN,\nLanguageTool, and YouTube. This previous dataset included\n4 that were run as local instances \u2014 GenomeNexus, OCVN,\nLanguageTool, and YouTube \u2014 for the purposes of computing\ncoverage. We therefore follow the previous evaluation and\ncompute black-box performance on all 7 and coverage on\nthe 4.\nWe evaluate using four popular fuzzers: EvoMaster [8],\nMoREST [7], RESTest [29], and RestTestGen [30]. We report\nresults for each fuzzer along with the aggregated results\nacross them all. We used a version of RESTest that includes\nARTE [9], a state-of-the-art example generation approach,\nas part of its implementation. Hence, we refer to it as\nRESTest\/ARTE below, helping show how I CICLcompares to\nand can complement ARTE.\n2) Approach: To measure performance in the fuzzing con-\ntext, we run I CICLwith each OpenAPI parameter that we\nextract from the fuzzing OpenAPI specifications. For each APIparameter, we overload the specification with two options: the\nparameter with examples, and the parameter without examples,\nfollowing Kim et al. [10]. Since most fuzzers do not directly\nuse API examples, we had to use a work-around, where we\nencode the examples both using the example attribute and as\nan enum. Listing 3 shows how we encode these values (the\ngenerated examples are USD,CAD andEUR). We also include\nthe original parameter, to allow for fuzzers to explore values\noutside these examples. This ensures the fuzzer can explore\nthe example values and mutate existing example values by\nhitting the overloaded endpoint. For example, a fuzzer could\nchoose the value CAD and then mutate it to CDF by hitting\nthe overloaded endpoint twice. We run each fuzzer on both\nthe original specification and the enhanced specification.\nWe were unfortunately unable to directly compare to the\napproach in Kim. et. al [10]. We have filed an issue and\nhave an ongoing discussion with the authors on the use of\ntheir artifact, and the paper does not directly ablate example\ngeneration. We did randomly sample 700 of our 13,346 mined\nAPI parameters (approximately 5%), finding that only 43\nenumerated examples occur in the description. Thus, even if\nKim et. al. [10] extracts examples with 100% accuracy, it could\nonly do so for 6% of all API parameters.\n3) Metrics: We use a combination of API fuzzing metrics\nand code coverage to evaluate the performance change of\nadding examples to each fuzzer.\nProportion of 2xx Requests: 2xx requests represent success-\nful invocations of API endpoints, i.e., requests that yielded\nan HTTP response code between 200\u20132997. These requests\nare saved for further fuzzing; thus having more 2xx requests\nmeans that the fuzzer is capable of testing functionality beyond\nsimple input validation.\nProportion of 4xx Requests: 4xx requests represent poorly\nformatted invocations, where the fuzzer invokes the API\nincorrectly. Ideally, a fuzzer should make fewer 4xx requests,\nas these are not testing deep functionality and wasting the\nfuzzing time budget.\nProportion of 5xx Requests: 5xx requests represent internal\nserver errors. The goal of fuzzing is to catch such errors, thus\nmore 5xx requests represent a successful fuzzing effort.\nBranch Coverage: The goal of fuzzing efforts is to auto-\nmatically test as much of the API as possible. Coverage\nis important, as higher code coverage indicates the fuzzer\nis testing a larger proportion of the API. We report branch\ncoverage achieved by each fuzzer, as well as averaged across\nall four.\n4) Results: Table II shows results across all fuzzers. Be-\nsides EvoMaster, all fuzzers exhibit a similar trend: exam-\nples lead to more 2xx requests (around 3%), fewer 4xx\nrequests (around 3%), and around the same 5xx requests. This\nmeans that our example generation approach can seamlessly\nintegrate with fuzzers to improve API testing, by better seeding\nfuzzers with realistic parameter examples that the fuzzers can\nuse to invoke APIs.\n7https:\/\/en.wikipedia.org\/wiki\/List ofHTTP status codes\nTABLE II: First three columns: API performance results for RESTest\/ARTE, EvoMaster, MoREST, and RestTestGen across all 7 APIs. The\nproportion of 2xx requests goes up, 4xx goes down and 5xx slightly increases with enhanced examples. EvoMaster is the one exception, with\n2xx request proportions decreasing. Last column: Coverage results. Coverage universally increases across all fuzzers with our enhancements.\nFreq. of 2xx Freq. of 4xx Freq. of 5xx Branch Cov.\nTool Name Base Enhanced Diff Base Enhanced Diff Base Enhanced Diff Base Enhanced Diff\nRESTest\/ARTE 0.28 0.31 +11% 0.57 0.54 -5% 0.10 0.10 +0% 4.0 6.2 +57%\nMoREST 0.01 0.04 +300% 0.83 0.81 -2% 0.10 0.10 +0% 3.2 17.5 +447%\nEvoMaster 0.29 0.24 -17% 0.58 0.62 +7% 0.08 0.07 -13% 6.9 12.2 +77%\nRestTestGen 0.21 0.26 +24% 0.\n\n[Text truncated due to context length limits]","filename":"Improving_Examples_in_Web_API_Specifications_using_Iterated-Calls_In-Context_Learning.pdf","responses":"[1] Machine Learning For Code: Applications of machine learning to software engineering tasks, including automatic generation of API parameter examples, testing, API understanding, and tooling for code-centric applications."}
{"id":"pdf_2","text":"SnipGen: A Mining Repository Framework for\nEvaluating LLMs for Code\nDaniel Rodriguez-Cardenas, Alejandro Velasco, and Denys Poshyvanyk\nDepartment of Computer Science, William & Mary\nWilliamsburg, V A\nEmail: dhrodriguezcar, svelascodimate, dposhyvanyk {@wm.edu }\nAbstract \u2014Large Language Models (LLMs), such as\ntransformer-based neural networks trained on billions of\nparameters, have become increasingly prevalent in software\nengineering (SE). These models, trained on extensive datasets\nthat include code repositories, exhibit remarkable capabilities\nfor SE tasks. However, evaluating their effectiveness poses\nsignificant challenges, primarily due to the potential overlap\nbetween the datasets used for training and those employed\nfor evaluation. To address this issue, we introduce SnipGen ,\na comprehensive repository mining framework designed to\nleverage prompt engineering across various downstream tasks\nfor code generation. SnipGen aims to mitigate data contamination\nby generating robust testbeds and crafting tailored data points\nto assist researchers and practitioners in evaluating LLMs\nfor code-related tasks. In our exploratory study, SnipGen\nmined approximately 227Kdata points from 338Krecent\ncode changes in GitHub commits, focusing on method-level\ngranularity. SnipGen features a collection of prompt templates\nthat can be combined to create a Chain-of-Thought-like\nsequence of prompts, enabling a nuanced assessment of LLMs\u2019\ncode generation quality. By providing the mining tool, the\nmethodology, and the dataset, SnipGen empowers researchers\nand practitioners to rigorously evaluate and interpret LLMs\u2019\nperformance in software engineering contexts.\nIndex Terms \u2014Deep learning, code generation, datasets, large\nlanguage models, evaluation\nI. I NTRODUCTION\nLarge Language Models (LLMs) have demonstrated signif-\nicant success across diverse software engineering (SE) tasks,\nincluding code auto-completion [1]\u2013[5], code summarization\n[6], [7], code review [8], [9], code translation [10], clone\ndetection [11], [12], and program repair [13]\u2013[19]. LLMs are\nneural models trained on huge datasets including complete\nGitHub repositories. Common testbeds for evaluating LLMs\nfor code such as HumanEval ,MBPP andCodeXGlue are no\nlonger sufficient [20]. In addition, as benchmarks and testbeds\nare released, new LLMs probably already seen those testbeds.\nTherefore the testbeds are prone to be outdated as soon as a\nnew LLM is released.\nLLMs perform complex tasks by relying on statistical\nknowledge acquired from data distributions, a phenomenon\ndescribed by Wei et al. as emerging capabilities [21]. Given the\nlimited understanding of the nature of this phenomenon, we\ncan formulate an important question: under what conditions\nLLMs produce the desired output? Prompt engineering ad-\ndresses this question by harnessing these capabilities, guiding\nLLMs to make more accurate predictions. Furthermore, giventhat LLMs can extract rules from the provided context ( i.e.,in-\ncontext learning), prompt engineering is a natural and intuitive\nway for people to use LLMs.\nRecent studies have demonstrated that LLMs exhibit im-\nprovements in accuracy for downstream tasks when prompts\nare enhanced and augmented [22], [23]. Moreover, new meth-\nods for crafting better prompts are being explored. For ex-\nample, Beurer-Kellner et al. [24] introduce the idea of Lan-\nguage Model Programming (LMP) which combines text-based\nprompting with scripting. Furthermore, Wei et al. [25] shows\nthat the incorporation of Chain-of-Thought (CoT) significantly\nimproves the ability of LLMs to perform complex reasoning.\nUnderstanding the internal mechanisms of LLMs presents a\nsignificant challenge. Current datasets and benchmarks often\nlack the curated data necessary for thorough performance\nanalyses. Therefore, there is a critical need for consistent data\npoints to effectively evaluate the performance of LLMs across\nvarious SE tasks. We argue that well-designed testbeds and\nprompts are the key to accurately assessing LLMs understand-\ning of complex information, such as task-related semantics.\nTo bridge the gap between existing datasets and bench-\nmarks, we developed SnipGen .SnipGen is a framework to\ncollect source code snippets from GitHub. Each snippet is\nautomatically augmented with prompts tailored for various\nsoftware tasks. Practitioners and researchers can query and\ngenerate new prompts according to the SE task and experiment\nwith different configurations for evaluating LLMs for code.\nOur goal is to provide resources that can more accurately\nassess the performance of LLMs and aid in the construction\nof more detailed benchmarks.\nThe contributions of this paper are listed as follows: 1) A\nFramework for mining software repositories and crafting data\npoints augmented with prompts for specific SE downstream\ntasks. 2) a generated testbed comprising Python snippets\nwith calculated features from the AST, natural language,\nand vulnerabilities analysis [26]. 3) Prompt-generated dataset\nwith mutated snippets crafted for Code Completion, Commit\ngeneration, and Code summarization. 4) source code and\ncomplementary material used in this research are published\nin an open-source repository [27].\nII. T HESnipGen FRAMEWORK\nSnipGen is a framework to extract snippets at method\ngranularity from GitHub. SnipGen follow steps for curating thearXiv:2502.07046v2  [cs.SE]  16 Feb 2025\nextracted raw data and take features from the data such as the\nnumber of identifiers, vocabulary, tokens, etc. Features derived\nfrom their AST representations and further complementary\ndata. Our dataset can potentially improve the quality of the\npredictions in downstream tasks by augmenting the prompts,\nthereby enabling LLMs to perform more effectively.\n1Data Collection2Pre-processing\n3\nData V alidationRepository\nFilterCommit\nfilteringMethods &\nComments\nAST\nParsing\nFeature\ncomputation\nDatabase\ninsert\nRemove\nducplicates\nText & Code\nMeaningfulness\nFeature\nvalidation4SE T estbed Generation\nRandomCut code\nDocstring filtering\nCommit filteringSpecificationJaccard\nsimilarity\nFilter\nRaw data\nfilteting\nPrompt\ntemplate\nCode\nSelection\nGenerate\nprompt5\n Model Evaluation\nModel\nExecutionPredicted\nOutcomeAnalysis\nPrompt Generation\n\u00abSinpiGen\u00bb\nData curation\u00abPyDriller\u00bb\nCollector\u00abSinpiGen\u00bb\nFeature extractor\nComment\/\nUncomment\nTestbeds\nIndexed\nData\nHugginfaceArtifacts\nSinpGen\nMiner\n\u00abSinpiGen\u00bb\nTestbed specificationSnipGen ArchitectureVul. Detection\nFig. 1: SnipGen Data collection and prompt generation\nFig. 1 depicts the process followed by SnipGen to gen-\nerate a testbed and the SnipGen architecture. The SnipGen\narchitecture comprises components to collect, curate, store,\nextract, and generate a SE-oriented testbed. The process be-\ngins with 1, the Data Collection phase, where source code\nsnippets are extracted with pydriller library [28] from selected\nrepositories in GitHub given a tie window. A set of snippets\nrepresenting a Python method is extracted from each commit.\nThis is followed by 2, a Pre-processing step, where the -\nData Curation- SnipGen component stores the raw data in a\nMySQL database. Once the data is formatted and saved in the\nstorage, SnipGen looks for exact match snippets and removes\nduplicates. The -Feature extractor- component parses the code\ninto the AST representation using tree-sitter [29] and computes\nassociated features ( i.e.,the number of AST levels, AST nodes,\ncomments, function name).\nThe data validation at step 3is a manual evaluation where\nthe authors confirm the dimension values and the meaningful-\nness of the Docstring and linked code, the two authors first\nselected the docstring with more than 20 words and evaluate\nthe description against the code snippet. The description must\ndepict the steps or intention of the snippet.\nThe testbed generation step 4, filters the raw data, evaluates\nthe Jaccard similarity, and identifies vulnerable code. The\nraw filtering depends on the SE task, for example for code\ncompletion SnipGen filters the valid code with more than two\nlines of code. SnipGen uses CodeQL [30] for vulnerability\ndetection and appends the vulnerability location on the snippet.\nFinally, step 5uses the selected snippets from 4andapplies the prompt template to the aimed SE task generating\na final prompt. SnipGen enables the model evaluation and\nbenchmarking as used in [31]\u2013[33]. The following subsections\ninclude a detailed description of the features of each data point.\nA. Data Point Feature Structure\nsnippet\nID Integer\ncommitID Integer\nrepo String\npath String\nfile_name String\ncommit_message String\ncode String\nurl String\nlanguage Stringast_data\nID Integer\nsnippetID Integer\nurl String\nast_errors String\nn_ast_errors Integer\nast_levels Integer\nn_ast_nodes Integercode_features\nID Integer\nsnippetID Integer\nn_whitespaces Integer\nn_words Integer\nnloc Integer\ncomplexity Integer\ntoken_counts Integer\nn_identifiers Integer\nmutations\nID Integer\nsnippetID Integer\nrandom_cut String\nsignature Stringdocumentation\nID Integer\nsnippetID Integer\ndocstring String\nn_words Integer\nn_whitespaces Integer\nvocab_size Integer\nvulnerabilities\nID Integer\nsnippetID Integer\nspan_position T upleprompts\nID Integer\nsnippetID Integer\nse_task String\nprompt_script String\nFig. 2: SnipGen data schema. The snippet represents the core\ncommit collected with documentation. Linked tables contain\ncalculated features.\nSnipGen can collect a set of Python methods that serve\nas evaluative data points. Each data point has associated\nfeatures at seven dimensions as observed at Fig. 2. These seven\ndimensions describe the static feature from the snippet. We aim\nto link code fragments with their properties. The first dimen-\nsion corresponds to snippets\u2019 identification, which includes\nthecommit id(i.e., commit hash), repository name, path,\nfilename ,funname ,commit message . The second dimension\nis related to the associated documentation docstring . The doc-\nstring extended to complementary natural language features\nsuch as nwords, vocab size, language, and nwhitespaces .\nThe third dimension corresponds to the snippet\u2019s syntactic in-\nformation, which includes the actual code base, nasterrors ,\nnastlevels ,nastnodes ,nwords ,vocab size,token count ,\nand nwhitespaces . The fourth dimension corresponds to\ncanonical software metrics, which include nloc,complexity ,\nnidentifiers . The fifth dimension depicts the span position\nfor vulnerabilities detected from the code snippet. The sixth\ndimension is associated with the snippet mutation when the\ncode is randomly cut one line after the signature, therefore\nSnipGen identifies the signature as the original snippetID and\ncut code. Finally, the seventh dimension comprises the linked\nfeatures to the generated prompt. SnipGen labels the prompt\nto the SE task and the prompt configuration.\nB. Software Engineering Tesbed Task\nData curation, pre-processing, and data validation produce\na testbed oriented to evaluate a model. For instance, a Ran-\ndomCut andWithDocString testbeds might evaluate the model\nTABLE I: Prompt templates for each SE task using collected\nSnipGen features.\nSE Task ID Prompt Template\nCode\ncompletionP1 Complete the following <language >method: <RandomCut >\nP2 You have a <language >function named <signature >, the function starts with\nthe following code <RandomCut >. The function is in charge of <docstring >\nP3 Create a function that accomplish the following functionality in <lan-\nguage>code:<docstring >\nCommit\ngenerationP4 Please describe the following code change to create log message: status before\n<RandomCut >status now <code>\nSumm. P5 I need a summary for the following code: <code>\nProcessing\npromptP6 Change the method signature by <signature >\nP7 Reduce or complete the method using only <nloc>lines of code\nP8 remove comments; remove summary; remove throws; remove function modifiers\nat SE tasks, such as code completion \u2014generating code to fill\nin missing parts of a function. WithDocString testbed selects\nthe snippets with valid documentation and code so the LLM\ninput compresses both a description and code. FromCommit\ntestbed is focused on selecting meaningful commit messages\nand linked source code so that to evaluate either commit gen-\neration \u2014producing commit messages based on code changes\norcode generation producing the complete snippet from the\ndescription. FromDocString testbed select only meaningful\ncode descriptions ( i.e., only docstring ) to generate the code\nsnippet also configuring a code generation case. SnipGen can\nbe used to evaluate code summarization \u2014creating natural\nlanguage descriptions of the functionality implemented in the\nprovided source code. If we select the original code from the\nWithDocString testbed and the ones at the top of docstring\nlength then we can use the testbed for summarization.\nC. Prompt templates\nThe effectiveness of LLMs in code generation is greatly\ninfluenced by prompt design. At this point SnipGen only\nproduces a set of data points that can be organized as an input\nfor an autoregressive LLM since the tesbed contains the input\nand the expected output. SnipGen combines prompt templates\nand gathered data points to build the final prompt input. The\nstructure, keywords, and context of a prompt play a crucial role\nin shaping results and analyses. Prompts can be configured as\na single-step or multi-step, with the latter allowing iterative\nrefinement based on the model\u2019s initial response. Chau et\nal. [34] explore such multi-step configurations. Table I lists\neight prompt templates, practitioners can modify the template\naccording to the evaluation task. From the proposed list,\nP1\u2212P5supports single-step SE tasks, while P6\u2212P8enables\nmulti-step processing by combining prompts to refine outputs.\nFor instance, SnipGen can combine P1 +P8,P3 +P6, or\nP3 +P8for code completion.\nForcode completion ,SnipGen defines three prompts. P1\nasks the model to complete a method from a randomly se-\nlected cut position ( RandomCut ) in the specified programming\nlanguage ( language ).P2extends P1by including additional\ndetails, such as the method\u2019s signature and docstring .P3,\nin contrast, provides only an NL description extracted from\nthe method\u2019s docstring . In commit generation, P4instructs\nthe model to create an NL description of the changes madeto transform the RandomCut version of a method into its\ncomplete code ( code ).P5is designed to ask the model to\ngenerate the commit message from the mutated code and the\nactual code. Lastly, in code summarization ,P6provides only\nthe code, which the model uses to generate a corresponding\nsummary.\nD. SnipGen Prompt Generation and Use\nSE Task Task Context Prompt Template\nSampling Snippet Mutation Prompt  Generation\nModel Evaluation Training Testing\nCanonical\nDatasetsRaw\nDatasetA\nB\nCanonical Evaluation Code generationSummarization &\nCommit GenerationSnipGen\nFig. 3: SnipGen Dataset and use. Adescribes the SnipGen data\ncollection and steps until prompt generation. Bdescribes the\ncanonical path for training and evaluate LLMs\nTheSnipGen framework is designed to select a SE task and\nevaluate a LLM using the testbed with a given context with\na designed prompt. Fig. 3 depicts the options a practitioner\nhas to evaluate a LLM. The database supports a query to\nfilter the snippets according to the SE task, for instance for\ncode completion we can sample the snippets with linked\ndocstring with more than 10 words, this provides the task\ncontext (see, Fig. 3 section A). The prompt generation might\ncontain a mutated snippet such as RandomCut to perform the\nrequired task. For example, for code completion, we will need\na partial code snippet that must be auto-completed by the LLM\ntherefore we need to cut the original snippet smartly. SnipGen\ncan use the RandomCut method to split the code beyond the\nmethod signature. Practitioners can still evaluate the model\nusing canonical datasets and metrics to compare against the\nnew collected SnipGen testbed.\nIII. E XPERIENCE REPORT\nIn this section, we describe our experience of using SnipGen\nfor collecting a testbed and generating a set of prompts. We\nalso briefly describe three use cases illustrating how SnipGen\nwas successfully used to evaluate LLMs for code.\nA. SnipGen Testbed Generation\nThe experience with SnipGen begins by mining reposi-\ntories from GitHub, as detailed in Sec. II. We focused on\nthe most popular Python repositories, applying the follow-\ning query filters: language:Python fork:false size: >= 30000\npushed: >2021-12-31 stars: >2000 .The query gathers the\nmost popular repositories in Python. We selected the top\n200 repositories including keras, numpy, pandas, sentry, etc.\nWe extracted the new snippets reported on commits between\n2022 and 2023 from selected repositories. Then we used the\ndata curation to remove duplicates and feature extraction to\ngenerate and extract the associated features. We configured\nTABLE II: Dataset size and deduplication percentage\nSE Task Testbed I\/O Dupes Dupe % Size Prompts\nCode\nCompletionRandomCut code\u21d2code 120 2.4% 4880 9760\nWithDocString code&text \u21d2code 145 2.9% 4855 9710\nCode\nGenerationFromDocString text\u21d2code 76 1.5% 4924 14772\nFromCommit text\u21d2code 97 1.9% 4903 4903\nSumarization SummarizationGen code\u21d2text 156 3.1% 4844 4844\nVulnerabilities VulnerabilitySpan code\u21d2code 2 0.4% 410 410\na0.7similarity threshold [35], [36] to de-duplicate snippets\nusing HuggingFace tokenizer BPE. SnipGen saves the raw data\nand their features into a JSON and a database. We randomly\nvalidated 960 out of \u2248227Kdata points to confirm the\nextracted features and the meaningfulness of the Docstring\nand linked code.\nWe sampled until 5kdata points from the RawData testbed\nto construct six testbeds, each tailored for a specific SE task\nas described at Sec. II-B. To create RandomCut , we selected\ndata points with more than 10tokens or 100 characters, and\nsubsequently, each data point was randomly truncated after the\nmethod signature. For SummarizationGen and FromCommit ,\nwe filtered RawDataDocstring data points with more than\n10 words or 50 characters. Table. II provides information\nabout the SE task associated with each curated testbed, the\npercentage of detected duplicates, the final size, and the\ngenerated number of prompts.\nB. Successful Use Cases\nGaleras [31]: Galeras is a benchmark for measuring the\ncausal effect of SE prompts for code completion. Galeras\nconfigures a set of treatments to assess the influence of\npotential confounders on the outcomes of ChatGPT ( i.e.,GPT-\n4). The selected confounders are: prompt size(from prompts),\nnwhitespaces (from documentation), token counts , and nloc\n(from code features). This use case of SnipGen demonstrates\nthat prompt engineering strategies (such as those listed in\nTable I - processing prompt) have distinct causal effects on\nthe performance of ChatGPT.\nSyntaxEval [33]: In this use case Syn taxEval evaluates\nthe ability of Masked Language Models ( i.e.,Encoder-based\nTransformers) to predict tokens associated with specific types\nin the AST representation ( i.e.,syntactic features). SyntaxEval\nused SnipGen to construct a code completion testbed with ap-\nproximately 50KPython snippets. SyntaxEval aims to account\nfor potential confounders such as astdata andcode features\n(illustrated in Fig. 2), the analysis revealed no evidence that\nthe evaluated syntactic features influenced the accuracy of the\nselected models\u2019 predictions.\nASTxplainer [32]: ASTxplainer is an explainability method\ndesigned to assess how effectively a LLM ( e.g., decoder-\nbased transformers) predicts syntactic structures. ASTxplainer\naggregates next-token prediction values through syntactic de-\ncomposition, quantified as AsC-Eval values to evaluate the\neffectiveness. ASTxplainer findings reveal that the ability to\npredict syntactic structures strongly depends on the LLM\u2019s\nparameter size and fine-tuning strategy. Furthermore, causal\nanalysis controlling for confounding variables (e.g., astdataandcode features ) shows that AsC-Eval values at the snippet\nlevel negatively impact the cross-entropy loss of the evaluated\nLLMs.\nIV. S IMILAR DATASETS\nSignificant efforts have produced datasets for evaluating\nLLMs in SE tasks, including DeepFix for program repair [37],\nCodeContest and CoNaLa for program synthesis [38], [39],\nand CodeSearchNet for code retrieval [40]. Expansions like\nCodeXGLUE [41], xCodeEval [42] target broader tasks, while\nbenchmarks such as HumanEval and SecurityEval focus on\nfunctional correctness and vulnerabilities [43], [44]. Despite\nthese efforts, existing datasets often suffer from contamination\n[20], [45], with overlaps between training and evaluation data,\nand benchmarks are prone to memorization by models [46],\nlimiting their effectiveness in assessing true generalization.\nRecent research work has explored the dynamic generation\nof prompts and testbeds, for instance, EvoPrompt is a frame-\nwork for automatic discrete prompt optimization that connects\nLLMs with Evolutionary Algorithms [47]. Evol-instruct is a\nsystematic approach to generate instruction-response pairs by\niteratively improving prompts and responses through model\nself-enhancement [48]. LiveCodeBench is a benchmark for\nevaluating LLMs designed to generate code [20]. Unlike Snip-\nGen, LiveCodeBench addresses issues of data contamination\nby using continuously updated problems from online coding\ncompetitions.\nV. L IMITATIONS AND FUTURE WORK\nDocumentation Quality Analysis: Meaningfulness evalua-\ntion for the docstring and linked code can not be automatized\nand depends on the project context. To handle this limitation,\nwe conducted a manual validation. As part of future work,\nSnipGen should streamline the manual validation process and\nascertain the significance of comments and documentation\nwithin the snippets.\nVulnerability Detection: The detection of vulnerabilities is\nreliant solely on the CodeQL tool and its updates; we did not\nemploy any alternative tools to validate these results.\nAssumption Regarding Snippet Exposure :SnipGen miti-\ngates to select \u201ccontaminated\u201d data ( i.e.,already seen snippets)\nby selecting snippets from specific commit time windows. A\npractitioner can specify the time windows depending on the\nLLM release date. SnipGen aims to reduce data contamination\nby including a prompt and variating the cut code. However,\nit\u2019s important to note that the extracted code changes might\ninclude older lines of code or reused code fragments. Our\nevaluation does not encompass the entire project history to\nidentify older references.\nFor future work, 1) we propose to extend this dataset to\nsupport multiple programming languages; 2) Integrate a wider\nnumber of SE tasks. 3) Rank each data point according to the\ncyclomatic complexity number of AST nodes, documentation,\nand number of identifiers. The rank will prove better criteria\non which snippets are more interesting to evaluate the LLM.\nREFERENCES\n[1] J. Austin, A. Odena, M. Nye et al. , \u201cProgram synthesis with large\nlanguage models,\u201d 2021.\n[2] D. Hendrycks, S. Basart, S. Kadavath et al. , \u201cMeasuring coding chal-\nlenge competence with APPS,\u201d CoRR , vol. abs\/2105.09938, 2021.\n[3] M. Chen, J. Tworek, H. Jun et al. , \u201cGeneration Probabilities are Not\nEnough: Improving Error Highlighting for AI Code Suggestions,\u201d 2021,\npublisher: arXiv Version Number: 2.\n[4] M. White, C. Vendome, M. Linares-Vasquez et al. , \u201cToward deep learn-\ning software repositories,\u201d in 2015 IEEE\/ACM 12th Working Conference\non Mining Software Repositories , 2015, pp. 334\u2013345.\n[5] M. Ciniselli, N. Cooper, L. Pascarella et al. , \u201cAn empirical study on the\nusage of transformer models for code completion,\u201d IEEE Transactions\non Software Engineering , vol. 48, no. 12, pp. 4818\u20134837, 2022.\n[6] A. LeClair, A. Bansal, and C. McMillan, \u201cEnsemble Models for\nNeural Source Code Summarization of Subroutines,\u201d Jul. 2021,\narXiv:2107.11423 [cs].\n[7] K. Moran, A. Yachnes, G. Purnell et al. , \u201cAn empirical investigation into\nthe use of image captioning for automated software documentation,\u201d in\n2022 IEEE International Conference on Software Analysis, Evolution\nand Reengineering (SANER) , 2022, pp. 514\u2013525.\n[8] R. Tufano, L. Pascarella, M. Tufano et al. , \u201cTowards automating code\nreview activities,\u201d in 2021 IEEE\/ACM 43rd International Conference on\nSoftware Engineering (ICSE) , 2021, pp. 163\u2013174.\n[9] R. Tufano, S. Masiero, A. Mastropaolo et al. , \u201cUsing pre-trained models\nto boost code review automation,\u201d in 2022 IEEE\/ACM 44th International\nConference on Software Engineering (ICSE) , 2022, pp. 2291\u20132302.\n[10] A. T. Nguyen and T. N. Nguyen, \u201cGraph-based statistical language\nmodel for code,\u201d in ICSE\u201915 . IEEE Press, 2015, p. 858\u2013868.\n[11] M. White, M. Tufano, C. Vendome et al. , \u201cDeep learning code frag-\nments for code clone detection,\u201d in 2016 31st IEEE\/ACM International\nConference on Automated Software Engineering (ASE) , 2016, pp. 87\u201398.\n[12] M. Tufano, C. Watson, G. Bavota et al. , \u201cDeep learning similarities\nfrom different representations of source code,\u201d in 2018 IEEE\/ACM 15th\nInternational Conference on Mining Software Repositories (MSR) , 2018,\npp. 542\u2013553.\n[13] \u2014\u2014, \u201cLearning How to Mutate Source Code from Bug-Fixes,\u201d ICSME\n2019 , pp. 301\u2013312, 2019.\n[14] Y . Zhou, S. Liu, J. Siow et al. , \u201cDevign: Effective Vulnerability\nIdentification by Learning Comprehensive Program Semantics via Graph\nNeural Networks.\u201d\n[15] M. White, M. Tufano, M. Mart \u00b4\u0131nez et al. , \u201cSorting and transforming\nprogram repair ingredients via deep learning code similarities,\u201d in 2019\nIEEE 26th International Conference on Software Analysis, Evolution\nand Reengineering (SANER) , 2019, pp. 479\u2013490.\n[16] M. Tufano, J. Pantiuchina, C. Watson et al. , \u201cOn learning meaningful\ncode changes via neural machine translation,\u201d in 2019 IEEE\/ACM 41st\nInternational Conference on Software Engineering (ICSE) , 2019, pp.\n25\u201336.\n[17] M. Tufano, C. Watson, G. Bavota et al. , \u201cAn empirical investigation\ninto learning bug-fixing patches in the wild via neural machine transla-\ntion,\u201d in 2018 33rd IEEE\/ACM International Conference on Automated\nSoftware Engineering (ASE) , 2018, pp. 832\u2013837.\n[18] Z. Chen, S. Kommrusch, M. Tufano et al. , \u201cSequencer: Sequence-to-\nsequence learning for end-to-end program repair,\u201d IEEE Transactions\non Software Engineering , vol. 47, no. 9, pp. 1943\u20131959, 2021.\n[19] A. Connor, A. Harris, N. Cooper et al. , \u201cCan we automatically fix bugs\nby learning edit operations?\u201d in 2022 IEEE International Conference\non Software Analysis, Evolution and Reengineering (SANER) . Los\nAlamitos, CA, USA: IEEE Computer Society, mar 2022, pp. 782\u2013792.\n[20] N. Jain, K. Han, A. Gu et al. , \u201cLiveCodeBench: Holistic and Contami-\nnation Free Evaluation of Large Language Models for Code,\u201d Jun. 2024,\narXiv:2403.07974 [cs].\n[21] J. Wei, Y . Tay, R. Bommasani et al. , \u201cEmergent Abilities of Large\nLanguage Models,\u201d Oct. 2022, arXiv:2206.07682 [cs].\n[22] Y . Zhou, A. I. Muresanu, Z. Han et al. , \u201cLarge language models are\nhuman-level prompt engineers,\u201d ArXiv , vol. abs\/2211.01910, 2022.\n[23] J. White, S. Hays, Q. Fu et al. , \u201cChatgpt prompt patterns for improving\ncode quality, refactoring, requirements elicitation, and software design,\u201d\nArXiv , vol. abs\/2303.07839, 2023.\n[24] L. Beurer-Kellner, M. Fischer, and M. Vechev, \u201cPrompting is pro-\ngramming: A query language for large language models,\u201d Proc. ACM\nProgram. Lang. , vol. 7, no. PLDI, jun 2023.[25] J. Wei, X. Wang, D. Schuurmans et al. , \u201cChain-of-Thought Prompt-\ning Elicits Reasoning in Large Language Models,\u201d Jan. 2023,\narXiv:2201.11903 [cs].\n[26] D. Rodriguez-Cardenas, \u201cSnipgen tesbed to evaluate llms for code,\u201d\nhttps:\/\/doi.org\/10.5281\/zenodo.14279563, January 2025.\n[27] S. R. Group, \u201cSnipgen: A code snippet generation tool,\u201d https:\/\/github.\ncom\/WM-SEMERU\/snipgen, 2025, accessed: 2025-01-30.\n[28] PyDriller Contributors, \u201cPydriller documentation,\u201d https:\/\/pydriller.\nreadthedocs.io\/en\/latest\/, n.d., accessed: 2024-11-29.\n[29] Tree-Sitter Contributors, \u201cTree-sitter documentation,\u201d https:\/\/tree-sitter.\ngithub.io\/tree-sitter\/, n.d., accessed: 2024-11-29.\n[30] GitHub, \u201cAbout codeql,\u201d https:\/\/codeql.github.com\/docs\/\ncodeql-overview\/about-codeql\/, n.d., accessed: 2024-11-29.\n[31] D. Rodriguez-Cardenas, D. N. Palacio, D. Khati et al. , \u201c Benchmarking\nCausal Study to Interpret Large Language Models for Source Code ,\u201d\nin2023 IEEE International Conference on Software Maintenance and\nEvolution (ICSME) . Los Alamitos, CA, USA: IEEE Computer Society,\nOct. 2023, pp. 329\u2013334.\n[32] D. N. Palacio, A. Velasco, D. Rodriguez-Cardenas et al. , \u201cEvaluating\nand Explaining Large Language Models for Code Using Syntactic\nStructures,\u201d Aug. 2023, arXiv:2308.03873.\n[33] A. Velasco, D. N. Palacio, D. Rodriguez-Cardenas et al. , \u201cWhich syn-\ntactic capabilities are statistically learned by masked language models\nfor code?\u201d in Proceedings of the 2024 ACM\/IEEE 44th International\nConference on Software Engineering: New Ideas and Emerging Results ,\nser. ICSE-NIER\u201924. New York, NY , USA: Association for Computing\nMachinery, 2024, p. 72\u201376.\n[34] C. Liu, X. Bao, H. Zhang et al. , \u201cImproving ChatGPT Prompt for Code\nGeneration,\u201d May 2023, arXiv:2305.08360 [cs].\n[35] M. Allamanis, \u201cThe adverse effects of code duplication in machine\nlearning models of code,\u201d in OOPLSA , 2019, pp. 143\u2013153.\n[36] C. Wang, K. Cho, and J. Gu, \u201cNeural Machine Translation with Byte-\nLevel Subwords,\u201d Dec. 2019, arXiv:1909.03341.\n[37] R. Gupta, S. Pal, A. Kanade et al. , \u201cDeepFix: Fixing Common C Lan-\nguage Errors by Deep Learning,\u201d Proceedings of the AAAI Conference\non Artificial Intelligence , vol. 31, no. 1, Feb. 2017, number: 1.\n[38] Y . Li, D. Choi, J. Chung et al. , \u201cCompetition-Level Code Generation\nwith AlphaCode,\u201d Feb. 2022, arXiv:2203.07814.\n[39] P. Yin, B. Deng, E. Chen et al. , \u201cLearning to mine aligned code and\nnatural language pairs from stack overflow,\u201d in International Conference\non Mining Software Repositories , ser. MSR. ACM, 2018, pp. 476\u2013486.\n[40] H. Husain, H.-H. Wu, T. Gazit et al. , \u201cCodeSearchNet chal-\nlenge: Evaluating the state of semantic code search,\u201d arXiv preprint\narXiv:1909.09436 , 2019.\n[41] S. Lu, D. Guo, S. Ren et al. , \u201cCodeXGLUE: A machine learning\nbenchmark dataset for code understanding and generation.\u201d\n[42] M. A. M. Khan, M. S. Bari, X. L. Do et al. , \u201cxCodeEval: A Large Scale\nMultilingual Multitask Benchmark for Code Understanding, Generation,\nTranslation and Retrieval,\u201d Nov. 2023, arXiv:2303.03004.\n[43] M. Chen, J. Tworek, H. Jun et al. , \u201cEvaluating Large Language Models\nTrained on Code,\u201d Jul. 2021, arXiv:2107.03374 [cs].\n[44] M. L. Siddiq and J. C. S. Santos, \u201cSecurityeval dataset: Mining vul-\nnerability examples to evaluate machine learning-based code generation\ntechniques,\u201d in Proceedings of the 1st International Workshop on Min-\ning Software Repositories Applications for Privacy and Security , ser.\nMSR4P&S 2022. New York, NY , USA: Association for Computing\nMachinery, 2022, p. 29\u201333.\n[45] A. Yadav, H. Beniwal, and M. Singh, \u201cPythonSaga: Redefining the\nBenchmark to Evaluate Code Generating LLMs,\u201d in Findings of the\nAssociation for Computational Linguistics: EMNLP 2024 . Miami,\nFlorida, USA: Association for Computational Linguistics, 2024, pp.\n17 113\u201317 126.\n[46] D. Ramos, C. Mamede, K. Jain et al. , \u201cAre large language models\nmemorizing bug benchmarks?\u201d 2024.\n[47] Q. Guo, R. Wang, J. Guo et al. , \u201cConnecting Large Language Models\nwith Evolutionary Algorithms Yields Powerful Prompt Optimizers,\u201d Feb.\n2024, arXiv:2309.08532 [cs].\n[48] C. Xu, Q. Sun, K. Zheng et al. , \u201cWizardLM: Empowering Large\nLanguage Models to Follow Complex Instructions,\u201d Jun. 2023,\narXiv:2304.12244.","filename":"SnipGen__A_Mining_Repository_Framework_for_Evaluating_LLMs_for_Code.pdf","responses":"[1] Machine Learning For Code: Exploration and evaluation of machine learning models for code-related tasks in software engineering, including code generation, code understanding, dataset curation, and prompt engineering for code tasks."}
{"id":"pdf_3","text":"Are Large Language Models Memorizing\nBug Benchmarks?\nDaniel Ramos\nCarnegie Mellon University, INESC-ID\nPittsburgh, PA, USA\ndanielrr@cmu.eduClaudia Mamede*\nCarnegie Mellon University, FEUP\nPittsburgh, PA, USA\ncmamede@andrew.cmu.eduKush Jain*\nCarnegie Mellon University\nPittsburgh, PA, USA\nkdjain@andrew.cmu.edu\nPaulo Canelas*\nCarnegie Mellon University, LASIGE\nPittsburgh, PA, USA\npasantos@andrew.cmu.eduCatarina Gamboa*\nCarnegie Mellon University, LASIGE\nPittsburgh, PA, USA\ncgamboa@andrew.cmu.eduClaire Le Goues\nCarnegie Mellon University\nPittsburgh, PA, USA\nclegoues@andrew.cmu.edu\nAbstract \u2014Large Language Models (LLMs) have become inte-\ngral to various software engineering tasks, including code gener-\nation, bug detection, and repair. To evaluate model performance\nin these domains, numerous bug benchmarks containing real-\nworld bugs from software projects have been developed. However,\na growing concern within the software engineering community\nis that these benchmarks may not reliably reflect true LLM\nperformance due to the risk of data leakage. Despite this concern,\nlimited research has been conducted to quantify the impact of\npotential leakage.\nIn this paper, we systematically evaluate popular LLMs to\nassess their susceptibility to data leakage from widely used\nbug benchmarks. To identify potential leakage, we use multiple\nmetrics, including a study of benchmark membership within com-\nmonly used training datasets, as well as analyses of negative log-\nlikelihood and 5-gram accuracy . Our findings show that certain\nmodels, in particular codegen-multi , exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J ,\nwhile newer models trained on larger datasets like LLaMa 3.1\nexhibit limited signs of leakage. These results highlight the need\nfor careful benchmark selection and the adoption of robust\nmetrics to adequately assess models capabilities.\nIndex Terms \u2014Automated Program Repair, Large Language\nModel, Data Leakage\nI. I NTRODUCTION\nLarge language models (LLMs) have become ubiquitous for\nvarious software engineering tasks. Assessing these models\u2019\nabilities in context, beyond the basic evaluations typically\nperformed upon release (e.g., on HumanEval [1]), benefits\nfrom realistic benchmarks that represent real-world software\ndevelopment tasks. Two significant such tasks are bug find-\ning, through automated fault localization (FL) [2]; and bug\nfixing, through automated program repair [3] (APR). The\nSoftware Engineering community has released numerous bug\nbenchmarks for evaluating success on these tasks, consisting\nof real bugs from open-source software projects. Notable\nsuch datasets include, for example, Defects4J [4] ( Java ) and\nBugsInPy [5] ( Python ); similarly, ML researchers recently\nintroduced SWEBench [6].\n*Equal contributionHowever, a growing concern in software engineering\nresearch is the degree to which data leakagecompromises the\nevaluation of true model capability [7], [8]. Data leakage\nrefers to the use of information during model training\nthat would not normally be available during prediction,\nleading to inflated performance metrics that misrepresent a\nmodel\u2019s true effectiveness. The programs and bugs in many\nbenchmarks and solutions have been publicly accessible for\nyears, increasing the chance they were incorporated into LLM\ntraining data. For instance, the widely-used Defects4J dataset,\nbased on popular Java projects, was first released in 2014.\nTo illustrate, consider the example shown in Figure 1. Here,\nwe prompted codegen-multi (6 billion parameters) to predict\nthe next tokens based on an incomplete snippet (highlighted\nin yellow) from the bug-fix file for Defects4J bug #39 in\ntheLang project. The generated text, highlighted in blue,\nshows that codegen-multi reproduced the remainder of the\nsolution file verbatim, including specific comments (lines 31\nand 35), token by token. This exact reproduction strongly sug-\ngests that the model has memorized the benchmark solution.\nSuch behavior underscores the need for caution when using\nbenchmarks like Defects4J , as they may lead to misleading\nconclusions about a model\u2019s generalizability.\nResearchers in other domains, like NLP or ML, have at-\ntempted to quantify data leakage in their datasets [9], [10]. To\nthe best of our knowledge, this effort has not been undertaken\nfor popular bug datasets in Software Engineering. Detecting\nleakage is challenging. LLMs are often pre-trained by organi-\nzations that do not disclose their datasets, the total volume\nof data, nor model parameters. Consequently, determining\nwhether a benchmark has been directly included or merely\nmirrored in the model\u2019s training set is difficult. Even verifying\nthe presence of data only establishes that the model has seen\nit, not necessarily that it has memorized it, particularly when\nit is one training sample among billions.\nThus, we ask: Are large language models memorizing\nbug benchmarks? We systematically evaluate popular LLMs\nto quantify their susceptibility to data leakage on widely usedarXiv:2411.13323v3  [cs.SE]  31 Mar 2025\n1 * * ommited for brevity * *\n2\/\/ no search strings found, we are done\n3if (textIndex == -1) {\n4 return text;\n5}\n6int start = 0;\n7\/\/ get a good guess on the size of the result buffer so\n8\/\/ it doesnt have to double if it goes over a bit\n9int increase = 0;\n10\/\/ count the replacement text elements that are larger\n11\/\/ than their corresponding text being replaced\n12for (int i = 0; i < searchList.length; i++) {\n13 if (searchList[i]==null || replacementList[i]==null) {\n14 continue;\n15 }\n16 int greater = replacementList[i].length() -\n17 searchList[i].length();\n18 if (greater > 0) {\n19 increase += 3 * greater; \/\/ assume 3 matches\n20 }\n21}\n22\/\/have upper-bound at 20 %increase, then let Java take...patch\nPrompt input to codegen-multi .\nBoth codegen-multi output and the Defects4J solution.\nFig. 1: Excerpt from Defects4J (Lang:Bug 39 ). Given the first\nlines of the function until line 11, codegen-multi generated\nlines 12 to 23, matching the benchmark solution.\nbug benchmarks and raise awareness of the risks associated\nwith using established benchmarks, which may inadvertently\ninflate performance by testing memorized data.\nWe use multiple metrics to detect potential leakage. First,\nwe investigate whether benchmark data has membership within\nTheStack , a widely-used used pretraining code dataset. Fol-\nlowing this, we apply two core metrics for leakage from prior\nwork [9], [10]: Negative Log-Likelihood ( NLL)and 5-gram\naccuracy .NLL provides insight into model familiarity with\ncode snippets; 5-gram accuracy assesses the model\u2019s ability\nto reproduce exact sequences. We apply these metrics to both\nwell-known bug benchmarks, and a new dataset of high-quality\ncode repositories from 2024, which we mined from GitHub .\nThis new dataset is less likely to have appeared in models\u2019\ntraining, which allows us to compare model performance\nbetween potentially familiar data versus likely novel code.\nOur findings suggest that older models, in particular\ncodegen-multi , exhibit very high 5-gram accuracy and low\nNLLon benchmark data, indicating a higher likelihood of mem-\norization. Our evidence suggests that newer models trained on\nmore extensive datasets, like LLaMa 3.1 , show less memoriza-\ntion. Nonetheless, across all metrics, and models, Defects4J\n\u2014 arguably the most widely-used bug benchmark \u2014 con-\nsistently exhibits the highest rate of potential memorization.\nThese results underscore the importance of carefully selecting\nbenchmarks to ensure reliable evaluations.TABLE I: Evaluation benchmark statistics, including newer\nand older benchmarks to assess leakage reduction over time.\nBenchmark Year # Bugs LOC (k) # Stars Language\nDefects4J (v1.5) 2019 438 321 736 Java\nBugsInPy 2020 493 1253 80 Python\nBugsC++ 2021 209 4297 42 C++\nGitBug-Java 2023 199 257 26 Java\nSWEBenchLite 2023 300 3148 1954 Python\nNew Java Repos 2024 - 132 >100 Java\nNew Python Repos 2024 - 65 >100 Python\nII. M ETHODOLOGY\nFigure 2 overviews our methodology, which comprises three\nmajor components: (1) data collection (Section II-A, (2) model\nselection (Section II-B), and (3) evaluation (Section III).\nA. Data Collection & Filtering.\nWe select widely used bug benchmarks across common pro-\ngramming languages, gathering ground-truth files containing\nreference solutions for each bug fix. To provide a likely not-\nleaked datasets for comparison, we also curated a set of recent\nopen-source repositories from GitHub .\nTo collect benchmarks of interest, we reviewed\nprogram-repair.org and selected highly starred benchmarks\nacross three programming languages: BugsCpp [11],\nDefects4J [4], and BugsInPy [5]. We next included two\nrecent datasets to serve as reference points: GitBug-Java [12],\nwhich was recently published to address the leakage issue,\nand SWEBench-Lite [6], due to its rising popularity for\ncode-related tasks [13], [14], [15]. Table I shows details,\nincluding release year, bug count, lines of code, stars, and\nlanguage. We choose SWEBench-Lite instead of SWEBench\ndue to computational constraints.\nWe collected the ground truth files for each patched bug. To\nreduce computational costs and prevent bias towards particular\nfiles, we removed files with more than 85% overlap to produce\na unique sample set. Duplicate files appear when multiple bugs\nin a dataset occur in the same file (e.g., Defects4J in project\nLang , Bugs 1 and 3 affect the same file). We kept the oldest\nfile for consistency. We collected only fixed files since these\ncorrespond to the solutions a model may have memorized.\nWe collected a new dataset of 3,214 GitHub repositories\nwritten in Java (1,183) and Python (2,031), targeting reposi-\ntories from 2024 to reduce the likelihood that state-of-the-art\nLLMs have seen them. To focus on high-quality repositories,\nwe applied a 100-star minimum threshold as a proxy for\ncommunity interest. To ensure novelty, we used MinHash [16]\nwith Locality Sensitive Hashing (LSH) [17] to filter repos-\nitories overlapping with existing training data. Due to code\nduplication, some files in 2024 repositories might overlap\nwith older repositories. Therefore, we included repositories\nfrom 2022\u20132023 (>100 stars) to exclude 2024 repositories\ncontaining duplicated data. Finally, we randomly sampled 250\nfiles per language to manage computational resources for\nevaluation.\nMost used benchmarks from \nprogram-repair .org\/benchmarksDATA COLLECTION\nDEDUPLICA TIONNEW & UNSEEN \nGITHUB REPOSIT ORIES MODEL  SELECTION\nCodeGen  (6B)\nCodeLLama (7B)\nLLama  3.1 (8B)\nStarCoder 2  (7B)\nCodeGemma  (7B)\nGemma 2  (2B)\nMistral (7B)LLama 3.1  (70B)\nGemma 2  (27B)LEAKAGE DETECTION\n5-GRAM ACC NLL MEMBERSHIP\nGithub repositories written in \nPython and Java\nDEDUPED BENCHMARKS\nDEDUPED\nBENCHMARKS\nintmain (SKY intmain ( int HOW SURPRISED A\nMODEL  IS WHEN\nPREDICTING THE\nNEXT  WORDHOW OFTEN A MODEL\nACCURA TELY\nPREDICTS A SEQUENCE\nOF N TOKENSFig. 2: Overview of our methodology for detecting leakage. We collected bug benchmarks and unseen repositories from 2024.\nWe evaluated NLLandN-gram accuracy on base models, and analyzed membership of the benchmarks in TheStack .\nTABLE II: Models used for evaluation, including their training\nbudget in trillions of tokens, number of layers, and cutoff year.\nModel Tokens (T) Cutoff Year\nCodegen Multi (6B) [18] 1 2022\nCodeLlama (7B) [19] 2.5 2023\nLlaMa 3.1 (8B \/ 70B) [20] 15.0 2024\nStarCoder 2 (7B) [21] 3.5 2024\nGemma 2 (2B \/ 27B) [22] 2.0 \/ 13.0 2024\nCodeGemma (7B) [23] 6.5 2024\nMistral (7B) [24] - -\nB. Model Selection.\nWe select a combination of models used for fault localiza-\ntion [25], program repair [26], and vulnerability detection [27].\nTable II shows model information.These models are from fam-\nilies of well-known code-related models, including codegen-\nmulti ,LLaMa 3.1 ,Gemma 2 ,StarCoder , and Mistral .\nNote that we focus on open-source base models because\nour method requires computing the negative log-likelihood\n(NLL) on sequences, which is generally not possible with\nclosed-source models. We exclude instruction-tuned models\nand concentrate solely on pretrained models before fine-tuning.\nSince instruction-tuned models are optimized for conversa-\ntional formats, n-gram accuracy may be a less suitable metric\nfor measuring memorization in these models.\nC. Leakage Detection\nWe follow strategies from prior work [9], [10] to evaluate\nmodels for potential data leakage. Membership operates at the\nrepository level; Negative Log Likelihood andN-gram accu-\nracy, at the file level (i.e., the compute model familiarity with\na given file). In the bug datasets, these are the fixed (patched)\nfiles; in our novel dataset, they are randomly sampled files\n(Section II-A).\nMembership : If a repository is included in a widely used\npretraining dataset, many models have probably seen that\nrepository\u2019s code. We do not have direct access to, nor\nknowledge of, the training datasets for all evaluation models.\nHowever, we have partial information about the use of pre-\ntraining datasets, such as for open-source models, and closed-\nsource models are likely to use them as well. We therefore\nassess membership via whether a benchmark\u2019s repositories arepresent in TheStack [28], a dataset of permissibly licensed\nsource code in 358 programming languages intended for\ntraining and fine-tuning code-based models. Given its size and\npopularity, several models report having trained on it, such as\nStarCoder 2 [21]; other closed-source models are likely to also\nuse it. We used the Am I in the Stack tool1on each benchmark\nrepository, across the several versions of TheStack .\nNegative Log Likelihood ( NLL):NLL evaluates how closely\nan input sequence aligns with patterns the model has learned\nduring training in terms of how \u201cnatural\u201d the sequence\nappears to the model. If the model has seen a data point\nduring training, we expect it to have a lower NLL on that\npoint compared to unseen data. If the model has encountered\na data point many times during training, NLLis expected to be\nparticularly low (i.e., close to zero) compared to arbitrary code.\nTo compute NLL, we use the reference implementation\npublicly available on HuggingFace.2Calculating the exact NLL\nfor lengthy sequences is usually impractical because LLMs are\ntrained on the limited context (moreover, we cannot fit an en-\ntire sequence in memory). Therefore, we split lengthy solution\nfiles into overlapping chunks, processed them individually, and\ncombined them using a striding technique. We use strides of\n512 tokens when a sequence does not fit into the model\u2019s\ncontext window.\nN-gram accuracy :N-gram accuracy measures the extent to\nwhich a model\u2019s output exactly matches a reference sequence\nat the level of n-grams (i.e., contiguous sequences of nto-\nkens). High n-gram accuracy indicates that the model\u2019s output\nclosely resembles the reference text, suggesting memorization.\nN-gram accuracy of 1.0 indicates the model can produce a\nsequence verbatim.\nWe follow prior work [9] and use 5-grams ( 5-grams strike\na balance between compute efficiency and metric accuracy).\nSince most files cannot fit the context window, we use striding\nto cover the entire sequence. Following Xu et al. [9], we\ncompute 5-grams from five uniformly distributed starting\npoints per stride. For each starting point, we provide the model\nwith the preceding context, and check whether the predicted\nstring matches ground truth.\n1https:\/\/huggingface.co\/spaces\/bigcode\/in-the-stack\n2https:\/\/huggingface.co\/docs\/transformers\/en\/perplexity\nFig. 3: NLLby model and dataset. NLLis not comparable across models in different families, only across benchmarks within a\nfamily. NLLfor other models are consistent with the results displayed.\nTABLE III: Percentage of repositories in each benchmark\nleaked in TheStack versions 1.0, 2.0 and 2.1.\nBenchmark v1.0 (%) v2.0 (%) v2.1 (%)\nGitBug-Java 61.1 42.6 38.9\nBugsInPy 94.1 64.7 64.7\nBugsC++ 60.9 60.9 65.2\nDefects4J 80.0 80.0 80.0\nSWEBench-Lite 83.3 91.7 83.3\nIII. R ESULTS\nThis section presents results assessing possible leakage of\nbug benchmarks in base models, using the metrics described\nin Section II-C: membership in TheStack (Section III-A),\nNegative Log Likelihood (Section III-B), and 5-gram accu-\nracy (Section III-C). We also perform a regression analysis\nof model characteristics, NLL, and 5-gram accuracy (Sec-\ntion III-D) to better understand characteristics of models that\ninfluence data leakage. (Note that we subsequently discuss\nimplications in Section IV, and limitations and threats to the\nvalidity of our experiments in Section V.)\nA. Membership\nTable III shows benchmark membership in three versions\nofTheStack .3The table excludes our new Java and Python\ndata from 2024, as TheStack only includes data to 2023.\nOf all repositories, the new GitBug-Java benchmark has the\nlowest membership. TheStack contains high proportions of\nDefects4J andSWEBench-Lite .\nWhile membership does not necessarily mean a model\ntrained on TheStack has seen a specific fixed file (e.g., if a bug-\nfixing patch was applied after the dataset\u2019s cut-off date), the\nmodel may still be familiar with a project\u2019s source code. This\nfamiliarity could lead to higher-quality patches or results. This\nis not inherently problematic but is a critical factor to consider\nwhen assessing the model\u2019s potential for generalization.\nB. Negative Log Likelihood\nFigure 3 shows NLL values for families of open-source\nmodels, allowing us to examine trends in familiarity across\n3V1.0 is the initial 3TB of permissively licensed code, 2.0 expands to 15TB\nof code, and V2.1 eliminates \u201copt-out\" data.benchmarks.Note that Negative Log Likelihood ( NLL) depends\non tokenization and architecture. This means we can only\ndirectly compare NLLvalues within model families.\nFigure 3 shows that Defects4J consistently has the lowest\nNLL across all models. This strongly suggests potential data\nleakage. This is particularly evident with codegen-multi ,\nwhich has very low NLL(0.15) for Defects4J . This matches\nour observations in Figure 1 and suggests that codegen-\nmulti has memorized the Defects4J solutions. We observe\ncomparably low NLL (0.38) on the Gemma 2 27B model for\nDefects4J relative to other benchmarks and repositories.\nInterestingly, codegen-multi 6B exhibits low NLL(0.38) on\nSWEBench-Lite compared to other benchmarks and new data,\ndespite being the oldest model in our evaluation, trained on\nolder data, and the fact that SWEBench-Lite was published\nrecently. This is because the projects in the benchmark existed\nprior to benchmark publication, as we also saw in the member-\nship analysis (Table III). Moreover, although SWEBench-Lite\nis a new benchmark, the bug fixes date as early as 2017.\nFor all other models, the NLL values are fairly consistent\nacross non- Defects4J benchmarks. As expected, the new\nrepositories we collected exhibit higher NLLcompared to De-\nfects4J ,BugsCpp ,BugsInPy , and SWEBench-Lite . Evaluation\nbenchmarks are derived from prominent projects and may have\nbeen seen at a pretraining time multiple times, unlike our new\nrepositories, which likely were not. Note, however, a potential\nconfound, which is that our new repositories may be different\nin distribution compared to the models\u2019 training data, which\nmay contribute to higher NLL.\nFigure 4 visualizes how each benchmark compares against\nother benchmarks and repositories for each model family,\ndemonstrating how much more \u2018familiar\u2019 a particular model\nis with a benchmark compared to the others. For example,\ncodegen-multi \u2019sNLLonDefects4J is5.63\u00d7lower than on\nnew Java repositories, and 3.82\u00d7lower than on GitBug-\nJava . This ratio highlights a significant level of familiarity\nwith Defects4J compared to new repositories. Newer models,\nparticularly LLaMa 3.1 , exhibit relatively consistent NLLvalues\nacross all benchmarks. For example, the NLL ratio between\nDefects4J andGitBug-Java forLLaMa 3.1 70B is only 1.27,\nindicating that LLaMa 3.1 perceives Defects4J patches as\nDefects4J\n(v1.5)BugsInPy Bugs\nC++Gitbug\nJavaSWEBench\nLiteNew\nJavaNew\nPythonDefects4J(v1.5)\nBugsInPy\nBugsC++\nGitbugJava\nSWEBenchLite1.00 2.55 3.18 3.83 2.51 5.63 4.86\n0.39 1.00 1.25 1.50 0.98 2.21 1.91\n0.31 0.80 1.00 1.20 0.79 1.77 1.53\n0.26 0.67 0.83 1.00 0.65 1.47 1.27\n0.40 1.02 1.27 1.53 1.00 2.25 1.94NLL Ratios for Codegen 6B Multi\nDefects4J\n(v1.5)BugsInPy Bugs\nC++Gitbug\nJavaSWEBench\nLiteNew\nJavaNew\nPythonDefects4J(v1.5)\nBugsInPy\nBugsC++\nGitbugJava\nSWEBenchLite1.00 1.22 1.45 1.27 1.18 1.95 1.28\n0.82 1.00 1.19 1.04 0.97 1.59 1.04\n0.69 0.84 1.00 0.88 0.82 1.34 0.88\n0.79 0.96 1.14 1.00 0.93 1.54 1.01\n0.84 1.03 1.23 1.07 1.00 1.65 1.08NLL Ratios for LLama 3.1 70B\nDefects4J\n(v1.5)BugsInPy Bugs\nC++Gitbug\nJavaSWEBench\nLiteNew\nJavaNew\nPythonDefects4J(v1.5)\nBugsInPy\nBugsC++\nGitbugJava\nSWEBenchLite1.00 1.53 1.25 1.47 1.51 2.36 1.53\n0.65 1.00 0.82 0.96 0.99 1.54 1.00\n0.80 1.22 1.00 1.18 1.21 1.88 1.22\n0.68 1.04 0.85 1.00 1.02 1.60 1.04\n0.66 1.01 0.83 0.98 1.00 1.56 1.02NLL Ratios for Gemma 2 27B\n101\n100101Fig. 4: Heatmap illustrating the relative NLLratios across datasets for the codegen-multi ,LLaMa 3.1 , and Gemma 2 . Each cell\nrepresents the ratio of the NLLfor the dataset in the column to that of the dataset in the row. For example, the NLLfor new\nJava repos is 5.63\u00d7higher than that for Defects4J . Darker colors correspond to higher ratios.\nonly slightly more predictable than GitBug-Java . This is\nexpected, as the newer LLaMa 3.1 family of models was\ntrained on significantly more data and is thus less prone to data\nmemorization. Specifically, the LLaMa 3.1 family was trained\non30\u00d7more tokens than codegen-multi .\nC. 5-Gram Accuracy\nFigure 5 shows 5-gram accuracy results, a complementary\nassessment of potential model memorization (full tables, in-\ncluding other models are in the appendix). Note that, due to\ndifferences in model vocabularies and tokenization, interpre-\ntation of 5-gram accuracy can differ across models.\nDefects4J consistently exhibits the highest 5-gram accu-\nracy across all model families. Conversely, 5-gram accuracy\nonGitBug-Java is relatively similar to that of new repositories\nfor all models, which aligns with the expectation that most\nrepositories in the GitBug-Java benchmark were not included\nin pretraining data (as detailed in Table I). For example,\nboth codegen-multi and Gemma 2 show significantly higher\n5-gram match differences between Defects4J and new repos-\nitories (i.e., 34percentage points and 14percentage points,\nrespectively). Moreover, codegen-multi achieves 82% 5-gram\naccuracy onDefects4J , strongly suggesting that it has likely\nmemorized much of the benchmark\u2019s solutions.\nAs expected, new repositories generally exhibit lower 5-\ngram accuracy across all models, with averages of 47% and\n48% for Java and Python , respectively. These values happen\nlikely due to the presence of common coding patterns [29].\nConversely, and in line with our NLL findings, codegen-\nmulti shows a notably high 5-gram accuracy onSWEBench-\nLite , even though it is a recently published dataset, as it\nincorporates older data.\nWhen it comes to LLaMa 3.1 family, both LLaMa 3.1 70B\nand 8B exhibit consistent 5-gram accuracy across bench-\nmarks, which could suggest that these models are less prone\nto memorization due to their exposure to substantially larger\ndatasets during pretraining. However, it is important to notice\nthat 5-gram accuracy alone may not conclusively prove an\nabsence of memorization (contrary to high n-gram accuracy\nwhich reliably indicates strong pattern retention). For exam-\nple, Figure 6b shows an example where LLaMa 3.1 70B isprompted to predict a patch from BugsInPy (fastapi :Bug 12).\nOn this file, one of the expected 5-grams is \u201c __init__(self, \"\n, and here LLaMa 3.1 predicts \u201c __init__(\\nself, \" instead.\nThis causes the 5-grams not to match, even though the content\nis the same. Therefore, it is crucial to evaluate models by\nconsidering NLL,5-gram accuracy , and membership as whole,\nas no single metric provides a complete picture of data leakage.\nD. How Do Model Characteristics Influence Risk of Leakage?\nTo gain deeper insights, and using the data collected during\nthe study, we estimate regression models for the average\nvalues of NLL and 5-gram accuracy . The regressions allow\nus to explore relationships that metrics alone cannot reveal.\nUsing regression analysis, we can identify factors such as\ntraining parameters and budget as significant influences on\nthese metrics. Therefore, we use a simple mixed-effects linear\nmodel to predict the average NLLand 5-gram accuracy . As\npredictors, we include the models\u2019 number of parameters\nand number of tokens used during pretraining (i.e., training\nbudget ), which serves as a proxy for the unique token count.\nWe acknowledge that reporting the exact number of unique\ntokens would be a more precise metric, but such data is often\nunavailable. We also account for potential variability caused by\ndifferences in the datasets and tokenizers by including dataset\nandtokenizer type as random effect. Table IV shows results.\nIn a linear regression, the intercept represents the predicted\nvalue of the dependent variable (averages of NLLand 5-gram\naccuracy ) when all predictors are at their reference values.\nTo make the interpretation easier, we centered the predictors\naround codegen-multi values. Therefore, the reference value\nofparameters is 6B and the reference value of training\nbudget is 1T tokens. The regression coefficients represent the\nchange in the dependent variable for a one-unit increase in the\npredictor, holding other predictors constant.\nForNLL, the predicted average is 0.744 when at reference\nlevel, i.e., parameters = 6B and training budget = 1T. For every\n1B increase in number of parameters (above 6B), the predicted\nNLLaverage decreases by 0.002units (while holding training\nbudget constant, at 1T). Similarly, for every 1T tokens increase\nin training budget, predicted NLLdecreases by 0.014units. For\nFig. 5: 5-gram accuracy by model and dataset. Due to space constraints, we selected a sample of the most relevant models.\n5-gram accuracy for other models are consistent with the results displayed.\nTABLE IV: Summary of regressions tests for negative log like-\nlihood ( NLL) and 5-gram accuracy . We report the coefficient\nestimates with their standard errors in parentheses.\nNLL 5-gram\nIntercept 0.744 (0.094) *** 0.465 (0.049) ***\nParameters -0.002 (0.001) * 0.001 (4.0e\u22124) *\nTraining budget -0.014 (0.004) ** 0.006 (2.4e\u22123) *\nNote: *** p-value < 0.001, ** p-value < 0.01, * p-value < 0.05\n5-gram accuracy , the predicted average is 0.465 when predic-\ntors are at reference level. For every 1B increase in number\nof parameters, the predicted 5-gram accuracy increases by\n0.001units. Similarly, a 1-unit increase in the training budget\nleads to an increase of 0.006in5-gram accuracy . All reported\ncoefficients have statistical significance.\nRegression results reveal consistent trends across model\nfamilies. For example, both LLaMa 3.1 8B and 70B were\ntrained on the same data with a training budget of 15T tokens.\nHowever, the 70B model is approximately ten times larger\nthan the 8B model, leading to an overall increase in 5-gram\naccuracy across all benchmarks (as shown in Figure 5) and\ndecrease in NLL. Similarly, within the Gemma 2 family, the 2B\nmodel was trained on 3T tokens, while the 27B was trained\non 12T. Here, we observe the same trend: average 5-gram\naccuracy increases in the 27B model and NLLdecreases.\nRegression results also imply that models with more pa-\nrameters tend to exhibit higher n-gram accuracy and, con-\nsequently, memorize more. For example, Figure 6b shows\nthat LLaMa 3.1 70B, which has the same training budget as\nLLaMa 3.1 8B, accurately predicts strings, class names, and\nif-statements in contexts where these predictions might not be\nimmediately apparent, which may indicate memorization.\nIV. D ISCUSSION\nOur evaluation provides compelling evidence that data leak-\nage is an especially significant issue for Defects4J (V1.5).\nThis is evident from the lower NLLvalues and higher 5-gram\naccuracy . In particular, codegen-multi achieves 82% 5-gram\naccuracy onDefects4J , while both CodeLlama 7B andGemma-\n2 27B attain 64%. Moreover, given that Defects4J is incorpo-rated into the widely-used pretraining dataset ( TheStack ), with\n80% membership, eliminating this leakage in future models is\nlikely nearly impossible. We also observe similarly low NLL\nvalues in SWEBench-Lite forcodegen-multi .\nNewer benchmarks BugsInPy and BugsCpp exhibit lower\nleakage risk in almost all models. A smaller percentage of\ntheir repositories are indexed in the latest version of TheS-\ntack . While BugsInPy andBugsCpp exhibit slightly lower NLL\ncompared to new repositories, their 5-gram accuracy andNLL\nvalues are not significantly different from newer, more-likely-\nunseen benchmarks like GitBug-Java . This suggests that, for\nnow, researchers can use these benchmarks with a relatively\nlow risk of data leakage. However, as pretraining datasets\ncontinue to evolve, we recommend that researchers regularly\nassess TheStack membership, 5-gram accuracy andNLLval-\nues of these benchmarks, especially compared to more recent\ndata, to monitor and mitigate potential data contamination.\nWe also observe that the LLaMa 3.1 family seems to exhibit\nlower memorization of benchmark solutions. Nonetheless, we\nstill observed cases where LLaMa 3.1 70B outputs solution\nfiles despite little context (e.g., Figure 6).\nWe suggest researchers consider supplementing their eval-\nuations with more recent benchmarks such as GitBug-Java .\nBenchmarks like GitBug-Java , which focus on recent bugs\nand patches, are less likely to have been included in pretrain-\ning datasets compared to established benchmarks. Leveraging\nthese newer benchmarks can provide more reliable evaluations\nfor assessing model\u2019s capabilities.\nV. L IMITATIONS AND THREATS\nData Collection: Despite efforts to filter out older GitHub\nrepositories when collecting new data, we anecdotally ob-\nserved instances where files appeared to be adaptations of\nexisting files (e.g., from 2018) Our filtering process may not\nhave perfectly excluded legacy code. We moreover cannot\nguarantee that the new repositories are identically distributed\ncompared to those in the benchmarks. To mitigate this issue,\ngiven that the repositories in the benchmarks tend to be highly\nrecognizable projects, we applied a >100-star filter to argue\nthat the selected projects are of comparable quality.\n1 * * omitted for brevity * *\n2 if scheme.lower() != \"bearer\":\n3 if self.auto_error:\n4 raise HTTPException(\n5 status_code=HTTP_403_FORBIDDEN,\n6 detail=\"Invalid authentication\n7 credentials scheme\")\n8 else:\n9 return None\n10 return HTTPAuthorizationCredentials(scheme=scheme,\n11credentials=credentials) self.model.parse(credentials)\n12\n13class HTTP Digest(HTTPBase) BearerModel(BaseModel)::\n14 def __init__(self, *, bearerFormat: str = None,\n15 * * omitted for brevity * *\nPrompt input to LLaMa 3.1 8B.\nExtra code generated. Ground truth not generated.Matching Ground Truth & Predicted by LLaMa 3.1 8B.\n(a) Example of generation by LLaMa 3.1 8B .1 * * omitted for brevity * *\n2 if scheme.lower() != \"bearer\":\n3 if self.auto_error:\n4 raise HTTPException(\n5 status_code=HTTP_403_FORBIDDEN, \\n\n6 detail=\"Invalid authentication credentials\" \\n\n7 )\n8 else:\n9 return None\n10 return HTTPAuthorizationCredentials(scheme=scheme,\n11 credentials=credentials)\n12\n13class HTTPDigest(HTTPBase):\n14 def __init__( \\nself, \\n*, \\nqop: str = None \\n,\n15 * * omitted for brevity * *\nPrompt input to LLaMa 3.1 70B .\nExtra code generated. Ground truth not generated.Matching Ground Truth & Predicted by LLaMa 3.1 70B.\n(b) Example of generation by LLaMa 3.1 70B.\nFig. 6: Example patch of a bug from BugsInPy ( fastapi :Bug 12). We prompt each model with the 30 lines prior to the\npatch. We highlight the extra code generated not in the ground truth and ground truth code not generated by the model. The\nremaining lines of the examples are omitted for brevity.patch\npatch\nTrain + Test Splits : For benchmarks like Defects4J (V1.5)\nandBugsInPy , the patch files and buggy files are very similar\n(typically a patch involves only changing a small number\nof lines). LLMs may have only seen the train split of these\nbenchmarks at pretraining time. This would result in high 5-\ngram accuracy and low NLLon patch files, even if only buggy\nfiles were leaked. Nonetheless, we observed multiple cases\nwhere models output patched files verbatim. We mitigate this\nby looking at trends in NLL and 5-gram accuracy rather than\nabsolute numbers in our analysis.\nForgetting : The findings presented in this paper primarily\naddress leakage in the context of base models. Empirical\nresults show that models can \u201cforget\u201d portions of their pre-\ntraining data during fine-tuning [30]. That said, while full-\nscale model fine-tuning may reduce leakage risks, more recent\nfine-tuning strategies\u2014such as the addition of adapter layers\u2014\noften \u201cfreeze\u201d pretrained weights. This practice can inadver-\ntently increase the likelihood of data leakage.\n\n[Text truncated due to context length limits]","filename":"Are_large_language_models_memorizing_bug_benchmarks_.pdf","responses":"[1] Machine Learning For Code: Study and application of machine learning methods to software code tasks, including code generation, bug detection, repair, and related benchmarking."}
{"id":"pdf_4","text":"The Sustainability Face of Automated Program Repair Tools\nMATIAS MARTINEZ, Universitat Polit\u00e8cnica de Catalunya, Spain\nSILVERIO MART\u00cdNEZ-FERN\u00c1NDEZ, Universitat Polit\u00e8cnica de Catalunya, Spain\nXAVIER FRANCH, Universitat Polit\u00e8cnica de Catalunya, Spain\nAutomated program repair (APR) aims to automatize the process of repairing software bugs in order to reduce the cost of\nmaintaining software programs. While APR accuracy has significantly improved in recent years, its energy impact remains\nunstudied. The field of green software research aims to measure the energy consumption required to develop, maintain, and\nuse software products. Our main goal is to define the foundation for measuring the energy consumption of the APR activity.\nWe state that an environmentally sustainable (or green) APR tool achieves a good balance between the ability to correctly\nrepair bugs and the amount of energy consumed during such process. We measure the energy consumption of ten traditional\nAPR tools for Java and eleven fine-tuned Large-Language Models (LLM) trying to repair real bugs from Defects4J. The results\nof this study show the existing trade-off between energy consumption and repairability. In particular, APR tools such as TBar\nand RepairLlama repair more bugs than other approaches at the expense of a higher energy consumption. Other tools, such as\nSimFix and the LLM CodeT5-Large, provide a good trade-off between energy consumption and repairability. We also present\nguidelines consisting of a set of recommendations for developing greener APR.\nCCS Concepts: \u2022 Hardware !Power and energy ; \u2022Software and its engineering !Softwaremaintenance tools .\nAdditional Key Words and Phrases: Automated Program Repair, Software Sustainability, Energy Consumption of Software\nTools, Green Computing\n1 INTRODUCTION\nIn the last decade, following current societal needs, software sustainability has emerged as research field [ 1\u20133]. It\nconsists of different dimensions, including environmental sustainability, human sustainability, and economic sus-\ntainability. In this paper, we focus on environmental sustainability, defined as \u201chow software product development,\nmaintenance, and use affect energy consumption and the consumption of other natural resources. [\u2026] This dimension\nis also known as Green Software\u201d [4].\nThe study of environmental sustainability is of paramount importance in the realm of automated software\nengineering, i.e. the application of computation to software engineering activities with the objective of partially\nor fully automating these activities, thereby significantly increasing both quality and productivity [ 5]. The\nreason is that such computations demand high computational resources, especially nowadays: the advent of Big\nData, Internet of Things, Machine Learning, Generative AI and other contemporary methods and technologies,\nsupported by high-performance cloud-based infrastructure, allows researchers to design and implement complex,\nsophisticated automated solutions that were not feasible in the past, which ultimately implies high levels of\nenergy consumption.\nAuthors\u2019 Contact Information: Matias Martinez, Universitat Polit\u00e8cnica de Catalunya, Barcelona, Spain, matias.martinez@upc.edu; Silverio\nMart\u00ednez-Fern\u00e1ndez,UniversitatPolit\u00e8cnicadeCatalunya,Barcelona,Spain,silverio.martinez@upc.edu;XavierFranch,UniversitatPolit\u00e8cnica\nde Catalunya, Barcelona, Spain, xavier.franch@upc.edu.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\nCopyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy\notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and\/or a fee. Request permissions from\npermissions@acm.org.\n\u00a9 2025 Copyright held by the owner\/author(s).\nACM 1557-7392\/2025\/6-ART\nhttps:\/\/doi.org\/10.1145\/3744900\nACM Trans. Softw. Eng. Methodol.\n \n\n2 \u2022 M. Martinez et al.\nOne of these automation-prone activities is bug fixing through automated program repair (APR) tools. APR is\nbecoming popular due to the high economic cost that fixing bugs entails [ 6,7]. APR tools take as input a buggy\nprogram and deliver one or more patches that repair the bugs, when they are found. In recent years, numerous\nAPR tools have been capable of automatically repairing real bugs (e.g., [ 8\u201310]), mainly from open-source libraries\n[11\u201313]. Furthermore, APR has started to be adopted in industry [14\u201316].\nTo evaluate APR tools, researchers usually report two metrics [ 17]:a)Total number of bugs that are repaired\nwith aplausible patch (that is, a patch valid w.r.t. the correctness oracle), and b)Total number of bugs repaired\nwith at least one correctpatch.\nHowever, to our knowledge, thereisnopreviousworkintheAPRareathatfocusesonthestudyof\ntheenergyconsumptionofAPRtools . The APR research community has been oriented towards accuracy,\nwith a strong focus on obtaining state-of-the-art results, as happened in the AI community [ 18]. According to\ncurrent societal demands and the computational demanding nature of state-of-the-art techniques, we argue\nthatAPR researchers should also consider energy consumption as a main driver when designing and\nimplementing APR tools , carefully analyzing whether a minor gain in accuracy justifies high levels of energy\nconsumption. The absence of methodologies for measuring energy consumption and benchmarkings in APR\nmakes it challenging to evaluate how innovations in APR impact the overall energy consumption of the repair\nprocess. This paper aims at addressing this gap.\nThe inclusion of energy as a driver of APR research is important for several reasons. First, APR tools use\nexpensive correctness oracles, executed each time a candidate patch is synthesized. Test suites are examples of\nthem. As APR tools synthesize a considerable amount of patches before finding the correct one [ 19], the amount\nof energy up to that point could be considered wasteful. Second, even with the impressive progress of the field,\nstate-of-the-art APRs do not achieve high accuracy and are complementary (as shown, for example, in Fig. 2\nfrom [20]). This means that, in order to achieve high accuracy, a practitioner aiming to deploy APR would need\nto consider not just one tool, but several, increasing the energy required to repair a bug. Third, APR tools could\nbe triggered regularly; for example, a setup that connects APRs with CI\/CD (e.g., via a bot [ 21]) would require to\ninvoke APRs tools for each failed build caused by test cases with failure or error. For example, the study by Moritz\net al. [22] on Travis CI builds reports a median of 10.3% for builds with failed tests in Java projects. Triggering\nAPRs for each of these failed builds would require a large amount of repair attempt execution. Unfortunately, the\nenergy consumed by these repair attempts is unknown. To achieve green software development, we believe that\nit is essential to understand the energy consumption of APR activity. This work is a first step in that direction.\nThe ultimate goal of our research, expressed following GQM [ 23] is to: Analyze APR tools with the purpose of\nmeasurement and analysis with respect to energy consumption from the point of view of software developers in the\ncontext of bug repair .\nTo achieve this goal, we first measure the energy consumed by APR tools to find plausible and correct patches.\nWe focus on two families of APRs: a) Traditional tools (i.e., search-, constraint- and template-based), and b) Large-\nLanguage Model (LLM)-based tools. We also focus on the correctness of generated patches, in order to quantify\nthe energy \u2018wasted\u2019 on generating overfitting (incorrect) patches.\nThen, we study the relationship between energy consumed and twometrics studied in previous work as proxies\nofrepair efficiency , which is one aspect of the APR performance [ 19]:a)Repair time, studied by [ 24] in the\ncontext of APR, and b)Number of candidate patches (NCP), defined by [ 25], and studied by Liu et al. [ 19] in the\ncontext of APR. With this study, we want to know whether these two metrics can be used as a proxy for energy\nconsumption. In addition, we study whether repairing hard and important bugs requires more energy. For this,\nwe use the data provided by Motwani et al. [26].\nWe evaluate on the Defects4J benchmark [ 11] the energy consumption of ten publicly available traditional\nrepair tools (including TBar [ 27], Prapr [ 24] and SimFix [ 28]), ten fine-tuned large language models (including\nInCoder [ 29] and CodeGen [ 30]) used by Jiang et al. [ 31], and RepairLlama [ 32], a repair approach based on\nACM Trans. Softw. Eng. Methodol.\n \n\nThe Sustainability Face of Automated Program Repair Tools \u2022 3\nCodeLlama [ 33], which introduces LoRA [ 34], a state-of-the-art fine-tuning technique named PEFT (parameter-\nefficient fine-tuning), for program repair.\nThe results of this experiment show a moderate positive correlation between energy consumed and number of\nbugs correctly repaired. The traditional approach TBar, based on a collection of existing templates, is the most\nenergy-consuming approach to repair Defects4J bugs (118 kWh). In total, 28 bugs were correctly fixed by TBar,\nmore than any other traditional APR tool.\nAmong LLM-based tools, the largest models (those with \u00196-7 billion parameters) consume more energy due to\ntheir hardware requirements (they require several GPUs). In particular, RepairLlama consumes 1,263 Wh to repair\n61 bugs correctly. Tools based on LLM with similar size (6 billons parameters) such as InCoder and CodeGen,\nconsumes much less energy than RepairLLama (both \u0019820) but also repair less bugs, 37. Smaller models such as\nCodeT5Large and PLBARTLarge show a good balance between energy (608 Wh and 567 Wh, respectively) and\nrepairability (31 and 30 repaired bugs, respectively).\nWe also focus on the impact of overfitting patches on the overall energy consumed by the repair tools. In\ntraditional tools, approximately 6% to 40% of the energy is consumed by repair attempts that yield only overfitting\npatches. In contrast, for LLM-based tools, the energy spent on repair attempts that produce solely overfitting\npatches ranges from approximately 5% to 17%.\nBased on the insights from our research, we propose a set of guidelines with recommendations for developing\ngreener APR. These recommendations cover various types of APR, including LLM-based approaches, and address\nmultiple stages of the repair process, such as patch generation, model training, and patch validation.\nThe paper continues as follows. Section 2 presents a background on APR and energy consumption. Section 3\npresents the methodology to answer the research questions. Section 4 responds to the RQs. Section 5 presents\na discussion on this study. Section 6 presents guidelines with recommendations for greener APR. Section 7\npresents the threats to validity. Section 8 discusses the limitations of this work. Section 9 concludes the paper\nand highlights directions for future research.\nThis is an extended version of a two-page article that introduces the problem of APR and energy consump-\ntion [35].\nData availability statement. All data generated in this experiment are available as an appendix at https:\n\/\/github.com\/UPCBarcelonaTech\/green_apr.\n2 BACKGROUND\n2.1 Automated program repair\n2.1.1 Approaches and tools. During recent years, several APR approaches and tools have been proposed with\nthe goal of automatically fixing bugs in software applications [ 36,37]. As a recent controlled experiment with\n40 developers shows, proposing correct patches to developers helps successfully repair defects and speeds up\ndebugging time [38].\nAfamilyofAPRapproachesis test-suite based program repair approaches[ 8],whichusetestcasesasaproxyfor\nthe program specification. This family is further subdivided into several types. On the one hand, the traditional1\nfamily includes: (a) Search-based APRs, such as GenProg [ 8], PAR [40], SPR [9] and CapGen [ 41]. These APRs\nnavigate the search space composed of candidate patches to find the solution, i.e. the correct patch. Some of them\nuse information previously extracted from other programs (e.g., [ 42,43]) to speed up the search [ 44] or even\nin trained models (e.g., [ 45]) to filter out incorrect candidates; (b) Semantic-based APRs, such as SemFix [ 46],\nNopol [47] and Angelix [10], which use symbolic execution and constraint solving.\nOn the other hand, there exists the Neural network-based APR family, which includes, for instance, Se-\nquenceR [ 48], Coconut [ 49], Recoder [ 50], SelfAPR [ 51], KNOD [ 52] and RewardRepair [ 20]. These APRs rely on\n1We adopt the term traditional for these approaches, as done by two recent surveys: [39] and [37].\nACM Trans. Softw. Eng. Methodol.\n \n\n4 \u2022 M. Martinez et al.\nmachine learning to generate patches (e.g., using sequence-to-sequence learning). More recently, researchershave\nproposed LLM-based approaches that, when pre-trained in source code, show good results in fixing bugs [ 31,53].\nAlso, conversational chatbots, such as ChatGPT, are capable of proposing a fixed version of small buggy pro-\ngrams [54].\n2.1.2 Adoption of APR. APR has also gained traction in the industry. Practitioners have conducted experiments\nand implemented APR tools in real-world industrial contexts. Some examples are Meta\u2019s tool, Getafix [ 15],\nand Bloomberg\u2019s tools, Fixie [ 14] and B-Assist [ 55]. For instance, an experiment with Bloomberg\u2019s software\nengineers demonstrated that suggestions provided by B-Assist were accepted 74.56% of the time, either directly\nor after modification, through the GitHub user interface [ 55]. A recent survey conducted with 337 software\npractitioners [ 56] reveals that the majority are either aware of or actively use APR tools. Among the tools\nmentionedbyrespondents,themostfrequentlycitedwasAstor[ 57],whichincludesapproachessuchasjGenProg\nand jMutRepair, both studied in our paper. However, the survey also highlighted several disadvantages of APR\ntools. The most significant concern, reported by 196 respondents, was that these APR tools may overlook rare or\nuncommon behavior. Additionally, 177 respondents noted that the maintenance cost of APR tools can be high.\n2.1.3 Cost and Efficiency of APR. The cost of APR has been studied previously in a few previous works. For\nexample, Le Goues et al. [ 58] studied the monetary cost of repairing C bugs using GenProg [ 8]. Other works have\nstudied the efficiency of program repair. For example, Liu et al. [ 19] use the Number of Candidate Patches (NCP),\nmetric originally used by to measure the efficiency of repair approaches [ 25]. Nevertheless, beyond the progress\nmade in the field [ 37,39], to our knowledge, there are no previous works focusing on the energy consumption\nof APR. Moreover, even there are different studies on the energy consumption of LLMs, in both training and\ninference phases, to our knowledge, there is no previous work on the energy profile of LLMs used on program\nrepair (for example, energy of patch inference using LLMs).\n2.1.4 Patch Overfitting. Preliminary experiments with APR, such as Smith et al. [ 59], Martinez et al. [ 60], Qi et\nal. [61] and Le et al. [ 62] have detected one of the initial limitation of APR: a considerable portion of generated\npatchesareincorrect. Thesepatchesareknownas Overfitting patches.Just Correctpatchesarethosethat correctly\nrepair a bug. Overfitting patches are not accepted by developers.\nAPR tools rely on test cases from project under repair to identify bugs and validate fixes. Unfortunately,\na candidate patch passing all test cases is Plausible , but that does not guarantee correctness: it could also be\nIncorrect . This can happen in scenarios not covered by the tests. For example, an overfitting patch may produce\nan unexpected output for an input not included in the test cases, that is, it overfitsthe available tests and breaks\nuntested but desired functionality [59].\nTo show the real effectiveness of an APR, researchers introducing new repair approaches have reported the\namount of both plausible and correct generated patches. To illustrate the magnitude of the problem, the study\nby Liu et al. [ 19] shows that TBar, one of the most effective APR tools, produced 24 correct patches out of 72\nplausible ones: 66.6% of the generated patches are incorrect.\nInitial analyses of overfitting were performed by humans by annotating patches as either overfitting or correct\n(e.g [19,60]). Additionally, another type of analysis generates additional test cases not included in the buggy\nprogram, thus not used for validating candidate patches (e.g. [ 59,63]). Then, different patch overfitting detection\nsystems have emerged, such as PatchSim [ 64] and DiffTGen [ 65], xTestCluster [ 66], all of these based on test\ncase generation. The main idea of these approaches is to generate new inputs that allow to differentiate the\nbehaviors of overfitting patches from correct ones. Wang et al. [ 67] conducted a preliminary empirical study on\nthe effectiveness of automated patch overfitting detection, while Petke et al. [ 68] studied the magnitude of the\noverfitting problem.\nACM Trans. Softw. Eng. Methodol.\n \n\nThe Sustainability Face of Automated Program Repair Tools \u2022 5\nThen, researchers have trained ML models using patches labeled from previews work. These ML-based ap-\nproaches, given a plausible patch as input, produce a binary decision (correct or overfitting) and\/or return an\noverfitting score. Some of these ML-based approaches are ODS [ 69], Shibboleth [ 70], CACHE [ 71], and Tian et al.\n[72].\nAt the same time, repair techniques have incorporated mechanisms to avoid overfitting patches, such as\nProphet [ 45], ObjSim [ 73] and CapGen [ 41] which incorporate patch ranking module (in the case of Prophet\ntrained from labeled patches) to rank correct patches at high positions before incorrect plausible ones.\nIn this paper, we study the impact of overfitting patches on the energy consumed by an APR tool.\n2.2 Energy consumption of software\nEnergy consumption has been investigated in the scope of different types of systems, such as mobile apps [ 74],\nsoftware product lines [ 75] and more recently, ML-based systems [ 76]. As a matter of example, Strubell et al.\nquantified the approximate environmental costs of training a variety of successful neural network models [ 77].\nThey estimated that a human life per average implies 11,023 CO2emissions in one whole year whereas training\none big Transformer model with neural architecture search emits 626,155 CO2. In other work, Schwartz et al.\nreported that the cost of training ML models (in terms of the amount of computing resources used) as required by\nstate-of-the-art techniques has increased by a factor of 300.000x in only 6 years, doubling every few months [ 18].\nThis increase is primarily due to modeling and algorithmic choices, over hardware considerations [ 18], which has\nmotivated several researchers to focus on software when searching for energy consumption reduction [76, 78].\nHort et al. presented a literature study on the energy use of language models for source code [ 79]. Among\ndifferent software engineering tasks, they studied papers on program repair and report that just four works on\nthat field ([ 80], [48], [81] and [82]) provide details about a)hardware use for training the model, and b)time and\nenergy spend for training. More recently, Shi et al. presented a roadmap on LLMs for software engineering tasks\n[83]. However, none of them focus on the energy consumption of the repair process (i.e., the inference and the\nvalidation of patches), as we do in this paper.\nMoving towards the ecological transition, software development cannot only target accuracy, but energy\nefficiency as well. In this context, both traditional and state-of-the-art neural network-based (including LLM-\nbased)APRtoolsshallconsidertheoptimizationsoftheirparametersandtheamountsofenergyandtheresources\nused. However, this is not easy because software developers lack critical information and knowledge about\nsoftware energy consumption [84, 85].\n3 METHODOLOGY\nIn this paper, the type of investigation we choose to carry out the empirical study is Technology-oriented Experi-\nment[86]. This presents the method applied to carry out the experiment that allows us to answer the research\nquestions, presented in Section 3.1.1. Figure 1 shows an overview of the experiment process. Each gray box\ncorresponds to an activity explained in the remaining subsections.\n3.1 Experiment Scope\n3.1.1 Research Questions. The research goal defined in Section 1 breaks down in the following research questions\n(RQs) about energy consumption of ARP tools:\n(1)What is the energy consumption of APR tools on Defects4J? .\nThis RQ aims to obtain the energy profile of APR tools in the task of repairing real bugs.\n(2)Which APR tool achieves the best energy efficiency in correctly fixing bugs\nThis RQ aims to investigate the trade-off between energy consumption and the repairability of each tool.\nACM Trans. Softw. Eng. Methodol.\n \n\n6 \u2022 M. Martinez et al.\nBug\nAPR \ntoolRepair \nAttempt \nWattmeter \nPatch \nAssessment \n(Section 3.5) Energy \nComputation \n(Section 3.6) Experiment Execution \n(Section 3.3) Tools and Bugs \nSelection \n(Section 3.2) Analysis and \nInterpretation \n(Section 3.7) \nPower + Energy \n(Wh) \nData Validity \nEvaluation \n(Section 3.4) + Energy \n(Wh) \nDefects4J \nTBar \nPrAPR \nRepairLlama Incoder6B \u2026Patches \nExperiment \nScoping \n(Section 3.1) RQs Answers to \nthe RQ \n(Section 4) \nFig. 1. Experiment process for measuring energy consumption of APR tools.\n(3)What is the impact of patch overfitting on energy consumption of APR? This RQ aims to measure the amount\nof energy that is destined for a successful repair and the amount that is wasted in unsuccessful repairs\nwith overfitting patches.\n(4)To what extent does the energy consumed to find a patch correlate with: a) the repair time?, b) difficulty\nto fix a bug, c) the number of patch candidates (NPC) and d) project under repair?\nThis RQ checks whether there is a correlation between energy and other metrics that represent the repair\ntime, efficiency, difficulty to repair and project under repair.\n3.1.2 Variables. Our experiment has two independent variables: 1)Bug, and 2)APR tool; and the following\ndependent variables: a)number of bugs with plausible patches, b)number of bugs with correct patches, c)time\nand energy spent to find the first plausible patch, d)time and energy spent to find the first correct patch, e)energy\nspent during a repair attempt.\n3.2 APR Tools and Bugs Selection\nIn this section we define the criteria to select the APR tools and bugs of our experiment. We decide to study\nthe energy consumption of tools capable of repairing Java bugs. For this reason, we choose the benchmark\nnamed Defects4J [ 11], which is a benchmark of Java written buggy programs, commonly used to evaluate the\nrepairability of Java bugs (e.g., [ 19,87]. We focus on the version 1.2 a benchmark composed of 395 Java bugs. Each\nof these bugs is exposed by at least one test case. A repair attempt is the execution of a repair tool (e.g., TBar) on\na particular bug from Defect4J (e.g., Math-70) under configuration given to the tool as input. With respect to the\nAPR tools, we focus on two families of tools that have produced state-of-the art results over the last 10 years:\n1) traditional repair tools (inc. search-based, constraint-based and template-based), and 2) LLM based repair tools .\nWe now describe the selection process for the tools of each family.\nTraditional repair tools. We apply the same criteria as Durieux et al. [ 87] and Liu et al. [ 19], which include:\n1)availability of the tool, 2)availability of the source code of the tool (e.g., on a code platform such as Github\nor Gitlab), 3)the tool receives as input the buggy program to be repaired, but does not receive any additional\ninformation about the bug.\nFollowing the above criteria, we select in this study the following traditional tools: Avatar [ 88], FixMiner [ 43],\nNopol [47], SimFix [ 28], TBar [ 27], Dynamoth [ 89], jGenProg [ 57] (implementation of GenProg [ 8] for Java bugs),\njMutRepair [ 60], and kPAR [ 40] (implementation by [ 90]). As suggested by their authors, SimFix [ 28] is used\ninstead of the ACS tool [ 91]. We also add PRarp [ 24], which was not considered in [ 19,87]. We discard repair tools\nACM Trans. Softw. Eng. Methodol.\n \n\nThe Sustainability Face of Automated Program Repair Tools \u2022 7\nimplemented in the ARJA framework (which include ARJA, KALI-A, GenProg-A and RSRepair, among others)\nbecause, after inspecting their patches from [ 87], we found a large amount of non-plausible ones. Furthermore,\ngiven the limited computational resources we have to execute our large-scale experiment (see Section 3.3.3), we\ndiscard Java implementations of the Kali approach [ 61] and Cardumen [ 92] because, as observed by [ 19,87], they\nhave a lower ability to find correct patches.\nLarge language model (LLM)-based repair tools. A recent new family of APR approaches is based on neural\nnetworks, whose ability to repair Java bugs has been shown in a number of studies (e.g., [ 20,48\u201352,93,94]). The\npioneer approaches of this family trained the networks from scratch, using a large dataset of real bug fixing (e.g.,\n[48]). More recently, researchers proposed new approaches based on pre-trained LLMs (e.g., [ 31,53]) or chatbots\n(e.g., [54, 95]).\nIn this study, we represent the family of neural network-based approaches with the tools proposed by two\nworks. First, we choose the work from Jiang et al. [31] for the following reasons:\n\u000fProvides usable tools that allow to reproduce their experiment.\n\u000fUses ten open-source code LLMs from four families of large models: InCoder from Meta [ 29], CodeGen\nand CodeT5 [96] from SalesForce [30], and PLBART [97].\n\u000fFine-tunes these 10 models using a dataset of 143,666 bug fixes, provided by Zhu et al. [ 50]. These\nfine-tuned models are also publicly available.\n\u000fThefine-tunedmodelsachievestate-of-the-artresults,improvingtheperformanceofotherneuralnetwork-\nbased approaches, including RewardRepair [20], CURE [94], RECODER [50] and KNOD [52].\nThe ten LLMs used by Jiang et al. are: 1)CodeGen-350m (350 millions parameters), 2)CodeGen-2B (2 billions),\n3)CodeGen-6B (6 billions), 4)CodeT5-small (60 millions), 5)CodeT5-base (220 millions), 6)CodeT5-large (770\nmillions), 7)InCoder-1B (6.7 billions), 8)InCoder-6B (1.3 billions), 9)PLBART-Base (140 millions), 10)PLBART-\nLarge (400 millions).\nSecondly, we use RepairLlama from Silva et al. [32] for the following reasons:\n\u000fRepairLlama is built on top of CodeLlama [ 33] a state of the art LLM for code related task. CodeLlama is\nnot included in the work from Jiang et al. as it was released after their work.\n\u000fApplies the state-of-the-art fine-tuning technique named PEFT (parameter-efficient fine-tuning) for\nprogram repair. Instead of fine-tuning the entire network as done by Jiang et al., PEFT means that Silva et\nal. just fine-tune a small portion of the network, the adapters.\nIn this work, we use the RepairLlama version IR4 x OR2 which is the one that achieves the highest number of\ncorrect patches according to [32], and is built on top CodeLlama7B (7 billion parameters).\n3.3 Experiment Execution\n3.3.1 Tools preparation. Before launching the experiment, we need to modify some of the tools. By default,\npublicly available implementations of TBar, Avatar, FixMiner, and kPar, stop when the first plausible patch is\nfoundoraftertryingafixednumberofcandidatepatches(e.g.,10.000forTBar).Asweareinterestedincomputing\nthe energy to repair correctly a bug, and the first patch found can be plausible but incorrect, we modified the\nsource code of these tools with the goal of continuing the search after the first patch found, until the execution\nreaches a timeout. For the other traditional tools (e.g., Nopol, jGenProg), we indicate via the provided input\narguments to continue the search.\nWe also modify all traditional and LLM-based tools considered in this study to register the timestamp corre-\nsponding to the instant a plausible patch is found. This moment corresponds to the end of the validation of a\npatch. This timestamp allows us to compute the energy used from the beginning of the repair attempt to the\nACM Trans. Softw. Eng. Methodol.\n \n\n8 \u2022 M. Martinez et al.\nmoment the patch is found and validated. We also modify the code to register the number of candidate patches\nsynthesized (e.g., those that do not compile) in order to compute the metric NPC [25].2\n3.3.2 Experiment setup. In order to execute LLM-based tools, we use the scripts provided by Jiang et al. and from\nSilva et al. to repair Defect4J bugs using the fine-tuned version of these models, also provided by the authors3. We\ndo not change any parameter or hyperparameter. By default, their script queries the LLMs models to generate ten\ncandidate patches per buggy line. For each of these patches, the script compiles it, applies it to the buggy project,\nthen executes the initially failing test cases, and finally executes the test suite provided by the buggy program.\nWe modify their scripts to register the timestamps corresponding to the start and end of each of these phases.\nExperiments on traditional APRs (e.g., [ 19]) usually have different setups than experiments on LLM-based\nAPRs (including Jiang et al. [ 31] and Silva et al.[ 32]). Two of the main differences are the following. First, the\nconsidered LLM-based tools do not execute the fault localization phase; they give as input to the model only\nthe buggy location. That is known as perfect fault localization [19]. In the case of Jiang et al. the input is the\nbuggy line (i.e., the line where the human-written patch is located) and for Silva et al. is the buggy function. In\ncontrast, the traditional tools studied in this paper do not require the buggy line(s) as input. Consequently, they\napply a fault localization phase in order to discover potential locations on the code (aka suspicious code) where\ncandidate patches could be applied. This setup is known as Normal Fault Localization [19] Second, Jiang et al.\nfocus on a subset of bugs, namely single-hunk bugs, because, as they mention, the best deep learning-based APR\ntechniques are all specially designed for Java single-hunk [ 31]. Note that RepairLlama from Silva et al. is able\nto repair multi-hunk bugs, but these hunks must be located in the same function. For these reasons, to avoid\nunfair comparison, we analyze separately the energy consumption of traditional approaches and LLM-based and\ntraditional approaches, not aiming to compare the results between these two families of approaches.\n3.3.3 Executionplatform. Weperformrepairattemptsonahigh-performancecomputing(HPC)clustercomposed\nof two families of identical nodes: 1)nodes with CPU (Intel Xeon Gold 5220, 96 GB RAM); 2)nodes with CPU and\nGPU (Intel Xeon E5-2698 v4, 512GB RAM, 8 GPUs Tesla V100-SXM2-32GB). Given the execution scripts from\n[31], GPU-equipped nodes are required to run the selected LLM models.\nEach repair attempt is executed on a dedicated node. No other user has access to the nodes assigned to this\nstudy, and we do not execute any other task in parallel. Before the execution of a repair attempt, the cluster\ndeploys an image with the default standard environment (OS + libraries). This image is the same for every repair\nattempt. It is important to note that the environment does not execute any automated update after the image\ndeployment (e.g. library update from the internet). All of this ensures that all repair attempts are executed under\nthe same conditions. Each node is individually monitored by an OmegaWatt vendor wattmeter4, which provides\nthe electrical power consumed each 20 milliseconds with a precision of 0.1 watt. An experiment based on energy\nmeasurement should be performed several times in order to remove energy measurements that are potentially\noutliers [ 98]. For this reason, we execute each repair attempt five times and then compute the median of the\nmetrics obtained on each repair attempt.\nIn a repair attempt, before and after a repair tool execution, we execute the command sleepwith 30 second as\nparameter, in order to capture the power of the machine in idle state. That helps us to detect eventual anomalies,\nas we explain later.\nFor the execution of traditional APR tools, we set a timeout of 5 hours for each repair attempt, which is larger\nthan those used in previous Java repair experiments [ 60,87]. The repair attempts are executed on the nodes\nhaving just CPUs, as GPU is not required by traditional APR tools.\n2We need to compute NPC ourselves because in Liu et al. [19], the work that uses NCP as a proxy of efficiency, the NPC per repair attempt\n(e.g., tool-bug pair) is not shown, they only report a summary per tool.\n3Replicability packages from [31]: https:\/\/github.com\/lin-tan\/clm and from [32]: https:\/\/github.com\/ASSERT-KTH\/repairllama\n4https:\/\/mv.omegawatt.\n\n[Text truncated due to context length limits]","filename":"The_Sustainability_Face_of_Automated_Program_Repair_Tools.pdf","responses":"[1] Machine Learning For Code: Application of machine learning techniques to software code, including ML-based program repair and code-centric AI tooling."}
{"id":"pdf_5","text":"2366 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 51, NO. 8, AUGUST 2025\nRepairLLaMA: Ef\ufb01cient Representations and\nFine-Tuned Adapters for Program Repair\nAndr\u00e9 Silva ,S e nF a n g , and Martin Monperrus\nAbstract \u2014Automated Program Repair (APR) has evolved sig-\nni\ufb01cantly with the advent of Large Language Models (LLMs).\nFine-tuning LLMs for program repair is a recent avenue ofresearch, with many dimensions which have not been explored.Existing work mostly \ufb01ne-tune LLMs with naive code represen-tations and does not scale to frontier models. To address thisproblem, we propose RepairLLaMA, a novel program repairapproach that 1) identi\ufb01es optimal code representations forAPR with \ufb01ne-tuned models, and 2 ) pioneers state-of-the-art\nparameter-ef\ufb01cient \ufb01ne-tuni ng technique (PEFT) for program\nrepair. This results in RepairLLaMA producing a highly ef-fective \u2018program repair adapter\u2019 for \ufb01xing bugs with AI. Ourexperiments demonstrate the validity of both concepts. First,\ufb01ne-tuning adapters with program repair speci\ufb01c code repre-sentations enables the model to u se meaningful repair signals\nand produce better patches. Seco nd, parameter-ef\ufb01cient \ufb01ne-\ntuning helps \ufb01ne-tuning to converge and clearly contributes tothe effectiveness of RepairLLaMA in \ufb01xing bugs outside the\ufb01ne-tuning data distribution. Overall, RepairLLaMA correctly\ufb01xes 144 Defects4J v2, 109 HumanEval-Java, and 20 GitBug-Java bugs, outperforming all baselines.\nIndex Terms \u2014Automated program repair, large language mod-\nels, code representations, parameter-ef\ufb01cient \ufb01ne-tuning.\nI. INTRODUCTION\nAUTOMATED program repair (APR) [1],[2]aims at au-\ntomatically \ufb01xing a software bug without human inter-\nvention. Learning-based repair [3],[4],[5],[6],[7],[8],[9],\n[10] has become the mainstream solution to this problem due\nto the powerful ability of deep neural networks to learn com-\nplex bug \ufb01x patterns. Large language models (LLMs), pre-\ntrained on vast amounts of data, have pushed learning-basedrepair to the next frontier [8],[11]. There are two main re-\nsearch lines in applying LLMs to program repair: (1) \ufb01ne-tuning\ntechniques to adapt pre-trained LLMs and create specializedrepair models [5],[8],[10],[12], and (2) prompt engineering\nReceived 7 June 2024; revised 30 May 2025; accepted 2 June 2025. Date of\npublication 18 J une 2025; date of current version 15 August 2025. This work\nwas supported in part by Wallenberg AI, Autonomous Systems and SoftwareProgram (WASP) funded by the Knut and Alice Wallenberg Foundation.Recommended for acceptance by K. Tantithamthavorn. (Andr\u00e9 Silva and Sen\nFang contributed equally to this work.) (Corresponding author: Andr\u00e9 Silva.)\nAndr\u00e9 Silva and Martin Monperrus are with KTH Royal Institute of Tech-\nnology, 114 28 Stockholm, Sweden (e -mail: andreans@kth.se; monperrus@\nkth.se).\nSen Fang is with NC State University, Raleigh, NC 27695 USA (e-mail:\nsfang9@ncsu.edu).\nDigital Object Identi\ufb01 er 10.1109\/TS E.2025.3581062and agent-based appr oaches that lever age LLMs\u2019 capabilities\nthrough carefully designed inputs and interaction patterns[13],[14],[15],[16],[17].\nFine-tuning has the potential to learn domain-speci\ufb01c rep-\nresentations and leverage signals in order to generate high-quality patches. The drawback is t hat it requires substantial\ncomputational resour ces and high-quality trai ning data. Prompt\nengineering and agent-based approaches mostly consist of adhoc human-developed prompts and work\ufb02ows to instruct mod-\nels for speci\ufb01c tasks and are bounded by the limited size of con-\ntext windows. While both approaches offer different trade-offs,we argue that \ufb01ne-tuni ng is crucial for hi gh-quality automated\nprogram repair because: (1) it allows the model to internalize\nrepair patterns from a supervised corpus of bug \ufb01xes, (2) it en-ables learning of task-speci\ufb01c repair representations rather than\nrelying on manual prompt engineering steps, and (3) it provides\nopportunities to incorpor ate repair-speci\ufb01c information during\ntraining. Our paper is a key contribution in the space of \ufb01ne-\ntuning LLMs for program repair.\nFine-tuning LLMs for program repair is complex. Early work\nsimply re\ufb01nes the network we ights based on additional \ufb01ne-\ntuning data. However, this kind of \ufb01ne-tuning is rather primitiveand suffers from two signi\ufb01cant drawbacks. First, \ufb01ne-tuning\nis also known to be able to adapt the input\/output representa-\ntions of the data under study [18]. In the context of program\nrepair, there is an opportunity to \ufb01ne-tune with code repre-\nsentations that maximize downstream task performance, that\nis, repair performance. In particular, previous work overlooksthe realistic representation of fault localization in the input.\nSecond, previous work considered the most basic \ufb01ne-tuning\ntechnique, which is full-parameter \ufb01ne-tuning. As LLMs in-crease in size [19], full-parameter \ufb01ne-tuning poses important\nover\ufb01tting problems when \ufb01ne-tuni ng data is limited, which is\ntypically the case in program repair. In this paper, we addressthe problem of devising ef\ufb01cient \ufb01ne-tuning techniques [20]\nfor program repair, with a focus on code representations and\nadapters.\nWe propose RepairLLaMA, a new program repair approach\nthat leverages parameter-ef\ufb01cient \ufb01ne-tuning to adapt LLMsto handle repair-speci\ufb01c code representations. First, Repair-\nLLaMA\u2019s code representations incorporate fault localization\nsignals and are designed to s upport multi-loca tion bugs. Second,\nRepairLLaMA utilizes Low-Rank Adaption (LoRA), a state-\nof-the-art parameter-ef\ufb01cient \ufb01ne-tuning technique, to train a\nmuch smaller repair adapter (when compared to the full LLM)\nthat adapts the LLM for program repair while helping prevent\n\u00a9 2025 The Authors. This work is licensed under a Creative Co mmons Attribution 4.0 Licen se. For more information,\nsee https:\/\/creativecommons.org\/licenses\/by\/4.0\/\nSILV A et al.: R EPAIR LLAMA: EFFICIENT REPRESENTATIONS AND FINE-TUNED ADAPTERS 2367\nover\ufb01tting [21]. As we will demonstrate in this paper, the con-\ncept of repair adapter is novel and potent.\nOur experimental results validate RepairLLaMA\u2019s core de-\nsign. First, RepairLLaMA achieves state-of-the-art \ufb01ne-tuning\nperformance in three benchmarks, correctly \ufb01xing 144 De-\nfects4J v2 [22] bugs, and 109 and 20 bugs, respectively,\non recently proposed benchmarks HumanEval-Java [8]and\nGitBug-Java [23], which boosts internal and external valid-\nity. The experiments show that the devised code representa-tions with repair signals allow the LLM to synthesize patches\nmore effectively than the naive code-only representations. Also,\nRepairLLaMA clearly outperforms non-\ufb01ne-tuned baselines,incl. GPT-4. Moreover, our results also show the effective-\nness of parameter-ef\ufb01cient \ufb01ne-tuning: RepairLLaMA\u2019s repair\nadapters, with only 4M parameters, are 1600x smaller than the\ninitial pre-trained LLM, CodeLLama-7B [24]. To sum up, the\nef\ufb01cient representations and repair adapters of RepairLLaMAoutperform recent results on \ufb01ne-tuning for program repair [8],\n[10],[25] as well as world-class models such as GPT-3.5 and\nGPT-4.\nOverall, we make the following contributions:\n\u2022We design RepairLLaMA, an original \ufb01ne-tuning pipeline\nfor automated program repair with LLMs. RepairLLaMA\u2019srepresentations maximize knowledge from the program\nrepair domain, while keeping strong alignment with pre-\ntraining.\n\u2022We systematically evaluate different code representations\nfor program repair \ufb01ne-tuning. Our results clearly show\nthat the best code representation leverages the task-speci\ufb01csignals of fault localization and original buggy code.\n\u2022We demonstrate that parameter-ef\ufb01cient \ufb01ne-tuning per-\nforms competitively with full- parameter \ufb01ne-t uning in the\ncontext of program repair. The \u201crepair adapters\u201d of Repair-\nLLaMA are training-ef\ufb01cient, and achieve state-of-the-artrepair performance across three benchmarks, Defects4J,\nHumanEval-Java, and GitBug-Java, outperforming even\nGPT-4.\n\u2022For the sake of open science, we publish our source code,\nmodels, and artifacts at https:\/\/github.com\/ASSERT-KTH\/\nrepairllama.\nII. R\nEPAIR LLAMA: E FFICIENT FINE-TUNING\nFORPROGRAM REPAIR\nA. Overview\nFig. 1illustrates the pipeline o f RepairLLaMA for APR,\nwhich is divided into three consecutive stages. The core nov-\nelties of this pipeline are: 1 ) the APR speci\ufb01c code represen-\ntations, and 2) the end-to-end use of a parameter-ef\ufb01cient \ufb01ne-\ntuning technique.\nThe core of RepairLLaMA is a repair adapter .Arepair\nadapter is a plug-and-play extension of the model parame-\nters that modi\ufb01es the behavior of the LLM in order to maxi-\nmize performance on the repair task, for a given programminglanguage. The adapter is responsible for transforming a rich,\ntailored input representation of the buggy code into the \ufb01t output\nrepresentation of the patch.In the \ufb01rst stage of RepairLLaMA, the core choices are made,\nnamely: 1) the initial pre-t rained model (subsection II-C); 2)\nthe input code representation and output code representation(subsection II-D); and 3) the \ufb01ne-tuning dataset (subsection\nII-E). These choices are all important and are further discussed\nin the remainder of this section.\nIn the second stage, a repair adapter is trained. The repair\nadapter is a much smaller (i.e., approx. 4M parameters) plug-\nand-play adapter of the initial LLM while remaining competi-tive on the task of program repair.\nFinally, in the third stage, the repair adapter is employed to\n\ufb01x real-world bugs.\nB. Target Bugs\nThe \ufb01rst consideration when designing a \ufb01ne-tuning pipeline\nfor program repair is the bugs we aim to \ufb01x. This relates to 1)the programming language, 2) the type of bugs (syntax errors,\nruntime errors, functional errors, etc), and 3) the dif\ufb01culty of\nbugs, which can be proxied by the code span to modify in orderto \ufb01x the bug.\nIn this work, we focus on 1) Java bugs, 2) that are functional,\nand come with at least a failing test case, 3) that are intra-procedural, i.e. \ufb01xed with changes to a single function (called\nhereafter single-function bugs ). We do not make any assump-\ntion on the length of the met hod under repair, and 4) explicitly\nsupport bugs that r equire changes in mu ltiple locations in the\nfunction [26], beyond single-line or single-chunk bugs.\nC. Choice of the Initial LLM\nChoosing the suitable initial LLM for \ufb01ne-tuni ng is crucial.\nFor example, when \ufb01ne-tuning for code-related tasks, an LLM\npre-trained on large-scale code corpora is more effective thanone pre-trained on pure natural language data. To effectively\n\ufb01ne-tune an LLM for APR, we curate three criteria to choose\nthe initial model.\nFirst, the LLM should be publicly available and open-source.\nFine-tuning a closed-source LLM on the task-speci\ufb01c dataset\nis not a valid option. Although some companies like OpenAIdo provide an API for \ufb01ne-tuning their LLMs, it is expensive,\nand the ownership of the \ufb01nal model (incl. weights) does not\nmeet open-science reproduction criteria. Open-source models,such as LLaMA [27] or StarCoder [28], publish model weights\nonline, allowing anyone to modify and deploy them.\nSecond, the LLM should be pre-trained with large-scale\ncode data. As observed by related work [24],[28], LLMs pre-\ntrained on massive code data achieve better performance in\ncode-related tasks. Thus, we consider only LLMs specialized\non code.\nThird, the initial LLM shoul d have been trained with an\nin\ufb01lling objective [29] during pre-training. As observed by\nrelated work [8], in\ufb01lling is a natural and effective learning\nobjective for the program repair task, since it allows the modelto synthesize code according to both the context appearing\nbefore and after. It should also be supported by an off-the-shelf\nparameter-ef\ufb01cient \ufb01ne-tuning library.\n2368 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 51, NO. 8, AUGUST 2025\nFig. 1. Overview of RepairLLaMA. The core novelties of RepairLLaMA ar e the APR speci\ufb01c code representations and the engineering of an effective\nprogram repair adapter that is plugged into the underlying LLM.\nIn subsection III-B we instantiate those criteria in the context\nof functional program repair for Java.\nD. Choice of Code Representations\nSource code repres entation is a critical aspect that signif-\nicantly impacts the effectiveness of the model [30].I nt h i s\nsection, we discuss key characteristics of the source code rep-\nresentation design space. We introduce, motivate, and elaborateon input and output code representations speci\ufb01c to the program\nrepair task.\n1) Representation of Fault Localization: Virtually all the\nAPR literature a ssumes line-based faul t localization, with a\nsingle line given as input to the repair algorithm. This is not\nappropriate to \ufb01x multi-location bugs [26],[31]. Consider Fig. 3\n(OR4), which shows the canonical patch for the multi-location\nbug Chart-5 from Defects4J. In this case, fault localization\nmust identify a location where an entirely new if block shouldbe synthesized and inserted as well as another pre-existing if\ncondition, appearing later in th e code. To our knowledge, there\nis no fault localization technique able to predict tuples of blocksto be repaired together.\nIn this paper, we propose a novel way to represent fault\nlocalization information: our core idea is to represent fault\nlocalization not as a single line, but as a region. In Re-\npairLLaMA, we encode fault localization as a span rang-ing from the beginning of the suspicious region to its end.\nThis encoding is realistic because 1) identifying a buggy\nmethod is within reach of existing fault localization methods,and 2) exhaustively listing all suspicious code regions of a\nbuggy method is worst-case O(n\n2)in the number of method\nlines.2) Input Representation Space: In APR, the design space of\nthe input representation relates to what is shown from the buggy\ncode and to the presence of additional information. For exam-\nple, fault localization signals can be useful in scoping downwhere the code should be modi\ufb01ed. However, such information\nmight not be seen at the pre-training stage. For the LLM to\nutilize it, one must represent it in a way that it can learn during\ufb01ne-tuning. To study the input representation space, we design\nfour input representations tailored to APR (Fig. 2):\nIR1: Buggy function This naive representation describes\nthe code in the standard format as i t is written, simply as text.\nFig.2(IR1) shows the buggy function of the multi-location bug\nChart-5, a Defects4J bug. The advantage of IR1 is that it is thesame representation LLMs observe during pre-training. When\nusing this representation, the main limitation is that the model\nhas no access to fault localization information and, thus, needs\nto determine where to change the code, which can be considered\nas implicit anomaly detection.\nIR2: Buggy function w\/ FL comments This representation\nadds two comments signaling the start and end of the buggy\nchunk of code. For example, in Fig. 2(IR2), the three lines be-\ntween the start and end of the suspicious region are surrounded\nby comments signaling the beginning and end of the buggy\nchunk. By providing fault localization information, the modelcan scope its changes to the buggy section.\nIR3: Buggy function w\/ in\ufb01lling mask This representation\nuses the in\ufb01lling scheme some LLMs are trained for duringpre-training [29]. The buggy chunk is replaced by the in\ufb01lling\ntoken, which prompts the model to \ufb01ll it. For example, in Fig. 2\n(IR3), the three lines between the start and end of the suspiciousregion are replaced by the <FILL_ME >token. This representa-\ntion yields shorter inputs and requires less \ufb01ne-tuning since the\nSILV A et al.: R EPAIR LLAMA: EFFICIENT REPRESENTATIONS AND FINE-TUNED ADAPTERS 2369\nFig. 2. Buggy code of the multi-location bug chart-5 represented in our four\ndifferent input representations.\nin\ufb01lling objective has been used dur ing pre-training. However,\nby masking the buggy portion of code, this representation incursinformation loss that can be useful to generate a \ufb01x.\nIR4: Buggy function w\/ in\ufb01lling mask and buggy code\nThis representatio n combines the buggy code with the in\ufb01lling\nscheme. The buggy code is shown in a comment at the end of\nthe pre\ufb01x portion. For example, in Fig. 2(IR4), the buggy lines\nare kept in comments, and the <FILL_ME >token is placed\nimmediately afterward. This representation is different from the\none learned during pre-training and requires \ufb01ne-tuning. Code\nfound in the wild would typically not include buggy code ascomments, which is considered bad practice. Yet, with \ufb01ne-\ntuning, this representation might add valuable information to\nthe in\ufb01lling scheme.\n3) Output Representation Space: Output representations\nin APR correspond to the representation of the synthesized\ufb01xed code. A natural output representation is a diff over\nthe buggy code, aka a patch. As discussed in subsubsection\nII-D2 , \ufb01ne-tuning is required to adapt an LLM to generate\nsuch task-speci\ufb01c outputs. To study the output representa-\ntion space, we design four output representations tailored to\nAPR (Fig. 3):Fig. 3. Patch for multi-location bug chart- 5 represented in our four different\noutput representations.\nOR1: Fixed function The naive output is the full \ufb01xed\nfunction. It is not a diff. Fig. 3(OR1) shows the \ufb01xed function\nof the multi-location bug Char t-5. The major drawback of OR1\nis that such output may be much larger than the actual code\nchanges for \ufb01xing, and LLMs are known to be more effective\nat generating short sequences over long sequences.\nOR2: Fixed chunk In this representation, the output is com-\nposed of the \ufb01xed chunk of code to replace the buggy chunk of\ncode. The advantage is that the \ufb01xed chunk is typically shorterthan the full function, i.e. shorter than OR1. For example, in\nFig. 3(OR2), only 6 \ufb01xed lines are outputted. OR2 requires\nan input representation that includes fault localization (i.e. IR2,IR3, IR4) since the output contains no information regarding\nwhat to replace.\nOR3: Three-line context-diff The output is a typical con-\ntextual diff with a three-line context, aka a uni\ufb01ed diff. For\nexample, in Fig. 3(OR3), a uni\ufb01ed diff of the statement change\nis outputted. The main challenge of this representation is that\nthe model needs to learn to locate the bug locations during \ufb01ne-\ntuning, which is dif\ufb01cult. Additionally, this representation isalso lengthier than generating a \ufb01xed chunk (OR2) only.\nOR4: One-line context-diff The output is a contextual\ndiff with a shorter, one-line context. OR4 uses a one-line diff\n2370 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 51, NO. 8, AUGUST 2025\nTABLE I\nPOSSIBLE CODE REPRESENTATION PAIRS FOR FINE-TUNING LLM S\nFOR AUTOMATED PROGRAM REPAIR .THEY EXPLOIT THE\nCHARACTERISTICS OF THE APR T ASK,INCL.THE PRESENCE OF FAULT\nLOCALIZATION SIGNALS AND THE NOTION OF \u2018\u2018BUGGY CODE\u2019\u2019\nCode Representations FL Aligned w\/ PT Buggy Code\nIR1 x OR1 \u2717 \u2714\/\u2717 \u2714\nIR1 x OR3 \u2717 \u2714\/\u2717 \u2714\nIR1 x OR4 \u2717 \u2714\/\u2717 \u2714\nIR2 x OR2 \u2714 \u2717\/\u2714 \u2714\nIR3 x OR2 \u2714 \u2714\/\u2714 \u2717\nIR4 x OR2 \u2714 \u2717\/\u2714 \u2714\ncontext, making it shorter than OR3. For example, in Fig. 3\n(OR4), there are \ufb01ve source code lines less when compared with\nOR3. Despite this, it is still lengt hier than OR2 and also requires\nthe model to learn where to apply the patch.\n4) Input\/Output Representation Pairs: To utilize an LLM\nfor APR, input and output representations must be carefullypaired. This is because all input representations cannot be paired\nwith all output representations. For instance, IR1 cannot pair\nwith OR2 since one cannot apply a \ufb01xed chunk to the buggyfunction without the fault localization information. Table Ipro-\nvides the list of the code representation pairs that are studied\nin this paper. Each row corresponds to a code representationpair. Column FLindicates whether the pair includes or not\nfault localization information. Column Aligned w\/ PT provides\na relative assessment of the alignment of the representationw.r.t. the pre-training data\/objective. A red cross means that the\ncode representation is not aligned with the pre-training data and\nobjective. The left side shows the input and the right the outputrepresentations. Column Buggy Code indicates whether the pair\nincludes or not the original buggy code.\nThe \ufb01rst three rows (i.e., IR1xOR1, IR1xOR3, IR1xOR4)\ninclude code representation pairs that do not contain fault lo-\ncalization signals. The input is the same across all pairs (IR1),whereas the output can either be the full \ufb01xed function (OR1)\nor a diff (OR3, OR4). The key difference between the pairs is\nthe output length and format.\nThe latter three rows (i.e., IR2xOR2, IR3xOR2, IR4xOR2)\ninclude code representation pairs that contain fault localiza-\ntion information, either as tokens or as in\ufb01lling, which is spe-ci\ufb01c to program repair. The most aligned representation with\npre-training is IR3xOR2 since the pre-trained model has sup-\nport for in\ufb01lling. IR2 represents the in\ufb01lling objective withnever-before-seen comments, whereas IR4 keeps the buggy\ncode as comments. The natural output representation to pair\nwith these is OR2 since it only includes the new code to re-place the already localized buggy chunk, minimizing output\nlength. Note that we have empirically tested other combina-\ntions in a pilot experiment, and the ones not listed in Table I\nunderperform.\nE. Choice of Fine-Tuning Dataset\nAfter choosing an initial m odel and appropria te code repre-\nsentations, the next step is to curate a \ufb01ne-tuning dataset. First,\nthe dataset must be relevant to the task at hand. In the APRtask, a relevant dataset usually includes pairs of buggy and \ufb01xed\ncode samples. Second, the type of samples included should be\nsimilar to the target bugs. Third, the size of the dataset shouldbe considered. A larger dataset generally leads to better model\nperformance as it provides more examples for the model to\n\ufb01ne-tune from. However, it is important to balance size withquality - a smaller, high-quality dataset may b e more bene\ufb01cial\nthan a larger, low-quality one. Four th, the diversity of the dataset\nis important. A diverse dataset that covers a wide range of exam-ples can help the model generalize better to unseen data. Lastly,\nthe legality and ethics of the dataset should b e considered, in\nparticular regarding privacy and copyright.\nF . Program Repair Adapters for LLMs\nWith the recent release of various LLMs, the scale of pa-\nrameters has signi\ufb01cantly increased. For instance, state-of-the-\nart models such as LLaMA [19] and CodeLLaMA [24] range\nfrom 7B to 70B parameters. Fine-tuning these LLMs often\nrequires substantial GPU resources. As an example, Lv et al.\n[32] report that \ufb01ne-tuning the full parameters of LLaMA-7B\non an RTX 3090 consumes 126.08 GB at peak GPU memory\nusage, with the batch size and sequence length set to 1 and 1024\nrespectively. Fine-tuning current LLMs with limited resourcesis a challenge.\nRepairLLaMA uses LoRA [20], a state-of-the-art parameter-\nef\ufb01cient \ufb01ne-tuning method that reduces memory requirementswhile maintaining model performance. Instead of \ufb01ne-tuning\nall model parameters, LoRA freezes the pre-trained LLM\nweights and injects trainable low-rank matrices into speci\ufb01cattention layers. In addition to reducing memory requirements,\nby reducing the number of trainable parameters LoRA also\nacts as a regularizer [21] helping prevent over\ufb01tting during\n\ufb01ne-tuning.\nIn our implementation, we apply LoRA to the query\n(q_proj ) and value ( v_proj ) projection matrices in each\ntransformer\u2019s self-attention layer. These projection matrices\ntransform the input embeddings into query and value vectorsused in the attention. For each target weight matrix A\u2208R\nd\u00d7k,\nLoRA decomposes the update into a product of two lower-rank\nmatricesB\u2208Rd\u00d7randC\u2208Rr\u00d7k, whereris the rank. During\ninference, the effective weight matrix is W=W0+BA, where\nW0is the frozen pre-trained weight.\nThe trained matrices can be interpreted as a repair adapter\nthat is much smaller than the original LLMs. For example,\nwith rank r =8, our adapter only requires training 0.39%1of\nthe parameters compared to full \ufb01ne-tuning. This parameter-ef\ufb01cient approach allows RepairLLaMA to achieve strong pro-\ngram repair performance while being trainable on a single GPU.\nG. Inference Time\nThe \ufb01nal step is to deploy the repair adapter. The tar-\nget buggy program is fed to a fault localization algorithm\n1Take CodeLLaMA-7B as an example, its hidden size is 4096, so the\noriginal parameter of a matrix is 4096 \u00d74096. As for the LoRA adapter,\nthis value is 4096 \u00d78 + 4096 \u00d78.\nSILV A et al.: R EPAIR LLAMA: EFFICIENT REPRESENTATIONS AND FINE-TUNED ADAPTERS 2371\nand processed to generate an APR-speci\ufb01c code represen-\ntation. Then, the code represent ation is fed to the initial\nmodel combined with the LoRA repair adapter to generate alist of candidate patches for the buggy program. Patches are\nthen checked for plausibility and co rrectness per of f-the-shelf\ntechniques.\nIII. E\nXPERIMENTAL METHODOLOGY\nA. Research Questions\nIn this work, we focus on the following research questions:\n\u2022RQ1 (Code Representations for Fine-Tuning) : What\nis the best code representation to \ufb01ne-tune an LLM forprogram repair?\n\u2022RQ2 (Parameter-Ef\ufb01cient Fine-Tuning vs. Full Fine-\nTuning) : How does parameter-ef\ufb01cient \ufb01ne-tuning com-\npare against full-parameter \ufb01ne-tuning for program repair?\n\u2022RQ3 (RepairLLaMA vs. ChatGPT-based APR) :\nHow does RepairLLaMA compare against state-of-the-artChatGPT-based program repair?\nB. Implementation\nModel to Fine-Tune Per the criteria of subsection II-C,w e\nchoose CodeLlama-7b [24] as our initial LLM. CodeLLaMA\nis a publicly available LLM released in 2023 and is trained\non 500B code tokens. Per the experiments reported in [24],\nCodeLLaMA outperforms GPT-3.5 on two code generation\nbenchmarks.\nFine-tuning Dataset We choose Megadiff [33] as the \ufb01ne-\ntuning dataset, and process all samples into the different\ncode representations. First, the function pairs \u2013 each com-\nprising a buggy version and its \ufb01xed counterpart \u2013 are ex-\ntracted along with their corresponding diff identi\ufb01ers. Subse-\nquently, we eliminate pairs that do not change single func-tions, and remove duplicate pairs through textual compar-\nison. After that, we compute our custom code representa-\ntions. We keep only samples whose total length (input plusoutput) is shorter than 1024 tokens measured by the LLM\ntokenizer. Consequently, the \ufb01ne-tuning datasets range from\n30,000 to 50,000 \ufb01ne-tuning pairs (see our appendix repositoryat https:\/\/github.com\/A SSERT-KTH\/repairllama).\nEvaluation Benchmark We select three Java benchmarks\nfor our evaluation: Defects4J [22], HumanEval-Java [8], and\nGitBug-Java [23]. Following recent related work [11],[14],\n[34], we scope our evaluation to single-function bugs, as de\ufb01ned\nin subsection II-B. Defects4J comprises 835 real-world bugs\nfrom 17 open-source Java projects, from which we identify\n488 single-function bugs. HumanEval-Java is a bug bench-\nmark containing arti\ufb01cial bugs inserted in HumanEval [35]\nJava programs. HumanEval-Java contains 162 single-function\nbugs. GitBug-Java is a bug benchmark of recent bugs, collectedfrom the 2023 commit history of 55 open-source repositories,\ncomprising 90 single-function bugs. Contrary to Defects4J,\nGitBug-Java suffers from less data leakage in the pre-trainingdata since all bugs are much more recent than Defects4J. No-\ntably, GitBug-Java exclusively contains bugs from after thetraining data cutoff date of all models used in our experiments:\nCodeLlama-7b (September 2022) [24],gpt-4-0613 (September\n2021)\n2, and gpt-3.5-turbo-0613 (September 2021)2.\nFine-Tuning Hyperparameters We \ufb01ne-tune CodeLLaMA\nwith LoRA for each of our curated code representations with\nthe same hyper-parameter setti ngs: we set the learning rate\nto 5e-4 with cosine decay, max input length to 10243,t r a i n -\ning epoch to 2, and batch size to 16 per GPU, and we use\nAdam_W as the optimizer. For LoRA, we use a rank of 8, alphaof 16, dropout of 0.05, and inject the adaptation matrices in\ntheq_proj andv_proj layers. Using the same hyper-parameter\nsettings for each code representation ensures fair comparison.Each \ufb01ne-tuning run is executed on a server with 4xA100\n40GB GPUs.\nInference Setup In inference, we employ beam search as our\ndecoding strategy with a beam size of 10 per previous research\n[8]. Hence, for each bug, we generate 10 candidate patches. We\nuse the HuggingFace transformers library to implement all \ufb01ne-\ntuning and inference experiments. Inference is run on a single\nA100 40GB GPU.\nC. Patch Assessment\nPatch assessment is a notoriously hard task. In our evaluation,\nwe compute the best metrics for that task [8],[11],[26]:1 )\nAplausible patch is de\ufb01ned as one that successfully passes\nall test cases. 2) An exact-match patch is textually identical to\na developer-provided reference patch, incl. spaces and format-\nting. 3) An AST-match patch has an AST which is equivalent\nto the AST of the developer-provided reference patch. 4) Asemantic-match patch is a patch deemed equivalent after man-\nual assessment by an expert.\nPlausible and exact-match patches are straightforward. Let\nus dwell on the two other kinds.\nThe major advantage of an AST-match patch is to compute\nperformance regardless of form atting and indentation changes.\nIt is also more scalable than manually checking patches for\ncorrectness without expertise in the programs under repair. TheAST-match process involves converting plausible and reference\npatches into abstract syntax trees [36] and subsequently utiliz-\ning AST differencing [37] to compare their ASTs for discrep-\nancies.\nAsemantic-match patch is the most costly assessment to\nget. In this paper, to assess semantic equivalence, the two \ufb01rstauthors independently label all plausible but not AST\/Exact\nmatch patches in a \ufb01rst round. For the patches the two \ufb01rst\nauthors disagree upon, the third author breaks the tie.\nFor all four metrics, the higher the metric, the better the per-\nformance. We validate the candidate patches on a workstation\nwith an 18-core Intel Core i9-10980XE CPU and 128 GB ofRAM, operating under Ubuntu 22.04.3 LTS.\n2https:\/\/platform.ope nai.com\/docs\/models\n3Although longer input lengths enable the model to process more context\nand, potentially, \ufb01x more bugs, about four times more GPU memory is\nrequired when doublin g the input length.\n2372 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 51, NO. 8, AUGUST 2025\nD. Methodology for RQ1\nThe objective of RQ1 is to investigate the most effective\ncode representations for \ufb01ne-tuning an LLM for program repair.\nWhile existing research has delved into the utility of LLMs for\nprogram repair, the impact of the code representations, such as\ntheir realism, has been overlooked. It is known that variations\nin code representations may yield substantial differences inperformance for \ufb01ne-tuned LLMs [25]. Consequently, in RQ1,\nwe empirically evaluate 6 realistic code representation pairs\npresented in II-D and measure their performance.\nWe \ufb01ne-tune an LLM as described in III-B .W ep r o m p tt h e\nmodel to generate 10 patches for each bug using beam search\ndecoding. We then evaluate the generated patches as outlined\nin subsection III-C , to measure the effectiveness of each code\nrepresentation. We prompt the non \ufb01ne-tuned CodeLLaMA-7B\nas a baseline.\nWe assess the statistical signi\ufb01cance of performance differ-\nence by employing the McNemar test for each pairwise com-\nbination of representations. In our context, each statistical testlooks at the binary outcomes of two representations (or models)\nevaluated on the same set of benchmark examples, according to\nsemantical match. The null hypothesis ( H\n0) for the McNemar\u2019s\ntests is that the distributions are indistinguishable, i.e. the row\nand column marginal frequencies in the 2x2 contingency ta-\nble are equal, i.e., H0:P(A=1,B=0)=P(A=0,B=1) ,\nwhere A and B represent whether A and B, respectively, \ufb01xed\na given bug.\nE. Methodology for RQ2\nThe objective of RQ2 is to evaluate the respective effec-\ntiveness of parameter-ef\ufb01cient and full-parameter \ufb01ne-tuning.\nGenerally, parameter-ef\ufb01cient \ufb01ne-tuning methods represent a\ntrade-off between computational cost and model performance,in order to train LLMs with limited computational resources.\nWhile traditional full-parameter \ufb01ne-tuning approaches often\nyield better results, it comes at the expense of signi\ufb01cantlyhigher memory requirements and a larger-scale \ufb01ne-tuning\ndataset. In other words, fully \ufb01ne-tuning an LLM on a small\n\ufb01ne-tuning dat aset may result in over\ufb01 tting. In this experiment,\nwe explore and compare the effectiveness of parameter-ef\ufb01cient\nand full-parameter \ufb01ne-tuning in the speci\ufb01c context of program\nrepair, which has never been done to the best of our knowledge.\nBaseline. We consider two baseline base models in RQ2:\nCodeLLaMA-7B and deepseek-coder-6.7b-base [38]. To study\nthe difference between parameter-ef\ufb01cient \ufb01ne-tuning and full-parameter \ufb01ne-tuning, we \ufb01ne-tune both models with both ap-\nproaches. Here, we use the same hyper-parameters as in LoRA\n\ufb01ne-tuning described in RQ1. We use a learning rate of 2e-5 for\nfull-parameter-\ufb01ne-tuning. We employ the same statistical sig-\nni\ufb01cance test as described in subsection III-D when comparing\nRepairLlama against these baselines.\nAdditionally, we benchmark our approach against the\nhighest-performing model documented by Jiang et al. [34]\u2014\nspeci\ufb01cally, the \ufb01ne-tuned variant of Incoder-6B [39].\n\n[Text truncated due to context length limits]","filename":"Repairllama__Efficient_representations_and_fine-tuned_adapters_for_program_repair.pdf","responses":"[1] Machine Learning For Code: Applying machine learning techniques to software engineering tasks, including automated program repair, fine-tuning of LLMs for code, and development of code representations and adapters."}
{"id":"pdf_6","text":"https:\/\/doi.org\/10.1007\/s10664-025-10658-6\nDo LLMs consider security? an empirical study on responses\nto programming questions\nAmirali Sajadi1\u00b7Binh Le1\u00b7Anh Nguyen1\u00b7Kostadin Damevski2\u00b7\nPreetha Chatterjee1\n\u00a9 The Author(s) 2025\nAbstract\nThe widespread adoption of conversational LLMs for software development has raised new\nsecurity concerns regarding the safety of LLM-generated content. Our motivational study\noutlines ChatGPT\u2019s potential in volunteering context-speci\ufb01c information to the developers,\npromoting safe coding practices. Motivated by this \ufb01nding, we conduct a study to evaluate\nthe degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and\nLlama 3. We prompt these LLMs with Stack Over\ufb02ow questions that contain vulnerable\ncode to evaluate whether they merely provide answers to the questions or if they also warn\nusers about the insecure code, thereby demonstrating a degree of security awareness. Further,\nwe assess whether LLM responses provide information about the causes, exploits, and the\npotential \ufb01xes of the vulnerability, to help raise users\u2019 awareness. Our \ufb01ndings show that all\nthree models struggle to accurately detect and warn users about vulnerabilities, achieving\na detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs\ntend to identify certain types of vulnerabilities related to sensitive information exposure and\nimproper input neutralization much more frequently than other types, such as those involving\nexternal control of \ufb01le names or paths. Furthermore, when LLMs do issue security warnings,\nthey often provide more information on the causes, exploits, and \ufb01xes of vulnerabilities\ncompared to Stack Over\ufb02ow responses. Finally, we provide an in-depth discussion on the\nimplications of our \ufb01ndings, and demonstrated a CLI-based prompting tool that can be used\nto produce more secure LLM responses.\nCommunicated by: Xin Xia\nB Amirali Sajadi\namirali.sajadi@drexel.edu\nBinh Le\nbql23@drexel.edu\nAnh Nguyen\nadn56@drexel.edu\nKostadin Damevski\nkdamevski@vcu.edu\nPreetha Chatterjee\npreetha.chatterjee@drexel.edu\n1College of Computing and Informatics, Drexel University, Philadelphia, PA, USA\n2College of Engineering, Virginia Commonwealth University, Richmond, V A, USA\n0123456789().: V,-vol 123Empirical Software Engineering (2025) 30:101\nAccepted: 3 April 2025 \/ Published online: 16 April 2025\nKeywords Security evaluation \u00b7Vulnerability awareness \u00b7Large language models\n1 Introduction\nLarge language Models (LLMs) have become deeply integrated into software engineer-\ning work\ufb02ows, performing tasks such as code generation, summarization, debugging, and\naddressing queries related to programming (Liu et al. 2023a ; Hou et al. 2023 ; Zheng et al.\n2023 ; Belzner et al. 2023 ). In particular, LLM chatbots or conversational LLMs ,s u c ha s\nOpenAI\u2019s GPT (OpenAI 2023 ), Anthropic\u2019s Claude (Anthropic 2024 ), and Meta\u2019s Llama\n(Meta 2024 ), have signi\ufb01cantly impacted problem-solving activities by enabling interactive\nQ&As (Suad Mohamed 2024 ; Das et al. 2024 ; Da Silva et al. 2024 ). Developers use them\nto describe symptoms, provide contextual information, and seek guidance on solutions (Hou\net al. 2023 ). According to a 2023 survey, 92% of U.S.-based developers are using various\ngenerative models to perform or to automate some of their daily tasks (Shani 2024 ).\nHowever, the rapid adoption of LLMs by software developers has raised many concerns\nregarding the security implications of using LLMs. A recent study found that participants\nusing AI assistants produced code with signi\ufb01cantly more vulnerabilities (Perry et al. 2023 ).\nAlarmingly, these participants were also more con\ufb01dent in the security of their code, suggest-\ning that AI code assistants can foster a false sense of security, increasing the risk of introducing\nvulnerabilities into real-world software. Another study found that 32.8% of Python and 24.5%\nof JavaScript code produced by GitHub Copilot are vulnerable (Fu et al. 2023 ). These vulner-\nabilities, if exploited, can lead to severe consequences, such as the Log4Shell vulnerability\n(Kosinski 2023 ). In 2024 alone, over 34,000 vulnerabilities were reported (CVE Program\n2024 ), highlighting the increasing frequency and severity of cybersecurity threats that endan-\nger the safety, security, and reliability of software systems.\nBeyond generating vulnerable code, using LLMs can impact software security in more\nintricate and subtle ways. For instance, novice developers may unknowingly input insecure\ncode (copied from Q&A forums) and ask LLMs to refactor and adapt it to their problem\ncontext. Similarly, during debugging, a developer might provide a block of code containing\nvulnerabilities, such as unsanitized user input in an SQL query, without being aware of its\npotential security implications. If LLMs fail to identify and address these vulnerabilities,\ndevelopers may integrate \ufb02awed code into their projects, relying on the model without rec-\nognizing the potential security risks themselves. To better understand this phenomenon, let\nus consider the following examples. In a SO question, a developer asks for help to resolve\nan issue with writing to a \ufb01le:\nI have a text file with one URL per line, like:\nhttps:\/\/www.google.com\nhttps:\/\/www.facebook.com\n...\nThe problem is, when I write the resulting URL to a file,\nI get anadditional %0A at the end of each line. Can you\nplease explain to me why is this happening? I am using this\nscript to fetch them:\n123101 Page 2 of 29 EmpiricalSoftwareEngineering(2025)30 :101\nadd = open (\"manual_list.txt\" ,\"r\")\nfor ainadd:\nresponse = requests.get(a, timeout=(2, 5), verify=\nFalse)\nfout = open (\"mylist.txt\" ,\"a\")\nfout.write( response.url+ \"\\n\" )\nfout.close()\nHere, the developer is focused on removing the extra %0A characters, which could be\nresolved using the strip() function i.e., requests.get(a.strip(), timeout=\n(2, 5), verify=False)) . However, they fail to notice a signi\ufb01cant security risk\nposed by the verify=False parameter. Disabling SSL certi\ufb01cate veri\ufb01cation is gener-\nally considered poor security practice and exposes the application to serious risks, including\nman-in-the-middle attacks, where an attacker could intercept or manipulate the data sent over\nHTTPS. If prompted with this question, GPT-4, Claude 3, and Llama 3 all correctly explain\nthe issue with %0A and suggested using the strip() function to \ufb01x it. However, none of\nthe LLMs mentioned the security implications of the verify=False parameter, leaving\nthe developer unaware of the potential vulnerability. By failing to inform the user of this\nvulnerability, the LLM responses can indirectly reinforce the faulty implementation and, in\nmany cases, further build upon it, perpetuating insecure coding practices.\nSeveral studies have explored the potential risks associated with the use of LLMs and\nexamined concerns regarding the generation of insecure code (Pearce et al. 2022 ; Siddiq and\nSantos 2022 ; Khoury et al. 2023 ; Siddiq et al. 2024b ), inaccuracies in vulnerability detection\n(Ullah et al. 2024 ; Akuthota et al. 2023 ; Purba et al. 2023 ; Zhou et al. 2024b ), and poten-\ntial misuse for offensive applications, ranging from hardware to user-level attacks (Happe\nand Cito 2023 ; Falade 2023 ). However, most studies have focused on the security of LLM-\ngenerated code, often neglecting the natural language generated by LLMs that plays a critical\nrole in interactive learning and problem-solving (Eastman 2023 ; Xia and Zhang 2023 ;H a o\net al. 2024 ). Additionally, unlike prior research that focuses on detecting vulnerabilities in\nLLM-generated code or evaluating the capabilities of LLMs when explicitly tasked to detect\nvulnerabilities, our work investigates the ability of LLMs to proactively identify vulnera-\nbilities in user-supplied code. This re\ufb02ects real-world use cases where developers that rely\non LLMs for various tasks inevitably prompt LLMs with vulnerable code. Our study is the\n\ufb01rst to assess the security awareness of LLMs using the textual information provided by the\nLLMs alongside or independent of code. We assess not only LLMs\u2019 ability to proactively\ndetect vulnerabilities but also their effectiveness in communicating critical information \u2013\nsuch as causes, exploits, and \ufb01xes \u2013 to enhance developer understanding and prevent the use\nof exploitable and insecure code.\nWe begin with a motivational study using an existing dataset of developer conversations\nwith ChatGPT, investigating the frequency and speci\ufb01city of vulnerability-related warnings\nissued by ChatGPT. Mainly, our motivational study aims to understand the way ChatGPT\nengages with security topics. Initial \ufb01ndings suggest that ChatGPT can offer valuable,\ncontext-speci\ufb01c security guidance that encourages safer practices. More importantly, we\nfound instances where the model voluntarily pointed out the security risks. These \ufb01ndings\nwere signi\ufb01cant, as they demonstrated ChatGPT\u2019s potential to proactively raise developers\u2019\nsecurity awareness during development. This observation motivated us to dig deeper and per-\nform a systematic examination of the security awareness across three popular LLMs: Claude\n3, GPT-4, and Llama 3. We curated a dataset of 300 Stack Over\ufb02ow questions that contain\nvulnerable code. In half of these questions, security vulnerabilities were explicitly noted by\n123EmpiricalSoftwareEngineering(2025)30:101 Page3of29101\nthe SO participants (Mentions-Dataset); in the other half, vulnerabilities were not identi\ufb01ed\n(Transformed-Dataset). We used these questions as prompts to the LLMs and analyzed their\nresponses, focusing on whether they proactively recognized and addressed security concerns\nin both code and text-based guidance. Speci\ufb01cally, we investigate the following research\nquestions:\nRQ1: Given insecure code, do LLMs warn developers of the potential security \ufb02aws or\nrequired security modi\ufb01cations?\nThrough RQ1, we aim to investigate the degree to which LLMs exhibit security aware-\nness. We conduct a qualitative analysis to examine whether LLMs simply provide answers\nto questions or if they also warn users about the security \ufb02aws and suggest potential mod-\ni\ufb01cations to improve code security. Our results indicate that LLMs seldom issue security\nwarnings about vulnerabilities unless explicitly prompted to do so.\nRQ2: In instances where users are reminded of security concerns, are they informed about\nthe causes, potential exploits, and possible \ufb01xes of the vulnerabilities?\nThrough RQ2, we aim to determine the types of information included in the LLM security\nwarnings. We qualitatively analyze the information LLMs provided for each vulnerability,\nspeci\ufb01cally checking if they detailed the causes , potential exploits , and possible \ufb01xes.W e\nthen perform the same analysis on user-provided SO responses containing warnings about\ninsecure code and compare these to the LLM responses across the two datasets. According\nto our \ufb01ndings, in cases where vulnerabilities are pointed out, LLMs generally offer more\ninformation about the causes, exploits, and \ufb01xes of the insecurities compared to SO responses.\nWe discuss the implications of our \ufb01ndings for improving LLM security awareness in\nSoftware Engineering (SE) across three key areas: a) Prompt Engineering for SE, b) Inte-\ngrating LLMs with SE tools, and c) Designing LLMs for SE. By sampling 50 questions from\nour dataset, we explored various prompting techniques to increase the likelihood of LLMs\nissuing security warnings. We observed that adding short phrases such as \u201cAddress secu-\nrity vulnerabilities\u201d to the prompts showed some effectiveness, although limitations persist.\nAdditionally, we develop a pipeline for integrating outputs from static analysis tools like\nCodeQL into LLM prompts and \ufb01nd the potential of this method for enhancing security\nawareness of LLMs. Beyond these immediate solutions, our \ufb01ndings highlight the need for\ntargeted design improvements to address the substantial gaps in LLMs\u2019 security awareness,\nnot only in the code they generate but also in the explanations and recommendations they\nprovide. These insights point to key areas for future research, tool development, and LLM\nevaluations aimed at creating more security-conscious LLMs as programming assistance.\nOverall, this paper presents the \ufb01rst study on the security awareness of three popular\nLLMs in answering programming-related questions. We \ufb01nd that all three LLMs we studied\nrarely warn developers about security vulnerabilities. We fruther assess how well LLMs\ncan raise developer awareness and encourage the adoption of secure coding practices. We\nnotice that when LLMs do issue security warnings, they often provide more information\nabout the causes, potential exploits, and \ufb01xes of the vulnerability, compared to typical SO\nresponses. Observations from this study will inform future designs of tools and evaluation\nmethodologies that aim to make LLM-driven programming more secure. More speci\ufb01cally\nour paper makes the following contributions:\n\u0081A motivational study that examines the naturally occurring security-related conversa-\ntions between developers and ChatGPT and outlines the ability of LLMs for proactively\nwarning users about security.\n\u0081A benchmark for evaluating: (a) LLMs\u2019 capabilities in proactively detecting vul-\nnerabilities in real-world user-supplied code, and (b) issuing warnings with critical\n123101 Page 4 of 29 Empirical Software Engineering (2025) 30 :101\ninformation\u2014such as causes ,exploits ,a n d \ufb01xes\u2014to enhance developer understanding\nand mitigate integration of insecure code in existing code-bases.\n\u0081Preliminary results to demonstrate the opportunities for adapting simple and practical\nprompt engineering techniques to enhance the security of the LLM responses to pro-\ngramming questions.\n\u0081A CLI-based tool that analyzes developer queries and integrates CodeQL outputs to\ngenerate prompts that result in signi\ufb01cantly safer LLM responses.\n2 Motivational Study\nThe 2023 JetBrains survey, based on responses from 26k developers across 196 countries,\nreveals that 77% (i.e., approximately three in four developers) use ChatGPT (JetBrains 2023 ).\nGiven its widespread adoption, we conducted a exploratory study to explore whether security\nconsiderations naturally emerge in developer conversations with ChatGPT. This motivational\nstudy serves as an initial gauge to determine if LLMs engage with security topics at all, help-\ning us assess the feasibility of a larger, more comprehensive study. By examining existing\nChatGPT conversations with explicit security mentions, we aimed to understand the fre-\nquency and depth of security-related advice offered by the model. This approach allowed\nus to examine patterns in ChatGPT\u2019s security guidance, including the initiating party of\nthese discussions and the speci\ufb01c topics addressed. Figure 1illustrates part of a conversation\nin which ChatGPT issues a security warning, highlighting the security implications of the\nsuggested code.\nDataset In May 2023, OpenAI introduced a feature that allows users to share their conver-\nsations with ChatGPT through dedicated links (OpenAI 2023 ). Using this feature, Xiao et al.\n(2023 ) collected all the 3,794 developer-ChatGPT conversations publicly shared on GitHub\nand Hacker News until October 2023, forming the DevGPT dataset. To identify conversa-\ntions that mention security, we performed a keyword search on the text of these ChatGPT\nconversations. As keywords, we used the SO tags related to security proposed by Yang et al.\n(2016 ), such as \u201csecurity\u201d, \u201cweb-security\u201d, \u201csql-injection\u201d, and \u201cxss\u201d. To ensure comprehen-\nsiveness of our data selection, we used both hyphened and non-hyphened variations of the\nkeywords, such as \u201cweb-security\u201d and \u201cweb security\u201d, where applicable. This \ufb01ltering step\nresulted in a subset of the DevGPT dataset containing 233 conversations. We excluded 13 out\nof these 233 conversations that included security-related keywords but are predominantly in\nlanguages other than English, resulting in 220 conversations.\nFig. 1 DevGPT: ChatGPT Reminding User about the Security of the Code\n123Empirical Software Engineering (2025) 30 :101 Page 5 of 29 101\nAlthough DevGPT primarily contains developer-ChatGPT interactions, not all conver-\nsations are guaranteed to relate to software engineering (e.g., programming concepts,\ndebugging, or code generation). Therefore, we conducted a manual check to \ufb01lter out such\nnon-SE conversations. In addition, as part of this manual step, two authors validated the per-\nformance of our keyword search by distinguishing conversations that discuss security from\nthose that merely included security-related keywords without actually addressing security\ntopics, i.e., false positives. The Cohen\u2019s kappa agreement McHugh ( 2012 ) for classifying con-\nversations as security-related or non-security-related achieved the strong score of 0.799, after\nwhich, the authors resolved all con\ufb02icts and \ufb01nalized the data through discussion. Finally, we\nidenti\ufb01ed 102 technical conversations with ChatGPT mentioning software security, shared\nacross various data sources, including code \ufb01les (45), issue threads (33), Hacker News (11),\npull requests (9), and GitHub commits (4).\nProcedure We conducted a qualitative analysis to investigate the role of security in the 102\ndeveloper-LLM conversations with mentions of security. We identi\ufb01ed which party (Devel-\noper or ChatGPT) brings up the security considerations, examined the types of available\ninformation, and identi\ufb01ed the type of vulnerability mentioned in each conversation. Two\nauthors of this paper manually annotated the dataset (annotation instructions with examples\nare included in our replication package). The analysis was performed in an iterative approach\nconsisting of multiple sessions. To calculate inter-rater agreement, we used Cohen\u2019s Kappa\ncoef\ufb01cient. This process resulted in substantial agreement values (i.e., >0.6): 0.88 for types\nof information ,0 . 6 6f o r types of vulnerability , and 0.62 for initiating party .\nTypes of Information To examine the types of information in the ChatGPT conversations,\nwe used the Pan et al. ( 2021 ) taxonomy for for categorizing information in developer chats.\nDeveloper chats (e.g., Slack, Discord) closely align with developer-LLM conversations since\nboth support dynamic, rapid, and iterative information exchange (Chatterjee et al. 2019 ). Con-\nsequently, we believe this taxonomy is well-suited for our study. The information categories\nin the taxonomy are listed in Table 1.\nTypes of Vulnerability The understand the types of vulnerabilities that were discussed,\nwe used a taxonomy proposed by Russo et al. ( 2019 ) and manually identi\ufb01ed the relevant\ncategory for each conversation. We chose Russo et al.\u2019s taxonomy because it provided a\npredetermined set of vulnerability types, allowing us to present an overview without needing\nto determine the level of granularity in CWEs for each vulnerability. Russo et al. categorize\nvulnerabilities into ten distinct types, providing a broad yet concise overview suitable for our\nanalysis.\nAdditionally, we took note of whether ChatGPT makes broad mentions of security, or\nif it offers detailed information about security. We de\ufb01ne a speci\ufb01c mention as a mention\nthat contains any form of implementation details. An example of speci\ufb01c mention could be\na conversation in which ChatGPT warns the user about the possibility of an SQL injection\nTable 1 Distribution of Types of\nInformation in DevGPT\nConversations with Security\nMentionsInformation type Instances\nTechnical Discussion 49\nProgramming Information 31\nProgramming Problems 10\nGeneral Information 5\nDocumentation Information 4\nLibrary Problems 3\n123101 Page 6 of 29 Empirical Software Engineering (2025) 30 :101\nbased on the prompt\u2019s code and offers ways to preventing it. On the other hand, we de\ufb01ne a\nbroad mention as a mention that lacks any implementation details, such as the sentence \u201c It\u2019s\nimportant to consider additional security measures when dealing with \ufb01le uploads... \u201d.\nInitiating Party We also determined whether it is ChatGPT or the user who \ufb01rst introduces\nthe topic of security into the conversation. Speci\ufb01cally, we identi\ufb01ed instances where Chat-\nGPT provided security-related information or where the user directly asked about security\naspects of development. In cases where the question inherently involves security but lacks\nexplicit security mentions in the user\u2019s prompt, we do not attribute the initiation of the security\ndiscussion to either party.\nFindings Table 1illustrates the distribution of the types of information available in all\n102 conversations in our dataset. The most common type of information is Technical Dis-\ncussion , which relates to conceptual conversations about software engineering. The frequent\noccurrences of security mentions in Technical Discussions points to the fact that security is\nmore often included when conversations revolve around higher level issues related to soft-\nware development. In many of these instances, ChatGPT informs the users about the good\npractices with regard to security, making statements such as: \u201c Always ensure that you\u2019re\nfollowing the correct steps as mentioned in the WordPress.com OAuth2 documentation, and\nhandle the tokens securely, keeping them out of URLs whenever possible to maintain secu-\nrity\u201d.Programming Information , i.e., conversations in which the user is mostly trying to get\nthe LLM to generate code or to explain programming concepts, and Programming Problems ,\ni.e., conversations in which the user is mostly trying to resolve programming problems, are\nthe second and third most common types of conversations that lead to notions of security. In\nthese cases, we often see ChatGPT trying to inform the user about the security concerns in\ntheir code or even the code generated by ChatGPT itself as well as the potential \ufb01xes for these\nconcerns. Security, however, is rarely brought up in conversations about the user\u2019s problems\nwith speci\ufb01c libraries or when the users are attempting to retrieve any general or documenta-\ntion related information. General Information , i.e., discussions that are not closely related to\nthe project itself, such as the best choice of IDE or job-hunting experiences, Documentation\nInformation ,a n d Library Problems are the least frequently observed types of information in\nconversations.\nTable 2provides an overview of the types of vulnerabilities .N o t a b l y ,\u201c Authentication\nbypass or Improper Authorization \u201da n d\u201c Cross-Site Scripting or HTML Injection \u201da r et h e\nmost frequently discussed types of vulnerability, while others have much fewer mentions.\nFurther, some types of vulnerabilities such as \u201c Buffer\/Stack\/Heap\/Integer Over\ufb02ow, Format\nString and Off-by-One \u201d were not present in any conversation. We also observed that 54\nconversations include broad mentions of security and the other 48 conversations include\nspeci\ufb01c mentions. These results indicate that in many instances ChatGPT makes statements\nabout security without pointing out a speci\ufb01c vulnerability.\nOur \ufb01ndings about the initiating party show that in 69 out of the 102 cases, ChatGPT is\nthe one who \ufb01rst mentioned security. In 14 out of those 69 conversations, the user followed\nup on this mention of security and further discussed the topic, while in 49 conversations the\nusers did not directly follow up on the mention of security and in 6 instances the conversation\ndid not continue by the user. Further, out of the 69 instances where ChatGPT \ufb01rst brought\nup the topic of security, 42 mentions were broad, while 24 were speci\ufb01c. In the remaining\n33 of the 102 conversations, it was the user who \ufb01rst mentioned security. For example, one\n123Empirical Software Engineering (2025) 30 :101 Page 7 of 29 101\nTable 2 Summary of Vulnerability Categories Mentioned by ChatGPT Throughout Conversations\nVulnerability Cate-\ngoryDescription Instances Example\nAuthentication bypass\nor Improper Autho-\nrizationAllowing attacker\nto bypass required\nauthentication or not\nperforming required\nauthentication checks18 authorization code should be unique and tem-\nporary for each user session...each code can\nonly be exchanged once for security reasons.\nCross-Site Scripting or\nHTML InjectionAllowing attacker to\nexecute arbitrary code\nin the web browser and\nstealing cookie-based\ncredentials.16 This means that the cookie can only be\naccessed via HTTP requests and not through\nclient-side scripts, which is a good security\nmeasure to prevent XSS attacks.\nSQL Injection Not properly sanitiz-\ning user input before\nusing them in SQL\nqueries.5 Prepared statements are ef\ufb01cient and safe\nagainst SQL injection attacks.\nInformation Disclo-\nsure and\/or Arbitrary\nFile ReadAllowing attacker to\nget access to informa-\ntion and \ufb01les.3 Avoid hardcoding them in your Python script.\nInstead, use environment variables or AWS\npro\ufb01les.\nDirectory Traversal Allowing attacker to\ngain read access to\narbitrary \ufb01le content.2 Exposing the entire node_modules directory\npublicly is generally a bad idea, due to the\npotential security risks and unnecessary expo-\nsure of dependencies.\nRemote Code Execu-\ntionAllowing attacker to\nexecute arbitrary code\nwithin the affected\napplication, poten-\ntially leading to\nunauthorized access or\na privilege escalation.1 The vm2 library is a sandbox that can run\nuntrusted code securely. It\u2019s built on top of\nthe Node.js vm module and adds additional\nsecurity.\nuser said \u201cI have this class for generating user tokens... <code snippet>... What has better\nsecurity, my class or using SHA-256?\u201d\nOverall, through this analysis several key observations have emerged. The existence of\n102 instances with mentions of security within DevGPT dataset indicates a degree of empha-\nsis on security. Further, in 69 of these 102 conversations, ChatGPT volunteered the security\ninformation, without a direct request. 49 of these 69 instances were broad mentions e.g.,\n\u201cLastly, always keep the user\u2019s privacy and security in mind. \u201d, while 24 contained imple-\nmentation details tailored to the user\u2019s speci\ufb01c use-case. In addition, 14 of these 69 mentions\nled to further inquiries about security by the users. For instance, in one conversation, Chat-\nGPT reminded the user about the importance of using prepared statements in order to avoid\nSQL injection, at which point, the user asked ChatGPT to rewrite this code with prepared\nstatement.\nGiven our observations, this empirical exploration of real-world developer conversations\nwith ChatGPT highlights its potential to proactively inform developers about security vul-\nnerabilities and provide context-speci\ufb01c solutions for writing secure programs. Although\ninfrequent, such proactive contributions promote security awareness, helping developers\navoid insecure practices and prevent them from building upon vulnerable code. These \ufb01nd-\nings highlighted the importance of systematically studying this behavior. Consequently, we\n123101 Page 8 of 29 Empirical Software Engineering (2025) 30 :101\ndesigned our main study to evaluate the extent and consistency of this behavior across three\npopular LLMs, under controlled scenarios.\n3 Methodology\nBuilding upon our \ufb01ndings from the motivational study, we designed an experimental study\nto systematically evaluate the security awareness of three prominent LLMs, Claude 3, GPT-4,\nand Llama 3. Our goal is to determine whether LLMs can proactively detect vulnerable code in\nprompt inputs, and how consistently they issue security warnings and relevant vulnerability-\nrelated information to the users. To this end, we \ufb01rst collected Stack Over\ufb02ow (SO) questions\ncontaining vulnerable code snippets, which we then used as prompts for the LLMs. We\nqualitatively analyzed the LLM responses to address our research questions.\nBy examining whether an LLM issues a warning about the security of the code in the\nSO question, we evaluate the LLM\u2019s security awareness. When an LLM not only answers\nthe question but also highlights the security issue in the code, it demonstrates a high level\nof security awareness. Conversely, if the LLM addresses the question without mentioning\nsecurity, it demonstrates a low level of security awareness. This premise forms the basis for\nour assessment of LLMs\u2019 security awareness in answering developers\u2019 questions. Figure 2\nprovides an overview of our approach, which we discuss in detail next.\n3.1 Dataset Collection and Refactoring\nData Collection\nWe collect a dataset of SO questions with insecure code but without any explicit mention\nof security within the questions themselves. Figure 3shows an example of such a question,\nwhere the JavaScript code uses the eval function to dynamically create a variable based on user\ninput, introducing a signi\ufb01cant risk of code injection or a cross-site scripting (XSS) attack.\nThe developer posting the question seems unaware of the related security issue; however, one\nof the respondents points this out in a comment \u201cYou have a pretty nasty XSS vulnerability\nhere\u201d .\nHowever, not all SO questions containing vulnerable code receive responses that point\nout the security implications of the code. In fact, earlier studies highlight the concerns with\ndevelopers copy-pasting insecure code from SO posts (Fischer et al. 2017 ). Therefore, we aim\nto create two datasets for our study: (a) Mentions-Dataset: a set of questions that received\nresponses (either answers or comments) mentioning and\/or addressing the security concerns\nFig. 2 Overview of our study methodology\n123Empirical Software Engineering (2025) 30 :101 Page 9 of 29 101\nFig. 3 Example of a SO Question Containing Vulnerable Code (Highlighted Red), and a User Response\n(Comment), Pointing Out the Vulnerability\nin the question\u2019s code (as shown in Fig. 3), (b) Transformed-Dataset: a set of questions that\ndespite containing vulnerable code, do not receive such responses. The code in this dataset\nhas been transformed (Later detailed in this section) to ensure that it is not easily recognizable\nfor the LLMs.\nThe Mentions-Dataset represents a best-case scenario where the vulnerability has already\nbeen highlighted by the community, and the code appears exactly as it was originally posted\non SO. This allows us to observe how LLMs respond to questions that both exist in their\ntraining data and include explicit security warnings. In contrast, the Transformed-Dataset\nintentionally excludes any mention of security in the responses and applies code refactoring\nto reduce the chance of data leakage from the LLM\u2019s training data. By comparing these two\ndatasets, we can examine LLM behavior across a spectrum of possible real-world situations:\nfrom code that has been explicitly marked as vulnerable to code that has not been associated\nwith any vulnerabilities by the SO responses. This contrast helps us assess not only how\nwell LLMs leverage familiar content (Mentions-Dataset) but also their ability to generalize\nto insecure code they may not have associated with vulnerabilities before (Transformed-\nDataset).\nTo collect these datasets, we used the SO data dump (Stack Exchange 2023 ) from March\n2015 to March 2024, which contains over 10 million SO questions. First, we selected ques-\ntions that contain tags \u201cpython\u201d or \u201cjavascript\u201d to focus on these widely-used programming\nlanguages. According to the Stack Over\ufb02ow ( 2024 ) Developer Survey, both JavaScript and\nPython are listed in the top 3 most commonly used languages, making our results more\nrepresentative of common real-world scenarios. We excluded questions that did not have\n123101 Page 10 of 29 Empirical Software Engineering (2025) 30 :101\naccepted answers, resulting in a total of 4.89 million questions. To ensure that there were no\nmentions of security in the natural language section of the questions, we performed keyword\nsearches using the same keywords from Section 2. We discarded questions containing any of\nthose keywords, leaving us with 4.77 million questions. Next, we discarded the questions that\ndid not contain code snippets and analyzed the code in remaining questions using CodeQL\n(GitHub 2022 ), a static analysis tool often used to identify security vulnerabilities (Hamer\net al. 2024 ;S i d d i qe ta l . 2024b ; Pearce et al. 2022 ). As CodeQL does not require access to\nthe entire codebase, it effectively detected vulnerabilities directly within the code snippets.\nThis step resulted in the detection of 4935 questions with insecure code snippets.\nWe also made sure that each question in Mentions-Dataset has received at least one answer\nor comment containing a security-related keyword. Similarly, we ensured that Transformed-\nDataset only consists of questions that have no security-related keywords in their answers or\ncomments. At each step of the \ufb01ltering process, three authors manually checked the datasets\nto ensure their integrity. By inspecting all the instances in the subset of Mentions-Dataset,\nwe identi\ufb01ed 150 unique SO questions i.e., 75 Python and 75 JavaScript. Next, we selected\nthe same number of questions form the Transformed-Dataset, resulting in a total of 300\nquestions.\n\n[Text truncated due to context length limits]","filename":"Do_LLMs_consider_security__an_empirical_study_on_responses_to_programming_questions.pdf","responses":"[1] Machine Learning For Code: Applications of machine learning in software development and code-related tasks (including code generation, analysis, and tool support)."}
