{"text":"Document 1\nRepairBench: Leaderboard of Frontier Models for Program Repair Andr\u00e9 Silva and Martin Monperrus KTH Royal Institute of Technology, Sweden {andreans, monperrus}@kth.se https:\/\/repairbench.github.io\/ Abstract AI-driven program repair uses AI models to repair buggy software by producing patches. Rapid advancements in AI surely impact state-of-the-art performance of program repair. Yet, grasping this progress requires frequent and standardized evaluations. We propose RepairBench, a novel leaderboard for AI-driven program repair. The key characteristics of RepairBench are: 1) it is execution-based: all patches are compiled and executed against a test suite, 2) it assesses frontier models in a frequent and standardized way. RepairBench leverages two high-quality benchmarks, Defects4J and GitBug-Java, to evaluate frontier models against real- world program repair tasks. We publicly release the evaluation framework of RepairBench. We will update the leaderboard as new frontier models are released. 1 Introduction In recent years, AI-driven program repair [Zhang et al., 2023, 2024a] has emerged as a key application of AI in software engineering. Program repair is the task of automatically fixing software bugs, and AI-driven repair uses AI models to generate bug-fixing patches. Existing evaluation methodologies [Xu et al., 2022, Jiang et al., 2023] are inadequate for keeping pace with the rapid evolution of AI. They fail to capture the longitudinal perspective required to track the progress of AI-driven program repair over new generations of AI models. In this paper, we focus on frontier models, those state-of-the-art models that push the boundaries of AI capabilities. We propose RepairBench, a novel leaderboard aimed at a frequent, sound, and standardized evaluation of frontier models for program repair. RepairBench consistently evaluates frontier models on a high- quality set of program repair tasks. RepairBench employs carefully curated benchmarks: 1) Defects4J v2 [Just et al., 2014], a widely-adopted benchmark in the software engineering community, and 2) GitBug-Java [Silva et al., 2024], a benchmark of recent bugs from 2023, that has been designed to address benchmark leakage. A key design decision is that all bugs in RepairBench are real-world bugs coming from real-world programs. They also come with executable tests to verify the correctness of patches beyond syntactic match. RepairBench carefully selects evaluation metrics to ensure a meaningful comparison across different models: 1) AST Match@1, which captures syntactic correctness w.r.t. the reference patch written by the human developer, and 2) Plausible@1, which captures correctness based on the execution of all test cases. The latter is the default one used for ranking because it accounts for execution. To sum up, our contributions are: \u2022Leaderboard : We publish RepairBench as a leaderboard on the web at https:\/\/ repairbench.github.io\/ \u2022Data : We publicly share all prompts and patches at https:\/\/github.com\/ASSERT-KTH\/ repairbench \u2022Code : We open-source the code to produce the leaderboard at: https:\/\/github.com\/ ASSERT-KTH\/elle-elle-aimearXiv:2409.18952v1  [cs.SE]  27 Sep 2024 2 Methodology We devise the RepairBench methodology, a rigorous and standardized methodology to measure the performance of frontier models in program repair. This section outlines the key components of RepairBench, including the benchmarks, models, prompts, and the evaluation process. 2.1 Models RepairBench exclusively focuses on frontier models. Frontier models are models that, at the time of their release, stand out due to their performance across a wide-range of tasks when compared with the state-of-the-art. Their capabilities are demonstrated in general-purpose [Hendrycks et al., 2020, Liang et al., 2022, Chiang et al., 2024] and code-specific tasks [Jain et al., 2024]. In other words, frontier models lie at the border of what AI models are currently capable of doing. We select frontier models based on the following criteria: 1) they must demonstrate state-of-the-art capabilities (i.e., be frontier models) in other live evaluation systems (e.g., [Chiang et al., 2024, Jain et al., 2024], 2) they must be instruction-tuned, due to our prompt setup (see subsection 2.3), 3) they must be available through an API that is accessible to the RepairBench team, and 4) the estimated cost to evaluate each bug in RepairBench must not exceed a given price (see subsection 2.4). 2.2 Benchmarks RepairBench selects benchmarks per the following criteria: 1) being real-world programs (no toy programs, no competition programs), 2) being real-world bugs (no seeded or synthetic bugs), 3) including a variety of domains, 4) being executable, incl. at least one failing test case, 5) including a ground-truth patch written by a human developer, and 6) being well-engineered so that they can be integrated into the RepairBench framework with reasonable effort. RepairBench V1 includes the only two benchmarks that meet all those criteria: Defects4J v2 [Just et al., 2014], a widely-adopted benchmark in software engineering research, contains 835 real-world bugs from 17 open-source Java projects. We identify 484 single-function bugs which are utilized in RepairBench. GitBug-Java [Silva et al., 2024], is a benchmark of Java bugs from 2023, containing 199 real-world bugs from 55 open-source Java projects, from which we identify 90 single-function bugs utilized in RepairBench. In the next update of the leaderboard, we plan to introduce SWE-Bench [Jimenez et al., 2024]. 2.3 Prompts RepairBench employs the same prompt setup for all models, to ensure consistency. The prompt setup is zero-shot [Xia and Zhang, 2022], targets single-function bugs (i.e., bugs whose reference patch alters a single function), and is not iterative [Zhang et al., 2024b, Xia et al., 2024] (i.e., only a single call to the model is made). These choices are made for scoping reasons: RepairBench aims to provide a standardized evaluation of model capabilities, without accounting for additional approaches built on top of these models. RepairBench\u2019s prompt template includes: 1) the buggy function, 2) the failing test cases\u2019 code, and 3) the failing test cases\u2019 error message (runtime information). This set of ingredients captures the test-suite based program repair task [Parasaram et al., 2024]: the buggy function is the current program, and the failing test case\/error provides the difference between current and expected behavior as defined by the developers. All code snippets contain the original comments (e.g., inline comments, javadocs), and are surrounded by Markdown quotation marks. Finally, the model is prompted to return the repaired function inside quotation marks. Figure 1 shows an example prompt. The answers generated by the models are expected to contain the fixed version of the buggy function inside quotation marks. However, models are known to return additional natural language responses or explanations. To retrieve the generated code with reasonable leeway for such text, we extract the first code block generated by the model using regular expressions. 2 You are an automatic program repair tool . Your task is to fix the provided buggy code . The following code contains a buggy function : ```java \/** * Puts all values of this record into the given Map . * * @param map The Map to populate . * @return the given map. *\/ <M extends Map <String , String >> M putIn ( final M map) { for ( final Entry <String , Integer > entry : mapping . entrySet ()) { final int col = entry . getValue (). intValue (); map .put( entry . getKey () , values [col ]); } return map; } ``` The code fails the following tests . Test `org . apache . commons .csv . CSVRecordTest :: testToMapWithShortRecord `: ```java @Test public void testToMapWithShortRecord () throws Exception { final CSVParser parser = CSVParser . parse (\"a,b\", CSVFormat . DEFAULT . withHeader (\"A\", \"B\", \"C\")); final CSVRecord shortRec = parser . iterator (). next (); shortRec . toMap (); } ``` Test `org . apache . commons .csv . CSVRecordTest :: testToMapWithShortRecord ` error : ``` java . lang . ArrayIndexOutOfBoundsException : 2 ``` Please provide a fixed version of the buggy function , and only that function , inside a code block . Figure 1: Prompt for bug Csv-6 of Defects4J. The test case and runtime information guide frontier models in generating patches. 2.4 Costs Frontier models are typically expensive to evaluate due to both the energy cost to operate them and the provider\u2019s markup. RepairBench is, for the most part, supported by the RepairBench team, who pay model providers for the patch generation jobs and who execute patches in local infrastructure. To cap the amount of resources allocated to RepairBench, we define a maximum of 0.2 USD per evaluated bug, or approx. $115.1 for a total of 574 bugs. When a new frontier models is released, the RepairBench team estimates the cost to run RepairBench and proceeds only if the value is within the limit. The cost to generate patches is calculated according to the pricing of each organization, or the pricing of third-party model providers in case of open-weights models. 3 RepairBench is open to sponsorship from model providers, in which case the cost threshold is not considered. Cost is also important for program repair per se. Automated program repair fundamentally comptes with the costs of human developers. RepairBench provides a cost-aware [Hidv\u00e9gi et al., 2024] view of program repair, and the trade-off between repair cost and repair effectiveness. 2.5 Metrics The goal of program repair is to obtain a program that correctly fixes the bug without introducing any regression. Thus, evaluating models for program repair involves evaluating the multiple dimensions of the patches generated by the models. The patch should parse, compile, and type checks (depending on the target language). Correctness is evaluated by running the repaired code against a set of test cases to ensure that the original issue is resolved without introducing new errors. This is why we select benchmarks with reasonably good test suites. RepairBench evaluates and ranks models according to two metrics. Both of them are meant to be maximized: the higher the metric, the stronger the model. Plausible@1 : the probability that the first generated patch passes all test cases. By running all test cases, we check if the original bug is resolved without new bugs being introduced. Note that this metric does not guarantee that the patch is functionally equivalent to the reference implementation since test suites typically do not cover the entire specification and input domains. To compute thepass@k metrics, we rely on Chen et al. [Chen et al., 2021]\u2019s numerically stable and unbiased estimator, generating 10 non-deterministically sampled patches per bug with the provider\u2019s default settings and a temperature of 1.0. AST Match@1 : the probability that the first generated patch has the same abstract syntax tree (AST) as the reference patch provided by the benchmark. Unlike Plausible @1 ,AST-Match @1 is static and does not rely on the test suite. This metric is a strong indicator of correctness: if the ASTs are the same, it means that the model was able to produce the exact same patch as the human developer. Note that the pass@k metrics are more reliable than simply computing the total number of correctly fixed bugs: 1) generating patches is not deterministic, even when using deterministic sampling algorithms [Ouyang et al., 2023], 2) models are usually deployed with non-deterministic sampling algorithms in practice. pass@k accounts for the non-determinism by representing the probability of generating a correct patch given a budget of kgenerations. 3 Results This section contains the RepairBench results, and is structured to be updated over time with new frontier models. We plan to update the benchmarks for at least 3 years. Table 1 shows the leaderboard status as of September 30, 2024. Organization ModelDefects4J v2 (484 bugs) GitBug-Java (90 bugs) Total (574 bugs)Ref. Plausible@1 AST Match@1 Cost ($) Plausible@1 AST Match@1 Cost ($) Plausible@11AST Match@1 Cost ($) Anthropic claude-3-5-sonnet-20240620 41.5% 12.3% $ 57.91 26 .1% 9.0% $ 30.20 39.1% 11.7% $ 88.11 [Anthropic, 2024] OpenAI gpt-4o-2024-08-06 34.1% 8.4% $ 20.74 18 .8% 8.1% $ 9.77 31 .7% 8.3% $ 30.51 [OpenAI, 2024a] Google gemini-1.5-pro-001 30.3% 13.0% $44.95 16 .7% 9.6% $ 33.70 28 .2% 12.5% $78.65 [Reid et al., 2024] Meta llama-3.1-405b-instruct 28.9% 7.7% $ 17.42 16 .7% 7.3% $ 11.86 27 .0% 7.6% $ 29.28 [Dubey et al., 2024] DeepSeek deepseek-v2.5 26.6% 6.4% $ 14.17 17 .6% 7.3% $ 5.55 25 .1% 6.5% $ 19.73 [Liu et al., 2024] Alibaba Cloud qwen-2.5-72b-instruct 25.5% 6.7% $ 2.46 17 .3% 5.9% $ 2.28 24 .2% 6.6% $ 4.74 [Team, 2024] Mistral mistral-large-2407 24.5% 6.6% $ 27.17 15 .2% 6.6% $ 20.53 23 .0% 6.6% $ 47.70 [Mistral, 2024] OpenAI2o1-preview-2024-09-122\u2014 \u2014 \u2014 32.3% 12.1% $325 .71 \u2014 \u2014 \u2014 [OpenAI, 2024b] 1Models are sorted by the total Plausible@1 score. 2Only partial results available right now due to cost reasons. Table 1: Leaderboard of Frontier Models for Program Repair as of September 30, 2024 The leaderboard highlights a clear dominance of Anthropic\u2019s claude-3-5-sonnet-20240620 , which achieves the highest overall Plausible@1 score ( 39.1%). This means that this model captures the most of the expected behavior specified in one shot prompt, coupled together with perfect mastering of the syntax of the programming language. 4 OpenAI\u2019s gpt-4o-2024-08-06 and Google\u2019s gemini-1.5-pro-001 achieve the second and third best scores, respectively. gemini-1.5-pro-001 is the best model according to AST Match@1 ( 12.5%). OpenAI\u2019s o1-preview-2024-09-12 results are currently incomplete due to its high cost. Yet, we note that it achieves the best score on GitBug-Java with a 32.3%Plausible@1 score (as opposed to 26.1% forclaude-3-5-sonnet-20240620 ). 20 40 60 80 T otal Cost ($)0.240.260.280.300.320.340.360.38T otal Plausible@1 gemini-1.5-pro-001 (2024-05-24)gpt-4o-2024-08-06 (2024-08-06) llama-3.1-405b-instruct (2024-07-23) deepseek-v2.5 (2024-09-05) mistral-large-2407 (2024-07-24)qwen-2.5-72b-instruct (2024-09-19)claude-3-5-sonnet-20240620 (2024-06-20)Performance (Plausible@1) vs Cost Best Linear Fit Figure 2: Performance (Plausible@1) in function of the total cost (in USD). The most performant models are also the most expensive ones. Figure 2 plots the performance (Plausible@1) as function of the total cost to run RepairBench on each model. The most expensive frontier models are also the most performant. Anthropic\u2019s claude-3-5-sonnet-20240620 costs a total of $88.11 for a score of 39.1%. Alibaba Cloud\u2019s qwen-2.5- 72b-instruct model, the cheapest model (4.74$, approx. 20x less than claude-3-5-sonnet-20240620 ) in RepairBench, achieves a score (24.2%) comparable with DeepSeek\u2019s deepseek-v2.5 (25.1%) and better than Mistral\u2019s mistral-large-2407 (23.0%). 4 Discussion A critical concern when evaluating AI models is the issue of benchmark leakage [Dong et al., 2024, Matton et al., 2024]. Benchmark leakage occurs when models are exposed to test data during pre- training or post-training. Benchmark leakage, aka contamination, can lead to inflated performance results, giving a misleading picture of the actual ability to generalize and solve novel problems. This concern is particularly problematic for frontier models due to their training on huge amounts of data. We believe there is some benchmark leakage for Defects4j, but the actual extent is unknown. However, the low overall performance (approx. 30-40%) shows that the benchmark is not at all perfectly memorized. In RepairBench, we do mitigate benchmark leakage with the inclusion of GitBug-Java [Silva et al., 2024], a newly constructed benchmark with only recent bugs, from 2023 onwards. Moreover, RepairBench prioritizes an execution-based metric (Plausible@1) over purely static evaluations (AST Match@1): this helps assess model capabilities beyond superficial memorization of code. 5 Related Work 5.1 General-Purpose Benchmarks The evaluation of AI models typically relies on general-purpose benchmarks that assess performance across diverse domains. Among these, MMLU [Hendrycks et al., 2020] stands out as a comprehen- sive benchmark, encompassing problems from a wide array of academic disciplines. HellaSwag 5 [Zellers et al., 2019] focuses on testing models\u2019 commonsense reasoning, while benchmarks like GSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] are designed to evaluate models\u2019 mathematical problem-solving capabilities. In parallel, several live evaluation platforms have emerged to continuously measure model perfor- mance. HEML [Liang et al., 2022] aims to provide a comprehensive evaluation of models across a range of tasks. Other platforms, such as Vellum1, Open LLM Leaderboard2, and KLU.ai\u2019s leader- board3also provide live updates of model performances. Notably, ChatBotArena [Chiang et al., 2024] maintains real-time leaderboards based on battles between models and a crowd-sourced evaluation methodology. While these benchmarks cover a broad range of capabilities and reasoning tasks, none of them address the specificity of the program repair task. 5.2 Code Benchmarks Code-related tasks, such as code generation and repair, require specialized benchmarks. Being code, execution is a unique characteristic of the output and we claim that execution-based code benchmarks is crucial [Khan et al., 2024]. One of the most widely-used execution-based benchmarks in this area is HumanEval [Chen et al., 2021], which evaluates the ability of models to generate Python code for simple algorithmic problems. Although HumanEval has been an important tool for measuring the effectiveness of models in code generation, it is now exhausted, as frontier models achieve near-perfect scores. Also, it is not a program repair task. Program repair benchmarks [Le Goues et al., 2015] provide a suitable testing ground for AI-driven program repair. Several program repair benchmarks have been proposed across languages and domains [Csuvik and Vid\u00e1cs, 2022], [Gyimesi et al., 2019]. Some program repair benchmarks are exclusively static, without test cases available for execution [Avula et al., 2023]. RepairBench only focuses on executable benchmarks. Other benchmarks, despite including test cases, are not fully reproducible due to missing third-party dependencies and other low level problems [Madeiral et al., 2019, Saha et al., 2018]. RepairBench only focuses on reproducible benchmarks [Zhu and Rubio-Gonz\u00e1lez, 2023]. 5.3 Code Leaderboards Beyond sporadic evaluations, live evaluation platforms for code have been proposed. Aider\u2019s leaderboard4evaluates LLMs on their capability to write code according to a given instruction. In contrast, RepairBench focuses exclusively on program repair, which involves fixing real-world bugs in existing codebases. LiveCodeBench [Jain et al., 2024] offers a continuous evaluation of LLMs on a variety of code-related tasks, including self-repair [Fan et al., 2023], where the model is assessed based on its ability to fix code it has previously generated. While LiveCodeBench focuses on artificial tasks extracted from code competitions, RepairBench only evaluates models with real-world repair tasks that human developers have encountered during software development. Finally, Shariffdeen et al. [Shariffdeen et al., 2023] held a competition of program repair approaches, but do not include frontier models. 6 Conclusion RepairBench introduces a standardized, execution-based evaluation framework for assessing frontier models in AI-driven program repair. RepairBench relies on real-world bug benchmarks and focuses on execution for evaluating patches. As new frontier models will be released, RepairBench\u2019s leaderboard will provide insights into the longitudinal evolution of AI-driven program repair. 1https:\/\/www.vellum.ai\/llm-leaderboard 2https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard 3https:\/\/klu.ai\/llm-leaderboard 4https:\/\/aider.chat\/docs\/leaderboards\/ 6 7 Acknowledgments This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations\/data handling were enabled by the supercomputing resource Berzelius-2023-175 provided by National Supercomputer Centre at Link\u00f6ping University and the Knut and Alice Wallenberg foundation. References Anthropic. Claude 3.5 sonnet, June 2024. URL https:\/\/www.anthropic.com\/news\/ claude-3-5-sonnet . Sai Krishna Avula, Venkatesh V obbilisetti, and Shouvick Mondal. Minecraft: Automated mining of software bug fixes with precise code context. In Proceedings of the 38th IEEE\/ACM International Conference on Automated Software Engineering , 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021. Viktor Csuvik and L\u00e1szl\u00f3 Vid\u00e1cs. Fixjs: a dataset of bug-fixing javascript commits. In Proceedings of the 19th International Conference on Mining Software Repositories , pages 712\u2013716, 2022. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, and Ge Li. Generalization or memorization: Data con- tamination and trustworthy evaluation for large language models. arXiv preprint arXiv:2402.15938 , 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024. Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. Automated repair of programs from large language models. In 2023 IEEE\/ACM 45th International Conference on Software Engineering (ICSE) , pages 1469\u20131481. IEEE, 2023. P\u00e9ter Gyimesi, B\u00e9la Vancsics, Andrea Stocco, Davood Mazinanian, Arp\u00e1d Besz\u00e9des, Rudolf Ferenc, and Ali Mesbah. Bugsjs: a benchmark of javascript bugs. In 2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST) , pages 90\u2013101. IEEE, 2019. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations , 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021. D\u00e1vid Hidv\u00e9gi, Khashayar Etemadi, Sofia Bobadilla, and Martin Monperrus. Cigar: Cost-efficient program repair with llms. arXiv preprint arXiv:2402.06598 , 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024. 7 Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. Impact of code language models on automated program repair. In 2023 IEEE\/ACM 45th International Conference on Software Engineering (ICSE) , pages 1430\u20131442. IEEE, 2023. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations , 2024. Ren\u00e9 Just, Darioush Jalali, and Michael D Ernst. Defects4j: A database of existing faults to enable controlled testing studies for java programs. In Proceedings of the 2014 international symposium on software testing and analysis , pages 437\u2013440, 2014. Mohammad Abdullah Matin Khan, M Saiful Bari, Do Long, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. Xcodeeval: An execution-based large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6766\u20136805, 2024. Claire Le Goues, Neal Holtschulte, Edward K Smith, Yuriy Brun, Premkumar Devanbu, Stephanie Forrest, and Westley Weimer. The manybugs and introclass benchmarks for automated repair of c programs. IEEE Transactions on Software Engineering , 41(12):1236\u20131256, 2015. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 , 2022. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and efficient mixture-of- experts language model. arXiv preprint arXiv:2405.04434 , 2024. Fernanda Madeiral, Simon Urli, Marcelo Maia, and Martin Monperrus. Bears: An extensible java bug benchmark for automatic program repair studies. In 2019 IEEE 26th international conference on software analysis, evolution and reengineering (SANER) , pages 468\u2013478. IEEE, 2019. Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime V oisin, Ellen Gilsenan-McMahon, and Matthias Gall\u00e9. On leakage of code generation evaluation datasets. arXiv preprint arXiv:2407.07565 , 2024. Mistral. Large enough, July 2024. URL https:\/\/mistral.ai\/news\/mistral-large-2407\/ . OpenAI. Hello gpt-4o, May 2024a. URL https:\/\/openai.com\/index\/hello-gpt-4o\/ . OpenAI. Openai o1 system card, September 2024b. URL https:\/\/openai.com\/index\/ openai-o1-system-card\/ . Shuyin Ouyang, Jie M Zhang, Mark Harman, and Meng Wang. Llm is like a box of chocolates: the non-determinism of chatgpt in code generation. arXiv preprint arXiv:2308.02828 , 2023. Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, and Sergey Mechtaev. The fact selection problem in llm-based program repair. arXiv preprint arXiv:2404.05520 , 2024. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024. Ripon K Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, and Mukul R Prasad. Bugs. jar: A large-scale, diverse dataset of real-world java bugs. In Proceedings of the 15th international conference on mining software repositories , pages 10\u201313, 2018. Ridwan Shariffdeen, Martin Mirchev, and Abhik Roychoudhury. Program repair competition. In 2023 IEEE\/ACM International Workshop on Automated Program Repair (APR) , pages 19\u201320. IEEE, 2023. 8 Andr\u00e9 Silva, Nuno Saavedra, and Martin Monperrus. Gitbug-java: A reproducible benchmark of recent java bugs. In 2024 IEEE\/ACM 21st International Conference on Mining Software Repositories (MSR) , pages 118\u2013122. IEEE, 2024. Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL https:\/\/qwenlm. github.io\/blog\/qwen2.5\/ . Chunqiu Steven Xia and Lingming Zhang. Less training, more repairing please: revisiting automated program repair via zero-shot learning. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering , pages 959\u2013971, 2022. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489 , 2024. Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming , pages 1\u201310, 2022. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4791\u20134800, 2019. Quanjun Zhang, Chunrong Fang, Yuxiang Ma, Weisong Sun, and Zhenyu Chen. A survey of learning- based automated program repair. ACM Transactions on Software Engineering and Methodology , 33(2):1\u201369, 2023. Quanjun Zhang, Chunrong Fang, Yang Xie, YuXiang Ma, Weisong Sun, and Yun Yang Zhenyu Chen. A systematic literature review on large language models for automated program repair. arXiv preprint arXiv:2405.01466 , 2024a. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis , pages 1592\u20131604, 2024b. Hao-Nan Zhu and Cindy Rubio-Gonz\u00e1lez. On the reproducibility of software defect datasets. ICSE. IEEE , 2023. 9\nDocument 2\nImproving Examples in Web API Specifications using Iterated-Calls In-Context Learning Kush Jain Carnegie Mellon University United States kdjain@andrew.cmu.eduKiran Kate IBM Research United States kakate@us.ibm.comJason Tsay IBM Research United States Jason.Tsay@ibm.com Claire Le Goues Carnegie Mellon University United States clegoues@cs.cmu.eduMartin Hirzel IBM Research United States hirzel@us.ibm.com Abstract \u2014Examples in web API specifications can be essential for API testing, API understanding, and even building chat-bots for APIs. Unfortunately, most API specifications lack human- written examples. This paper introduces a novel technique for generating examples for web API specifications. We start from in-context learning (I CL): given an API parameter, use a prompt context containing a few examples from other similar API parameters to call a model to generate new examples. However, while I CLtends to generate correct examples, those lack diversity, which is also important for most downstream tasks. Therefore, we extend the technique to iterated-calls I CL(ICICL): use a few different prompt contexts, each containing a few examples, to iteratively call the model with each context. Our intrinsic evaluation demonstrates that I CICLimproves both correctness and diversity of generated examples. More importantly, our extrinsic evaluation demonstrates that those generated examples significantly improve the performance of downstream tasks of testing, understanding, and chat-bots for APIs. I. I NTRODUCTION Web Application Programming Interfaces (APIs) enable systems to communicate across a network [1], [2]. REpresen- tational State Transfer (REST) APIs have become the de facto standard for modern web applications [3]. This style enables clients and services to exchange information over HTTP. Large companies like Google, Amazon, and Apple expose services through REST APIs, including large enterprise services like Google Drive and Apple Authentication, as well as simpler services like REST Countries,1for querying information about a country. REST APIs are commonly described using OpenAPI spec- ifications [4]: one survey of communication service providers found that 73% of companies and 75% of suppliers use OpenAPI to describe their APIs.2OpenAPI specifications formalize the contract between API developer and API user, describing the structure of API requests and responses. Tools 1https:\/\/restcountries.com 2https:\/\/inform.tmforum.org\/features-and-opinion\/ the-status-of-open-api-adoption\/such as Redoc3and SwaggerUI4can automatically convert OpenAPI specifications into human-readable webpages, allow- ing developers to better understand these APIs. Additionally, specifications are commonly used in input validation [5], [6] and testing [7], [8]. Common downstream clients of OpenAPI specifications leverage realistic examples of OpenAPI parameters (when they exist) as a part of their workflow. Fuzzers [9], [10] use examples to guide API testing, producing fewer invalid requests and covering deeper code paths. Chat-bots [11], [12], [13] first build an underlying model of a system and then derive API calls from the natural language utterance. Recently- developed large language models (LLMs), like ChatGPT [14] and GPT-4 [15], benefit from using API parameter examples, and other LLMs use them for fine-tuning, as evaluated on dialog benchmarks [16]. API parameter examples can also improve human understanding, especially for novice users [17], [18]. However, despite their widespread adoption, most OpenAPI specifications lack API parameter examples (only 1,953 out of a dataset of 13,346 mined OpenAPI parameters have any examples). There has been some research on generating examples for OpenAPI specifications. Prior work follows two approaches: (i) extracting examples from API descriptions [10] or (ii) mining examples from knowledge bases [9], [19]. The goal of both approaches is to generate diverse and correct examples. Example correctness is important, as these examples serve as input to software testing and dialog systems. Con- versely, example diversity is also important, as examples that differ from one another help testing increase its coverage and help chat-bots generalize their natural-language understanding. Both approaches to example generation are limited: mining examples only works for examples present in knowledge bases, while extracting examples from descriptions only works when the description explicitly enumerates parameter examples. 3https:\/\/github.com\/Redocly\/redoc 4https:\/\/github.com\/swagger-api\/swagger-uiarXiv:2504.07250v1  [cs.SE]  9 Apr 2025 We present I CICL, which combines retrieval-based prompt- ing [20] with iterated calls to in-context learning (I CL) to generate diverse and correct API parameter examples. I CICL leverages the ability of LLMs to generate realistic examples based on their pretraining. Unlike knowledge bases, LLMs are pretrained on large swaths of the internet, and thus have a strong prior of the world around them. We take as input the OpenAPI specification without examples and generate exam- ples for all API parameters, regardless of whether examples exist on the internet or the descriptions specify example values. For correctness, we use greedy decoding (taking the highest probability token at each step) to generate one (likely) correct example. We perform postprocessing to only keep examples that are similar to our (likely) correct example. For diversity, we both increase temperature, and, unlike vanilla I CL, use iterated calls with multiple prompt contexts. One can increase temperature (smoothing the distribution of next token proba- bilities) to generate different model outputs. Additionally, we observe that the problem of example diversity is similar to the challenge of generating different model outputs, which is solved by ensembles [21] of different models. This observation leads us to use multiple prompt contexts, where each context consists of a different set of few-shot examples. We evaluate I CICL, finding that it generates diverse, cor- rectly typed examples. We further manually annotate a sample of 385 parameters and show that 75% of the generated examples are correct. We then demonstrate the usefulness of the generated examples in three downstream settings: fuzzing, dialogue benchmarks, and human API understanding, which we assess via an exploratory developer pilot. Our examples significantly improve performance in these tasks, improving branch coverage by 116%, dialog intent recognition by 3%, and dialog slot filling by 5%, compared to the original speci- fications. To summarize, our core contributions are as follows: \u2022We identify adding examples as a single improvement to API specifications that benefits several downstream use cases (understanding, fuzzing, chat-bots). \u2022Inspired by how ensembles use multiple models to im- prove results, we introduce I CICL, a new technique for using LLMs to generate API examples. We combine retrieval-based prompting, multiple prompt contexts, and post-processing to produce diverse yet correct examples. \u2022We include an extensive experimental evaluation that quantifies the value of the generated examples for several use-cases. These include fuzz testing, chat-bots, and an exploratory study of developers\u2019 API understanding. Our prompting, intrinsic, fuzzing, and exploratory study evaluation and code are at https:\/\/figshare.com\/s\/ 8eec881ddf8e6573f43f, including detailed reproduction instructions. We elide calls to internal company services in the prompting code, but release all other code. We are unfortunately unable to release our API parameter bank, intrinsic evaluation dataset, and SeqATIS dataset, as they are internal to the large technology company at which thisListing 1: Illustrative OpenAPI parameter from the Rest Countries API. Prior approaches struggle to generate correct examples for this API parameter; knowledge bases contain many false positives, and the description contains no examples. name: currency description: Search by ISO 4217 currency code in: path required: true schema: type: string work was conducted, but hope that the other elements of the artifact are informative for subsequent research. II. M OTIVATING EXAMPLE AND OVERVIEW Listing 1 shows an illustrative parameter for the \/currency endpoint5of the REST Countries API. As with most OpenAPI parameters, this specification contains its name, a short description, and a type. However, it does not contain any example values, nor can example values easily be ex- tracted from the description or name. To try the \/currency endpoint, a developer would either need domain knowledge of ISO 4217 currency codes or would need to search for an example. Fuzzers also fail to cover deeper code paths for this endpoint, as they would start from a random sequence of bits and would only arrive at a valid ISO 4217 currency code by chance. Two common approaches to generating example values, namely mining them from a knowledge base such as DBPedia or extracting examples from the description, would also fail here. While ISO 4217 is an entity in DBPedia (the knowledge base used by the state-of-the-art example generation tool, ARTE [9]), there are numerous other currency codes that are not ISO 4217, meaning that generated examples are semantically incorrect. ARTE [9] circumvents this by calling the API with examples to see if they are valid; however, this limits applicability to cases like fuzzing, which can send a large volume of requests to the API. The description also does not enumerate examples of currency codes that could be extracted. Figure 1 gives an overview of I CICL. It first retrieves parameters from the API parameter bank that are similar to the parameter from the original API specification (step 1). Then it creates a prompt context by greedily selecting the top-most similar retrieved parameters for in-context learn- ing (step 2). Following this, it uses the LLM with greedy decoding to obtain the greedy example, which has the highest confidence (step 3). It then creates multiple diverse prompt contexts, each of which includes the greedy example plus some retrieved parameters for in-context learning, selected to be similar but with some randomization (step 4), and uses iterated calls to the LLM with a higher temperature to obtain multiple diverse examples, one from each of the diverse prompt contexts (step 5). Then it creates a list 5https:\/\/restcountries.com\/v2\/currency Greedy context currencyCode: EUR order_crncyCode: USD Improved API spec. name: currency description: ISO 4217 type: string examples: USD,CAD,EUR \u2026Greedy example USDOriginal API spec. name: currency description: ISO 4217 type: string (no examples) \u2026Retrieved parameters 1. currencyCode: EUR 2. order_crncyCode: USD 3. local_tender: INR \u2026API chat-bot API understanding API fuzz-testingDiverse contexts \u2022currency: USD currencyCode: EUR \u2022currency: USD local_tender: INRDiverse examples USD,GBP,USD, CAD,ZAR,CAD, INR,MXN,CNY, EUR inputs output intermediate results clientsParameter bank id: abc, def currencyCode: EUR state: CA, CO, NY \u202612 3 4 56Large language modelFig. 1: Overview and running example of our approach. Circled numbers correspond to different steps in our approach. of filtered examples that include the greedy example and some of the diverse examples, which it adds to the API specification (step 6). Our approach performs well on the snippet in Listing 1, generating USD,CAD, and EUR, all valid currency examples. III. I CICL ICICLtakes an API specification without parameter exam- ples as input and returns an improved specification with those examples as output. Figure 1 outlines the approach. Offline, we create a parameter bank by mining parameters and examples, and pick an off-the-shelf LLM (Section III-A). Online, given a parameter in an OpenAPI specification, we retrieve relevant parameters from the parameter bank (Section III-B), build prompt contexts (Section III-C), and finally postprocess model output (Section III-D). A. Offline: Mining Examples, Model Selection We mine 1,236 OpenAPI specifications from API Guru [22] and Wittern et. al. [23]. This collection has OpenAPI specifica- tions for popular enterprise applications such as Box, Google Drive, YouTube, and others. We parse the mined specifications to extract each API parameter and corresponding examples. Of 13,346 parameters, 1,953 have examples. We use these mined examples as our parameter bank (shown among the inputs on the left of Figure 1). We use Falcon,6a 40B parameter model trained on one trillion tokens from the internet, for the LLM (middle of Figure 1). Falcon outperforms LLAMA, GPT-3, and MPT on the OpenLLM leaderboard. Falcon has also been extensively pretrained on code, which we hypothesize will help with type correctness. Using a large but not huge open-source model such as Falcon is representative of commercial settings that must balance cost and data exposure regulatory concerns. We model the task of example-generation as an instance of few-shot prompting, varying the prompt context to generate different examples. 6https:\/\/huggingface.co\/tiiuae\/falcon-40bB. Retrieving Relevant Parameters Given an API parameter, we first seek a set of relevant similar parameters from the parameter bank (Figure 1: 1). We first extract the initial 50 characters from the API parameter description (our dataset has a median description length of 54 characters, with the first 50 characters concisely representing a parameter\u2019s purpose or function). At times, the full description is excessively verbose, with all other parameter information outside the description having a median length of 63 char- acters, thus truncating at 50 characters ensures that we do not overwhelm other important information. We append the exact name of the API parameter to the description, ensuring that the name of the API parameter is factored into any similarity computation. Lastly, we append the operation ID, which offers additional context about the operation associated with the parameter. For example, the parameter \u2018 name \u2019 has different meanings if the operation ID is \u2018 getCountries \u2019 or \u2018getUserByUsername \u2019. We use the concatenated string of the parameter description, parameter name, and operation ID as the query for retrieval. We use BM25 [24] as the retrieval method, due to its high speed and accuracy [25]. BM25 calculates a weight of terms based on their frequency in both the query and the target documents. It then considers the term\u2019s prevalence across the entire parameter bank (intuitively infrequent terms discriminate better). When BM25 processes a parameter (such as \u2018currency\u2019 in the running example), it returns a similarity score for each API parameter in the parameter bank. These scores measure how closely each parameter in the parameter bank matches the parameter we are generating examples for. We leverage this distribution of similarity scores to craft prompt contexts (sets of few-shot examples for in-context learning). C. Prompt Context Generation Prompt context generation consists of two phases: eliciting the greedy example, and then constructing ten prompt con- texts (of five shots each) to elicit diverse examples. We use a two-phase approach to improve both correctness and diversity of the generated examples. Listing 2: LLM prompt for currency code. We provide the parameter that is missing examples and five few-shot examples. # Given an OpenAPI parameter, generate a unique example of the parameter. input_0 = { \"param_name\": \"currencyCode\", \"type\": \"string\", \"operation_id\": \"contractInfo\", \"description\": \"The currency code (ISO 4217)\", \"api_name\": \"beezup\" } # must generate a unique currencyCode string example_0 = \"EUR\" ... input_6 = { \"param_name\": \"currency\", \"type\": \"string\", \"operation_id\": \"v2Currency\", \"description\": \"Search by ISO 4217 currency code \", \"api_name\": \"rest-countries\" } # must generate a unique currency string example_6 = The first phase prompts the LLM with the top five re- trieved parameters with the highest similarity to the query (Figure 1: 2) as returned by retrieval (Section III-B). Greedy decoding in an LLM simply picks the most likely token at each generation step, thus deterministically yielding the sequence of most-probably tokens. By leveraging greedy decoding, this step aims to produce a (likely) correct example 3. Our model yields USD as the greedy example (a correct currency code). By providing the greedy example in all prompt contexts, we ensure that the LLM, even at a higher temperature setting, generates examples that align with the original example. The second phase improves example diversity by sampling from the distribution of similarity scores to generate 10 prompt contexts of five examples each 4. We take inspiration from ensembles [21], where multiple models produce different outputs that improve both the correctness and diversity of the resulting system. Our prompt contexts, each of which consists of a different set of API parameter examples (i.e., \u201cshots\u201d), are similar to the diverse models used in ensembles. We iter- atively call the LLM with each prompt context with a higher temperature of 0.5 to generate 10 example candidates 5. The order of these calls does not matter; they can be parallelized or batched. In the running example, the calls return USD,GPP, USD,CAD,ZAR,CAD,INR,MXN,CNY, and EUR. D. Postprocessing We perform postprocessing to narrow these 10 example candidates down to 3 examples to add to the improved API specification (Figure 1: 6). First, we filter out all examples that do not match in type to the API parameter we are generating examples for. This is the earliest opportunity for this filter, and we do it right away given the importance of type compatibility. We then add the greedy example to the 10 example candidates and perform deduplication. We alwaysinclude the greedy example in our set of three generated examples, as it is likely to be correct. Following this, we add all examples that the model generates multiple times to our set of three, and return this set if it contains at least three examples. For example, if the model generates the currency CAD twice, then we add it to the final set of three examples. If, at this point, there are fewer than three examples, we use BERT [26] to encode each example and the greedy example. We then select the most similar examples until we have three examples (illustrated by adding EUR in Figure 1). This ensures that the generated examples are similar in format and content to the greedy example, improving their likelihood of being correct. We choose to favor correct- ness over diversity here, given its importance to downstream tasks (testing and chat-bots). Using BERT embeddings ensures that we are comparing the semantic similarity of each example to the greedy example, rather than doing a simple text- based match (which, in the case of currency codes, is less meaningful). IV. I NTRINSIC EVALUATION While ultimately extrinsic evaluations (Section V) matter most for downstream clients, they are laborious to measure, so we used intrinsic evaluations for nimble iterative modeling. We evaluate examples generated by I CICLon intrinsic correctness and diversity. Specifically, we measure whether examples generated by I CICLare type correct, unique, and semantically diverse. We also hand-evaluated a smaller subset of examples for semantic correctness. We compare different components of ICICLacross these metrics. A. Experimental Setup 1) Dataset: We evaluate modeling approaches on a ran- domly sampled dataset of 1,000 OpenAPI parameters mined from mainstream services including but not limited to Box, Google Drive, and Gmail. We remove all parameters in the parameter bank from our set of API parameters prior to sampling. We also remove all Boolean and enum parame- ters (approximately 1,000 from the initial mined set) from our evaluation set, as predicting the values of these parameters is trivial. Due to computational cost, we do not run our intrinsic evaluation on the full final set of 13,346 parameters, instead focusing on a likely-representative random sample of 1,000 examples (approximately 1\/13) of the dataset. This sampling is in line with prior work [27], [28], which sample a similar proportion of the dataset for evaluation. These include 668 string, 129 array, 106 integer, 34 number, 14 object, and 5 datetime types. The remaining 44 parameters come from a variety of other types including color, tuples, and None types. 2) Approach: We evaluate the efficacy of each component of our approach (adding retrieval, sampling from the distri- bution of similarity scores, and applying our postprocessing). This is equivalent to an ablation study: the final setting is the full approach, earlier settings remove components. We prompt the model as described in each settings and evaluate the generated examples using the metrics described below. Static: Static refers to a static prompt of five parameter and example pairings for in-context learning. We also include the greedy example as part of the prompt and use a temperature of 0.5. Temperature corresponds to the level of randomness in text generation - temperature of 0 refers to sampling the most likely tokens, while higher temperature refers to sampling more diversely. We prompt the LLM 10 times to generate 10 examples and perform deduplication. Finally, we randomly select three examples to return to the user. Retrieval: Retrieval refers to the greedy retrieval approach. Rather than sampling 10 prompt contexts from the distribution of similarity scores, we only use a single prompt context containing the five most similar parameters for prompting. In other words, this setting performs in-context learning with retrieval, but no iterated calls. Retrieval (w\/contexts) : Our retrieval with context approach adds iterated calls with context sampling. Rather than selecting the five most similar examples for all 10 prompts, we build prompt contexts by randomly sampling from the distribution of similarity scores (similar parameters are more likely to be chosen than different parameters). Retrieval (w\/postprocessing) : This is our final approach used in extrinsic evaluations (fuzzing, dialog, and exploratory usability study). We apply our postprocessing that filters out type-incorrect examples and selects examples that are similar to the greedy example. This helps ensure that our examples are correct, both in type and in semantic meaning (close to a generated example likely to be correct). 3) Metrics: We define the following set of metrics to benchmark various prompting approaches. The main factors we consider are example correctness and example diversity. Type Correctness: Type correctness adheres to the strict definition of all generations from the LLM being the same type as the parameter. We use this strict definition to ensure all generations conform to the same example type. Recall that our intrinsic evaluation focuses on open-ended types (strings, numbers, arrays, objects, etc.) but not Boolean or enums (as generating values for types with small closed sets is trivial). Uniqueness: Uniqueness refers to the ability of the LLM to generate three case-insensitive unique examples from 10 generations. Higher uniqueness values indicate more diverse LLM generated examples. For example, if all 10 generations are the same example, the uniqueness would be 0, otherwise if there are three unique examples it would be 1. Diversity: Diversity is 1 minus mean cosine similarity be- tween the BERT [26] embeddings of examples. We choose to use BERT embeddings over TF-IDF or BM25 embeddings, as BERT embeddings detect semantic similarity, while other approaches only detect overlap of tokens (syntactic similarity). Example Correctness: Example correctness refers to gener- ated examples matching the specification. We define correct- ness as examples that both satisfy preconditions specified in the natural language description of the parameter and have consistent format between all generated examples. Correct examples can be used in an API call to the API under test without 4xx or input validation errors. Unlike the othermetrics, which are fully automated, this metric requires human effort. We manually annotate a randomly sampled subset of 385 out of our 1,000 sampled examples across all four settings, for 95% confidence in the correctness results. B. Intrinsic Evaluation Results TABLE I: Intrinsic evaluation metrics on 1,000 (columns Type, Unique, Both (type correct and unique), Div) and 385 (column Correct) randomly sampled examples. Each approach component improves type correctness, the proportion of unique examples, and overall correctness. Setting Type Unique Both Div Correct static 97% 48% 47% 0.22 70.4% retrieval 98% 55% 55% 0.20 73.2% w\/contexts 98% 66% 65% 0.23 65.7% w\/postprocessing 99% 67% 67% 0.19 74.3% Table I shows the results from running various components of I CICLon a selected OpenAPI parameters. We show how each component improves on the baseline. We find that type correctness of generated examples is relatively strong across all approaches (varying from 97% to 99%). We hypothesize this is due to LLMs\u2019 extensive training on code, where type is important in generating the next token. However, we do notice that type correctness does increase as we add retrieval-based prompting and our postprocessing, which improves type correctness to 99%. In terms of generating unique examples, we find that each step in our process improves upon the previous step. Retrieval and adding contexts see approximately a 10% improvement over the previous steps. Postprocessing improves uniqueness slightly, with 67% of examples generated having 3 examples. Cosine similarity between examples remains relatively stable across modes, with retrieval templating (third row) slightly improving example diversity. We do want examples to have consistent format, while still being diverse, likely resulting in lower diversity scores. The average Levenshtein edit distance on our dataset is 15 characters, suggesting the examples are still syntactically different from one another on average. Example correctness remains relatively stable across all four settings (varying from 66% to 74%). The correctness of our final approach is higher than any intermediate approach. Note that our evaluation of example correctness is conservative: in order for an example to be correct, all generations need to satisfy preconditions and have consistent format. Even examples not labeled as correct can still be useful for de- velopers (such as an example of a time parameter that is missing the timezone), meaning that the 74% correctness rate is likely an underestimate of the true utility of the examples. Overall, our approach is often correct, showing the promise that LLMs pose for usefully enhancing API specifications. The extrinsic evaluation in the following section shows that, despite not always being correct, synthetic examples benefit all three downstream clients we tried. Listing 3: Fuzzing enhanced OpenAPI specification. Examples generated by I CICLare both diverse and correct. name: currency description: Search by ISO 4217 currency code required: true schema: type: string enum: - USD - CAD - EUR example: USD ... name: currency description: Search by ISO 4217 currency code required: true schema: type: string V. E XTRINSIC EVALUATION This section evaluates I CICLon downstream tasks (clients on the right-hand side of Figure 1), namely software testing (Section V-A), API chatbots (Section V-B), and, by means of an exploratory pilot, human API understanding (Section V-C). A. Software Testing The goal of REST API testing is to find inputs that increase code coverage (and, ultimately, find bugs). At a high level, API fuzzers encode the schemas present in OpenAPI specifications, and use them to generate values for API endpoints. Coverage serves as a feedback mechanism: calls that increase coverage are saved for further mutation, while calls that do not are thrown out. 1) Dataset and fuzzers: We evaluate I CICLfor fuzzing using a dataset from Kim et. al. [10] consisting of both small and large APIs. We exclude the OMDB and Spotify APIs from that dataset, due to changes in both that make usage more challenging, and internal restrictions that block certain endpoints. This leaves seven widely-used REST API services \u2014 FDIC, REST Countries, ohsome, GenomeNexus, OCVN, LanguageTool, and YouTube. This previous dataset included 4 that were run as local instances \u2014 GenomeNexus, OCVN, LanguageTool, and YouTube \u2014 for the purposes of computing coverage. We therefore follow the previous evaluation and compute black-box performance on all 7 and coverage on the 4. We evaluate using four popular fuzzers: EvoMaster [8], MoREST [7], RESTest [29], and RestTestGen [30]. We report results for each fuzzer along with the aggregated results across them all. We used a version of RESTest that includes ARTE [9], a state-of-the-art example generation approach, as part of its implementation. Hence, we refer to it as RESTest\/ARTE below, helping show how I CICLcompares to and can complement ARTE. 2) Approach: To measure performance in the fuzzing con- text, we run I CICLwith each OpenAPI parameter that we extract from the fuzzing OpenAPI specifications. For each APIparameter, we overload the specification with two options: the parameter with examples, and the parameter without examples, following Kim et al. [10]. Since most fuzzers do not directly use API examples, we had to use a work-around, where we encode the examples both using the example attribute and as an enum. Listing 3 shows how we encode these values (the generated examples are USD,CAD andEUR). We also include the original parameter, to allow for fuzzers to explore values outside these examples. This ensures the fuzzer can explore the example values and mutate existing example values by hitting the overloaded endpoint. For example, a fuzzer could choose the value CAD and then mutate it to CDF by hitting the overloaded endpoint twice. We run each fuzzer on both the original specification and the enhanced specification. We were unfortunately unable to directly compare to the approach in Kim. et. al [10]. We have filed an issue and have an ongoing discussion with the authors on the use of their artifact, and the paper does not directly ablate example generation. We did randomly sample 700 of our 13,346 mined API parameters (approximately 5%), finding that only 43 enumerated examples occur in the description. Thus, even if Kim et. al. [10] extracts examples with 100% accuracy, it could only do so for 6% of all API parameters. 3) Metrics: We use a combination of API fuzzing metrics and code coverage to evaluate the performance change of adding examples to each fuzzer. Proportion of 2xx Requests: 2xx requests represent success- ful invocations of API endpoints, i.e., requests that yielded an HTTP response code between 200\u20132997. These requests are saved for further fuzzing; thus having more 2xx requests means that the fuzzer is capable of testing functionality beyond simple input validation. Proportion of 4xx Requests: 4xx requests represent poorly formatted invocations, where the fuzzer invokes the API incorrectly. Ideally, a fuzzer should make fewer 4xx requests, as these are not testing deep functionality and wasting the fuzzing time budget. Proportion of 5xx Requests: 5xx requests represent internal server errors. The goal of fuzzing is to catch such errors, thus more 5xx requests represent a successful fuzzing effort. Branch Coverage: The goal of fuzzing efforts is to auto- matically test as much of the API as possible. Coverage is important, as higher code coverage indicates the fuzzer is testing a larger proportion of the API. We report branch coverage achieved by each fuzzer, as well as averaged across all four. 4) Results: Table II shows results across all fuzzers. Be- sides EvoMaster, all fuzzers exhibit a similar trend: exam- ples lead to more 2xx requests (around 3%), fewer 4xx requests (around 3%), and around the same 5xx requests. This means that our example generation approach can seamlessly integrate with fuzzers to improve API testing, by better seeding fuzzers with realistic parameter examples that the fuzzers can use to invoke APIs. 7https:\/\/en.wikipedia.org\/wiki\/List ofHTTP status codes TABLE II: First three columns: API performance results for RESTest\/ARTE, EvoMaster, MoREST, and RestTestGen across all 7 APIs. The proportion of 2xx requests goes up, 4xx goes down and 5xx slightly increases with enhanced examples. EvoMaster is the one exception, with 2xx request proportions decreasing. Last column: Coverage results. Coverage universally increases across all fuzzers with our enhancements. Freq. of 2xx Freq. of 4xx Freq. of 5xx Branch Cov. Tool Name Base Enhanced Diff Base Enhanced Diff Base Enhanced Diff Base Enhanced Diff RESTest\/ARTE 0.28 0.31 +11% 0.57 0.54 -5% 0.10 0.10 +0% 4.0 6.2 +57% MoREST 0.01 0.04 +300% 0.83 0.81 -2% 0.10 0.10 +0% 3.2 17.5 +447% EvoMaster 0.29 0.24 -17% 0.58 0.62 +7% 0.08 0.07 -13% 6.9 12.2 +77% RestTestGen 0.21 0.26 +24% 0.  [Text truncated due to context length limits]\nDocument 3\nSnipGen: A Mining Repository Framework for Evaluating LLMs for Code Daniel Rodriguez-Cardenas, Alejandro Velasco, and Denys Poshyvanyk Department of Computer Science, William & Mary Williamsburg, V A Email: dhrodriguezcar, svelascodimate, dposhyvanyk {@wm.edu } Abstract \u2014Large Language Models (LLMs), such as transformer-based neural networks trained on billions of parameters, have become increasingly prevalent in software engineering (SE). These models, trained on extensive datasets that include code repositories, exhibit remarkable capabilities for SE tasks. However, evaluating their effectiveness poses significant challenges, primarily due to the potential overlap between the datasets used for training and those employed for evaluation. To address this issue, we introduce SnipGen , a comprehensive repository mining framework designed to leverage prompt engineering across various downstream tasks for code generation. SnipGen aims to mitigate data contamination by generating robust testbeds and crafting tailored data points to assist researchers and practitioners in evaluating LLMs for code-related tasks. In our exploratory study, SnipGen mined approximately 227Kdata points from 338Krecent code changes in GitHub commits, focusing on method-level granularity. SnipGen features a collection of prompt templates that can be combined to create a Chain-of-Thought-like sequence of prompts, enabling a nuanced assessment of LLMs\u2019 code generation quality. By providing the mining tool, the methodology, and the dataset, SnipGen empowers researchers and practitioners to rigorously evaluate and interpret LLMs\u2019 performance in software engineering contexts. Index Terms \u2014Deep learning, code generation, datasets, large language models, evaluation I. I NTRODUCTION Large Language Models (LLMs) have demonstrated signif- icant success across diverse software engineering (SE) tasks, including code auto-completion [1]\u2013[5], code summarization [6], [7], code review [8], [9], code translation [10], clone detection [11], [12], and program repair [13]\u2013[19]. LLMs are neural models trained on huge datasets including complete GitHub repositories. Common testbeds for evaluating LLMs for code such as HumanEval ,MBPP andCodeXGlue are no longer sufficient [20]. In addition, as benchmarks and testbeds are released, new LLMs probably already seen those testbeds. Therefore the testbeds are prone to be outdated as soon as a new LLM is released. LLMs perform complex tasks by relying on statistical knowledge acquired from data distributions, a phenomenon described by Wei et al. as emerging capabilities [21]. Given the limited understanding of the nature of this phenomenon, we can formulate an important question: under what conditions LLMs produce the desired output? Prompt engineering ad- dresses this question by harnessing these capabilities, guiding LLMs to make more accurate predictions. Furthermore, giventhat LLMs can extract rules from the provided context ( i.e.,in- context learning), prompt engineering is a natural and intuitive way for people to use LLMs. Recent studies have demonstrated that LLMs exhibit im- provements in accuracy for downstream tasks when prompts are enhanced and augmented [22], [23]. Moreover, new meth- ods for crafting better prompts are being explored. For ex- ample, Beurer-Kellner et al. [24] introduce the idea of Lan- guage Model Programming (LMP) which combines text-based prompting with scripting. Furthermore, Wei et al. [25] shows that the incorporation of Chain-of-Thought (CoT) significantly improves the ability of LLMs to perform complex reasoning. Understanding the internal mechanisms of LLMs presents a significant challenge. Current datasets and benchmarks often lack the curated data necessary for thorough performance analyses. Therefore, there is a critical need for consistent data points to effectively evaluate the performance of LLMs across various SE tasks. We argue that well-designed testbeds and prompts are the key to accurately assessing LLMs understand- ing of complex information, such as task-related semantics. To bridge the gap between existing datasets and bench- marks, we developed SnipGen .SnipGen is a framework to collect source code snippets from GitHub. Each snippet is automatically augmented with prompts tailored for various software tasks. Practitioners and researchers can query and generate new prompts according to the SE task and experiment with different configurations for evaluating LLMs for code. Our goal is to provide resources that can more accurately assess the performance of LLMs and aid in the construction of more detailed benchmarks. The contributions of this paper are listed as follows: 1) A Framework for mining software repositories and crafting data points augmented with prompts for specific SE downstream tasks. 2) a generated testbed comprising Python snippets with calculated features from the AST, natural language, and vulnerabilities analysis [26]. 3) Prompt-generated dataset with mutated snippets crafted for Code Completion, Commit generation, and Code summarization. 4) source code and complementary material used in this research are published in an open-source repository [27]. II. T HESnipGen FRAMEWORK SnipGen is a framework to extract snippets at method granularity from GitHub. SnipGen follow steps for curating thearXiv:2502.07046v2  [cs.SE]  16 Feb 2025 extracted raw data and take features from the data such as the number of identifiers, vocabulary, tokens, etc. Features derived from their AST representations and further complementary data. Our dataset can potentially improve the quality of the predictions in downstream tasks by augmenting the prompts, thereby enabling LLMs to perform more effectively. 1Data Collection2Pre-processing 3 Data V alidationRepository FilterCommit filteringMethods & Comments AST Parsing Feature computation Database insert Remove ducplicates Text & Code Meaningfulness Feature validation4SE T estbed Generation RandomCut code Docstring filtering Commit filteringSpecificationJaccard similarity Filter Raw data filteting Prompt template Code Selection Generate prompt5  Model Evaluation Model ExecutionPredicted OutcomeAnalysis Prompt Generation \u00abSinpiGen\u00bb Data curation\u00abPyDriller\u00bb Collector\u00abSinpiGen\u00bb Feature extractor Comment\/ Uncomment Testbeds Indexed Data HugginfaceArtifacts SinpGen Miner \u00abSinpiGen\u00bb Testbed specificationSnipGen ArchitectureVul. Detection Fig. 1: SnipGen Data collection and prompt generation Fig. 1 depicts the process followed by SnipGen to gen- erate a testbed and the SnipGen architecture. The SnipGen architecture comprises components to collect, curate, store, extract, and generate a SE-oriented testbed. The process be- gins with 1, the Data Collection phase, where source code snippets are extracted with pydriller library [28] from selected repositories in GitHub given a tie window. A set of snippets representing a Python method is extracted from each commit. This is followed by 2, a Pre-processing step, where the - Data Curation- SnipGen component stores the raw data in a MySQL database. Once the data is formatted and saved in the storage, SnipGen looks for exact match snippets and removes duplicates. The -Feature extractor- component parses the code into the AST representation using tree-sitter [29] and computes associated features ( i.e.,the number of AST levels, AST nodes, comments, function name). The data validation at step 3is a manual evaluation where the authors confirm the dimension values and the meaningful- ness of the Docstring and linked code, the two authors first selected the docstring with more than 20 words and evaluate the description against the code snippet. The description must depict the steps or intention of the snippet. The testbed generation step 4, filters the raw data, evaluates the Jaccard similarity, and identifies vulnerable code. The raw filtering depends on the SE task, for example for code completion SnipGen filters the valid code with more than two lines of code. SnipGen uses CodeQL [30] for vulnerability detection and appends the vulnerability location on the snippet. Finally, step 5uses the selected snippets from 4andapplies the prompt template to the aimed SE task generating a final prompt. SnipGen enables the model evaluation and benchmarking as used in [31]\u2013[33]. The following subsections include a detailed description of the features of each data point. A. Data Point Feature Structure snippet ID Integer commitID Integer repo String path String file_name String commit_message String code String url String language Stringast_data ID Integer snippetID Integer url String ast_errors String n_ast_errors Integer ast_levels Integer n_ast_nodes Integercode_features ID Integer snippetID Integer n_whitespaces Integer n_words Integer nloc Integer complexity Integer token_counts Integer n_identifiers Integer mutations ID Integer snippetID Integer random_cut String signature Stringdocumentation ID Integer snippetID Integer docstring String n_words Integer n_whitespaces Integer vocab_size Integer vulnerabilities ID Integer snippetID Integer span_position T upleprompts ID Integer snippetID Integer se_task String prompt_script String Fig. 2: SnipGen data schema. The snippet represents the core commit collected with documentation. Linked tables contain calculated features. SnipGen can collect a set of Python methods that serve as evaluative data points. Each data point has associated features at seven dimensions as observed at Fig. 2. These seven dimensions describe the static feature from the snippet. We aim to link code fragments with their properties. The first dimen- sion corresponds to snippets\u2019 identification, which includes thecommit id(i.e., commit hash), repository name, path, filename ,funname ,commit message . The second dimension is related to the associated documentation docstring . The doc- string extended to complementary natural language features such as nwords, vocab size, language, and nwhitespaces . The third dimension corresponds to the snippet\u2019s syntactic in- formation, which includes the actual code base, nasterrors , nastlevels ,nastnodes ,nwords ,vocab size,token count , and nwhitespaces . The fourth dimension corresponds to canonical software metrics, which include nloc,complexity , nidentifiers . The fifth dimension depicts the span position for vulnerabilities detected from the code snippet. The sixth dimension is associated with the snippet mutation when the code is randomly cut one line after the signature, therefore SnipGen identifies the signature as the original snippetID and cut code. Finally, the seventh dimension comprises the linked features to the generated prompt. SnipGen labels the prompt to the SE task and the prompt configuration. B. Software Engineering Tesbed Task Data curation, pre-processing, and data validation produce a testbed oriented to evaluate a model. For instance, a Ran- domCut andWithDocString testbeds might evaluate the model TABLE I: Prompt templates for each SE task using collected SnipGen features. SE Task ID Prompt Template Code completionP1 Complete the following <language >method: <RandomCut > P2 You have a <language >function named <signature >, the function starts with the following code <RandomCut >. The function is in charge of <docstring > P3 Create a function that accomplish the following functionality in <lan- guage>code:<docstring > Commit generationP4 Please describe the following code change to create log message: status before <RandomCut >status now <code> Summ. P5 I need a summary for the following code: <code> Processing promptP6 Change the method signature by <signature > P7 Reduce or complete the method using only <nloc>lines of code P8 remove comments; remove summary; remove throws; remove function modifiers at SE tasks, such as code completion \u2014generating code to fill in missing parts of a function. WithDocString testbed selects the snippets with valid documentation and code so the LLM input compresses both a description and code. FromCommit testbed is focused on selecting meaningful commit messages and linked source code so that to evaluate either commit gen- eration \u2014producing commit messages based on code changes orcode generation producing the complete snippet from the description. FromDocString testbed select only meaningful code descriptions ( i.e., only docstring ) to generate the code snippet also configuring a code generation case. SnipGen can be used to evaluate code summarization \u2014creating natural language descriptions of the functionality implemented in the provided source code. If we select the original code from the WithDocString testbed and the ones at the top of docstring length then we can use the testbed for summarization. C. Prompt templates The effectiveness of LLMs in code generation is greatly influenced by prompt design. At this point SnipGen only produces a set of data points that can be organized as an input for an autoregressive LLM since the tesbed contains the input and the expected output. SnipGen combines prompt templates and gathered data points to build the final prompt input. The structure, keywords, and context of a prompt play a crucial role in shaping results and analyses. Prompts can be configured as a single-step or multi-step, with the latter allowing iterative refinement based on the model\u2019s initial response. Chau et al. [34] explore such multi-step configurations. Table I lists eight prompt templates, practitioners can modify the template according to the evaluation task. From the proposed list, P1\u2212P5supports single-step SE tasks, while P6\u2212P8enables multi-step processing by combining prompts to refine outputs. For instance, SnipGen can combine P1 +P8,P3 +P6, or P3 +P8for code completion. Forcode completion ,SnipGen defines three prompts. P1 asks the model to complete a method from a randomly se- lected cut position ( RandomCut ) in the specified programming language ( language ).P2extends P1by including additional details, such as the method\u2019s signature and docstring .P3, in contrast, provides only an NL description extracted from the method\u2019s docstring . In commit generation, P4instructs the model to create an NL description of the changes madeto transform the RandomCut version of a method into its complete code ( code ).P5is designed to ask the model to generate the commit message from the mutated code and the actual code. Lastly, in code summarization ,P6provides only the code, which the model uses to generate a corresponding summary. D. SnipGen Prompt Generation and Use SE Task Task Context Prompt Template Sampling Snippet Mutation Prompt  Generation Model Evaluation Training Testing Canonical DatasetsRaw DatasetA B Canonical Evaluation Code generationSummarization & Commit GenerationSnipGen Fig. 3: SnipGen Dataset and use. Adescribes the SnipGen data collection and steps until prompt generation. Bdescribes the canonical path for training and evaluate LLMs TheSnipGen framework is designed to select a SE task and evaluate a LLM using the testbed with a given context with a designed prompt. Fig. 3 depicts the options a practitioner has to evaluate a LLM. The database supports a query to filter the snippets according to the SE task, for instance for code completion we can sample the snippets with linked docstring with more than 10 words, this provides the task context (see, Fig. 3 section A). The prompt generation might contain a mutated snippet such as RandomCut to perform the required task. For example, for code completion, we will need a partial code snippet that must be auto-completed by the LLM therefore we need to cut the original snippet smartly. SnipGen can use the RandomCut method to split the code beyond the method signature. Practitioners can still evaluate the model using canonical datasets and metrics to compare against the new collected SnipGen testbed. III. E XPERIENCE REPORT In this section, we describe our experience of using SnipGen for collecting a testbed and generating a set of prompts. We also briefly describe three use cases illustrating how SnipGen was successfully used to evaluate LLMs for code. A. SnipGen Testbed Generation The experience with SnipGen begins by mining reposi- tories from GitHub, as detailed in Sec. II. We focused on the most popular Python repositories, applying the follow- ing query filters: language:Python fork:false size: >= 30000 pushed: >2021-12-31 stars: >2000 .The query gathers the most popular repositories in Python. We selected the top 200 repositories including keras, numpy, pandas, sentry, etc. We extracted the new snippets reported on commits between 2022 and 2023 from selected repositories. Then we used the data curation to remove duplicates and feature extraction to generate and extract the associated features. We configured TABLE II: Dataset size and deduplication percentage SE Task Testbed I\/O Dupes Dupe % Size Prompts Code CompletionRandomCut code\u21d2code 120 2.4% 4880 9760 WithDocString code&text \u21d2code 145 2.9% 4855 9710 Code GenerationFromDocString text\u21d2code 76 1.5% 4924 14772 FromCommit text\u21d2code 97 1.9% 4903 4903 Sumarization SummarizationGen code\u21d2text 156 3.1% 4844 4844 Vulnerabilities VulnerabilitySpan code\u21d2code 2 0.4% 410 410 a0.7similarity threshold [35], [36] to de-duplicate snippets using HuggingFace tokenizer BPE. SnipGen saves the raw data and their features into a JSON and a database. We randomly validated 960 out of \u2248227Kdata points to confirm the extracted features and the meaningfulness of the Docstring and linked code. We sampled until 5kdata points from the RawData testbed to construct six testbeds, each tailored for a specific SE task as described at Sec. II-B. To create RandomCut , we selected data points with more than 10tokens or 100 characters, and subsequently, each data point was randomly truncated after the method signature. For SummarizationGen and FromCommit , we filtered RawDataDocstring data points with more than 10 words or 50 characters. Table. II provides information about the SE task associated with each curated testbed, the percentage of detected duplicates, the final size, and the generated number of prompts. B. Successful Use Cases Galeras [31]: Galeras is a benchmark for measuring the causal effect of SE prompts for code completion. Galeras configures a set of treatments to assess the influence of potential confounders on the outcomes of ChatGPT ( i.e.,GPT- 4). The selected confounders are: prompt size(from prompts), nwhitespaces (from documentation), token counts , and nloc (from code features). This use case of SnipGen demonstrates that prompt engineering strategies (such as those listed in Table I - processing prompt) have distinct causal effects on the performance of ChatGPT. SyntaxEval [33]: In this use case Syn taxEval evaluates the ability of Masked Language Models ( i.e.,Encoder-based Transformers) to predict tokens associated with specific types in the AST representation ( i.e.,syntactic features). SyntaxEval used SnipGen to construct a code completion testbed with ap- proximately 50KPython snippets. SyntaxEval aims to account for potential confounders such as astdata andcode features (illustrated in Fig. 2), the analysis revealed no evidence that the evaluated syntactic features influenced the accuracy of the selected models\u2019 predictions. ASTxplainer [32]: ASTxplainer is an explainability method designed to assess how effectively a LLM ( e.g., decoder- based transformers) predicts syntactic structures. ASTxplainer aggregates next-token prediction values through syntactic de- composition, quantified as AsC-Eval values to evaluate the effectiveness. ASTxplainer findings reveal that the ability to predict syntactic structures strongly depends on the LLM\u2019s parameter size and fine-tuning strategy. Furthermore, causal analysis controlling for confounding variables (e.g., astdataandcode features ) shows that AsC-Eval values at the snippet level negatively impact the cross-entropy loss of the evaluated LLMs. IV. S IMILAR DATASETS Significant efforts have produced datasets for evaluating LLMs in SE tasks, including DeepFix for program repair [37], CodeContest and CoNaLa for program synthesis [38], [39], and CodeSearchNet for code retrieval [40]. Expansions like CodeXGLUE [41], xCodeEval [42] target broader tasks, while benchmarks such as HumanEval and SecurityEval focus on functional correctness and vulnerabilities [43], [44]. Despite these efforts, existing datasets often suffer from contamination [20], [45], with overlaps between training and evaluation data, and benchmarks are prone to memorization by models [46], limiting their effectiveness in assessing true generalization. Recent research work has explored the dynamic generation of prompts and testbeds, for instance, EvoPrompt is a frame- work for automatic discrete prompt optimization that connects LLMs with Evolutionary Algorithms [47]. Evol-instruct is a systematic approach to generate instruction-response pairs by iteratively improving prompts and responses through model self-enhancement [48]. LiveCodeBench is a benchmark for evaluating LLMs designed to generate code [20]. Unlike Snip- Gen, LiveCodeBench addresses issues of data contamination by using continuously updated problems from online coding competitions. V. L IMITATIONS AND FUTURE WORK Documentation Quality Analysis: Meaningfulness evalua- tion for the docstring and linked code can not be automatized and depends on the project context. To handle this limitation, we conducted a manual validation. As part of future work, SnipGen should streamline the manual validation process and ascertain the significance of comments and documentation within the snippets. Vulnerability Detection: The detection of vulnerabilities is reliant solely on the CodeQL tool and its updates; we did not employ any alternative tools to validate these results. Assumption Regarding Snippet Exposure :SnipGen miti- gates to select \u201ccontaminated\u201d data ( i.e.,already seen snippets) by selecting snippets from specific commit time windows. A practitioner can specify the time windows depending on the LLM release date. SnipGen aims to reduce data contamination by including a prompt and variating the cut code. However, it\u2019s important to note that the extracted code changes might include older lines of code or reused code fragments. Our evaluation does not encompass the entire project history to identify older references. For future work, 1) we propose to extend this dataset to support multiple programming languages; 2) Integrate a wider number of SE tasks. 3) Rank each data point according to the cyclomatic complexity number of AST nodes, documentation, and number of identifiers. The rank will prove better criteria on which snippets are more interesting to evaluate the LLM. REFERENCES [1] J. Austin, A. Odena, M. Nye et al. , \u201cProgram synthesis with large language models,\u201d 2021. [2] D. Hendrycks, S. Basart, S. Kadavath et al. , \u201cMeasuring coding chal- lenge competence with APPS,\u201d CoRR , vol. abs\/2105.09938, 2021. [3] M. Chen, J. Tworek, H. Jun et al. , \u201cGeneration Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions,\u201d 2021, publisher: arXiv Version Number: 2. [4] M. White, C. Vendome, M. Linares-Vasquez et al. , \u201cToward deep learn- ing software repositories,\u201d in 2015 IEEE\/ACM 12th Working Conference on Mining Software Repositories , 2015, pp. 334\u2013345. [5] M. Ciniselli, N. Cooper, L. Pascarella et al. , \u201cAn empirical study on the usage of transformer models for code completion,\u201d IEEE Transactions on Software Engineering , vol. 48, no. 12, pp. 4818\u20134837, 2022. [6] A. LeClair, A. Bansal, and C. McMillan, \u201cEnsemble Models for Neural Source Code Summarization of Subroutines,\u201d Jul. 2021, arXiv:2107.11423 [cs]. [7] K. Moran, A. Yachnes, G. Purnell et al. , \u201cAn empirical investigation into the use of image captioning for automated software documentation,\u201d in 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) , 2022, pp. 514\u2013525. [8] R. Tufano, L. Pascarella, M. Tufano et al. , \u201cTowards automating code review activities,\u201d in 2021 IEEE\/ACM 43rd International Conference on Software Engineering (ICSE) , 2021, pp. 163\u2013174. [9] R. Tufano, S. Masiero, A. Mastropaolo et al. , \u201cUsing pre-trained models to boost code review automation,\u201d in 2022 IEEE\/ACM 44th International Conference on Software Engineering (ICSE) , 2022, pp. 2291\u20132302. [10] A. T. Nguyen and T. N. Nguyen, \u201cGraph-based statistical language model for code,\u201d in ICSE\u201915 . IEEE Press, 2015, p. 858\u2013868. [11] M. White, M. Tufano, C. Vendome et al. , \u201cDeep learning code frag- ments for code clone detection,\u201d in 2016 31st IEEE\/ACM International Conference on Automated Software Engineering (ASE) , 2016, pp. 87\u201398. [12] M. Tufano, C. Watson, G. Bavota et al. , \u201cDeep learning similarities from different representations of source code,\u201d in 2018 IEEE\/ACM 15th International Conference on Mining Software Repositories (MSR) , 2018, pp. 542\u2013553. [13] \u2014\u2014, \u201cLearning How to Mutate Source Code from Bug-Fixes,\u201d ICSME 2019 , pp. 301\u2013312, 2019. [14] Y . Zhou, S. Liu, J. Siow et al. , \u201cDevign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks.\u201d [15] M. White, M. Tufano, M. Mart \u00b4\u0131nez et al. , \u201cSorting and transforming program repair ingredients via deep learning code similarities,\u201d in 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER) , 2019, pp. 479\u2013490. [16] M. Tufano, J. Pantiuchina, C. Watson et al. , \u201cOn learning meaningful code changes via neural machine translation,\u201d in 2019 IEEE\/ACM 41st International Conference on Software Engineering (ICSE) , 2019, pp. 25\u201336. [17] M. Tufano, C. Watson, G. Bavota et al. , \u201cAn empirical investigation into learning bug-fixing patches in the wild via neural machine transla- tion,\u201d in 2018 33rd IEEE\/ACM International Conference on Automated Software Engineering (ASE) , 2018, pp. 832\u2013837. [18] Z. Chen, S. Kommrusch, M. Tufano et al. , \u201cSequencer: Sequence-to- sequence learning for end-to-end program repair,\u201d IEEE Transactions on Software Engineering , vol. 47, no. 9, pp. 1943\u20131959, 2021. [19] A. Connor, A. Harris, N. Cooper et al. , \u201cCan we automatically fix bugs by learning edit operations?\u201d in 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) . Los Alamitos, CA, USA: IEEE Computer Society, mar 2022, pp. 782\u2013792. [20] N. Jain, K. Han, A. Gu et al. , \u201cLiveCodeBench: Holistic and Contami- nation Free Evaluation of Large Language Models for Code,\u201d Jun. 2024, arXiv:2403.07974 [cs]. [21] J. Wei, Y . Tay, R. Bommasani et al. , \u201cEmergent Abilities of Large Language Models,\u201d Oct. 2022, arXiv:2206.07682 [cs]. [22] Y . Zhou, A. I. Muresanu, Z. Han et al. , \u201cLarge language models are human-level prompt engineers,\u201d ArXiv , vol. abs\/2211.01910, 2022. [23] J. White, S. Hays, Q. Fu et al. , \u201cChatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design,\u201d ArXiv , vol. abs\/2303.07839, 2023. [24] L. Beurer-Kellner, M. Fischer, and M. Vechev, \u201cPrompting is pro- gramming: A query language for large language models,\u201d Proc. ACM Program. Lang. , vol. 7, no. PLDI, jun 2023.[25] J. Wei, X. Wang, D. Schuurmans et al. , \u201cChain-of-Thought Prompt- ing Elicits Reasoning in Large Language Models,\u201d Jan. 2023, arXiv:2201.11903 [cs]. [26] D. Rodriguez-Cardenas, \u201cSnipgen tesbed to evaluate llms for code,\u201d https:\/\/doi.org\/10.5281\/zenodo.14279563, January 2025. [27] S. R. Group, \u201cSnipgen: A code snippet generation tool,\u201d https:\/\/github. com\/WM-SEMERU\/snipgen, 2025, accessed: 2025-01-30. [28] PyDriller Contributors, \u201cPydriller documentation,\u201d https:\/\/pydriller. readthedocs.io\/en\/latest\/, n.d., accessed: 2024-11-29. [29] Tree-Sitter Contributors, \u201cTree-sitter documentation,\u201d https:\/\/tree-sitter. github.io\/tree-sitter\/, n.d., accessed: 2024-11-29. [30] GitHub, \u201cAbout codeql,\u201d https:\/\/codeql.github.com\/docs\/ codeql-overview\/about-codeql\/, n.d., accessed: 2024-11-29. [31] D. Rodriguez-Cardenas, D. N. Palacio, D. Khati et al. , \u201c Benchmarking Causal Study to Interpret Large Language Models for Source Code ,\u201d in2023 IEEE International Conference on Software Maintenance and Evolution (ICSME) . Los Alamitos, CA, USA: IEEE Computer Society, Oct. 2023, pp. 329\u2013334. [32] D. N. Palacio, A. Velasco, D. Rodriguez-Cardenas et al. , \u201cEvaluating and Explaining Large Language Models for Code Using Syntactic Structures,\u201d Aug. 2023, arXiv:2308.03873. [33] A. Velasco, D. N. Palacio, D. Rodriguez-Cardenas et al. , \u201cWhich syn- tactic capabilities are statistically learned by masked language models for code?\u201d in Proceedings of the 2024 ACM\/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results , ser. ICSE-NIER\u201924. New York, NY , USA: Association for Computing Machinery, 2024, p. 72\u201376. [34] C. Liu, X. Bao, H. Zhang et al. , \u201cImproving ChatGPT Prompt for Code Generation,\u201d May 2023, arXiv:2305.08360 [cs]. [35] M. Allamanis, \u201cThe adverse effects of code duplication in machine learning models of code,\u201d in OOPLSA , 2019, pp. 143\u2013153. [36] C. Wang, K. Cho, and J. Gu, \u201cNeural Machine Translation with Byte- Level Subwords,\u201d Dec. 2019, arXiv:1909.03341. [37] R. Gupta, S. Pal, A. Kanade et al. , \u201cDeepFix: Fixing Common C Lan- guage Errors by Deep Learning,\u201d Proceedings of the AAAI Conference on Artificial Intelligence , vol. 31, no. 1, Feb. 2017, number: 1. [38] Y . Li, D. Choi, J. Chung et al. , \u201cCompetition-Level Code Generation with AlphaCode,\u201d Feb. 2022, arXiv:2203.07814. [39] P. Yin, B. Deng, E. Chen et al. , \u201cLearning to mine aligned code and natural language pairs from stack overflow,\u201d in International Conference on Mining Software Repositories , ser. MSR. ACM, 2018, pp. 476\u2013486. [40] H. Husain, H.-H. Wu, T. Gazit et al. , \u201cCodeSearchNet chal- lenge: Evaluating the state of semantic code search,\u201d arXiv preprint arXiv:1909.09436 , 2019. [41] S. Lu, D. Guo, S. Ren et al. , \u201cCodeXGLUE: A machine learning benchmark dataset for code understanding and generation.\u201d [42] M. A. M. Khan, M. S. Bari, X. L. Do et al. , \u201cxCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval,\u201d Nov. 2023, arXiv:2303.03004. [43] M. Chen, J. Tworek, H. Jun et al. , \u201cEvaluating Large Language Models Trained on Code,\u201d Jul. 2021, arXiv:2107.03374 [cs]. [44] M. L. Siddiq and J. C. S. Santos, \u201cSecurityeval dataset: Mining vul- nerability examples to evaluate machine learning-based code generation techniques,\u201d in Proceedings of the 1st International Workshop on Min- ing Software Repositories Applications for Privacy and Security , ser. MSR4P&S 2022. New York, NY , USA: Association for Computing Machinery, 2022, p. 29\u201333. [45] A. Yadav, H. Beniwal, and M. Singh, \u201cPythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs,\u201d in Findings of the Association for Computational Linguistics: EMNLP 2024 . Miami, Florida, USA: Association for Computational Linguistics, 2024, pp. 17 113\u201317 126. [46] D. Ramos, C. Mamede, K. Jain et al. , \u201cAre large language models memorizing bug benchmarks?\u201d 2024. [47] Q. Guo, R. Wang, J. Guo et al. , \u201cConnecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers,\u201d Feb. 2024, arXiv:2309.08532 [cs]. [48] C. Xu, Q. Sun, K. Zheng et al. , \u201cWizardLM: Empowering Large Language Models to Follow Complex Instructions,\u201d Jun. 2023, arXiv:2304.12244.\nDocument 4\nAre Large Language Models Memorizing Bug Benchmarks? Daniel Ramos Carnegie Mellon University, INESC-ID Pittsburgh, PA, USA danielrr@cmu.eduClaudia Mamede* Carnegie Mellon University, FEUP Pittsburgh, PA, USA cmamede@andrew.cmu.eduKush Jain* Carnegie Mellon University Pittsburgh, PA, USA kdjain@andrew.cmu.edu Paulo Canelas* Carnegie Mellon University, LASIGE Pittsburgh, PA, USA pasantos@andrew.cmu.eduCatarina Gamboa* Carnegie Mellon University, LASIGE Pittsburgh, PA, USA cgamboa@andrew.cmu.eduClaire Le Goues Carnegie Mellon University Pittsburgh, PA, USA clegoues@andrew.cmu.edu Abstract \u2014Large Language Models (LLMs) have become inte- gral to various software engineering tasks, including code gener- ation, bug detection, and repair. To evaluate model performance in these domains, numerous bug benchmarks containing real- world bugs from software projects have been developed. However, a growing concern within the software engineering community is that these benchmarks may not reliably reflect true LLM performance due to the risk of data leakage. Despite this concern, limited research has been conducted to quantify the impact of potential leakage. In this paper, we systematically evaluate popular LLMs to assess their susceptibility to data leakage from widely used bug benchmarks. To identify potential leakage, we use multiple metrics, including a study of benchmark membership within com- monly used training datasets, as well as analyses of negative log- likelihood and 5-gram accuracy . Our findings show that certain models, in particular codegen-multi , exhibit significant evidence of memorization in widely used benchmarks like Defects4J , while newer models trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage. These results highlight the need for careful benchmark selection and the adoption of robust metrics to adequately assess models capabilities. Index Terms \u2014Automated Program Repair, Large Language Model, Data Leakage I. I NTRODUCTION Large language models (LLMs) have become ubiquitous for various software engineering tasks. Assessing these models\u2019 abilities in context, beyond the basic evaluations typically performed upon release (e.g., on HumanEval [1]), benefits from realistic benchmarks that represent real-world software development tasks. Two significant such tasks are bug find- ing, through automated fault localization (FL) [2]; and bug fixing, through automated program repair [3] (APR). The Software Engineering community has released numerous bug benchmarks for evaluating success on these tasks, consisting of real bugs from open-source software projects. Notable such datasets include, for example, Defects4J [4] ( Java ) and BugsInPy [5] ( Python ); similarly, ML researchers recently introduced SWEBench [6]. *Equal contributionHowever, a growing concern in software engineering research is the degree to which data leakagecompromises the evaluation of true model capability [7], [8]. Data leakage refers to the use of information during model training that would not normally be available during prediction, leading to inflated performance metrics that misrepresent a model\u2019s true effectiveness. The programs and bugs in many benchmarks and solutions have been publicly accessible for years, increasing the chance they were incorporated into LLM training data. For instance, the widely-used Defects4J dataset, based on popular Java projects, was first released in 2014. To illustrate, consider the example shown in Figure 1. Here, we prompted codegen-multi (6 billion parameters) to predict the next tokens based on an incomplete snippet (highlighted in yellow) from the bug-fix file for Defects4J bug #39 in theLang project. The generated text, highlighted in blue, shows that codegen-multi reproduced the remainder of the solution file verbatim, including specific comments (lines 31 and 35), token by token. This exact reproduction strongly sug- gests that the model has memorized the benchmark solution. Such behavior underscores the need for caution when using benchmarks like Defects4J , as they may lead to misleading conclusions about a model\u2019s generalizability. Researchers in other domains, like NLP or ML, have at- tempted to quantify data leakage in their datasets [9], [10]. To the best of our knowledge, this effort has not been undertaken for popular bug datasets in Software Engineering. Detecting leakage is challenging. LLMs are often pre-trained by organi- zations that do not disclose their datasets, the total volume of data, nor model parameters. Consequently, determining whether a benchmark has been directly included or merely mirrored in the model\u2019s training set is difficult. Even verifying the presence of data only establishes that the model has seen it, not necessarily that it has memorized it, particularly when it is one training sample among billions. Thus, we ask: Are large language models memorizing bug benchmarks? We systematically evaluate popular LLMs to quantify their susceptibility to data leakage on widely usedarXiv:2411.13323v3  [cs.SE]  31 Mar 2025 1 * * ommited for brevity * * 2\/\/ no search strings found, we are done 3if (textIndex == -1) { 4 return text; 5} 6int start = 0; 7\/\/ get a good guess on the size of the result buffer so 8\/\/ it doesnt have to double if it goes over a bit 9int increase = 0; 10\/\/ count the replacement text elements that are larger 11\/\/ than their corresponding text being replaced 12for (int i = 0; i < searchList.length; i++) { 13 if (searchList[i]==null || replacementList[i]==null) { 14 continue; 15 } 16 int greater = replacementList[i].length() - 17 searchList[i].length(); 18 if (greater > 0) { 19 increase += 3 * greater; \/\/ assume 3 matches 20 } 21} 22\/\/have upper-bound at 20 %increase, then let Java take...patch Prompt input to codegen-multi . Both codegen-multi output and the Defects4J solution. Fig. 1: Excerpt from Defects4J (Lang:Bug 39 ). Given the first lines of the function until line 11, codegen-multi generated lines 12 to 23, matching the benchmark solution. bug benchmarks and raise awareness of the risks associated with using established benchmarks, which may inadvertently inflate performance by testing memorized data. We use multiple metrics to detect potential leakage. First, we investigate whether benchmark data has membership within TheStack , a widely-used used pretraining code dataset. Fol- lowing this, we apply two core metrics for leakage from prior work [9], [10]: Negative Log-Likelihood ( NLL)and 5-gram accuracy .NLL provides insight into model familiarity with code snippets; 5-gram accuracy assesses the model\u2019s ability to reproduce exact sequences. We apply these metrics to both well-known bug benchmarks, and a new dataset of high-quality code repositories from 2024, which we mined from GitHub . This new dataset is less likely to have appeared in models\u2019 training, which allows us to compare model performance between potentially familiar data versus likely novel code. Our findings suggest that older models, in particular codegen-multi , exhibit very high 5-gram accuracy and low NLLon benchmark data, indicating a higher likelihood of mem- orization. Our evidence suggests that newer models trained on more extensive datasets, like LLaMa 3.1 , show less memoriza- tion. Nonetheless, across all metrics, and models, Defects4J \u2014 arguably the most widely-used bug benchmark \u2014 con- sistently exhibits the highest rate of potential memorization. These results underscore the importance of carefully selecting benchmarks to ensure reliable evaluations.TABLE I: Evaluation benchmark statistics, including newer and older benchmarks to assess leakage reduction over time. Benchmark Year # Bugs LOC (k) # Stars Language Defects4J (v1.5) 2019 438 321 736 Java BugsInPy 2020 493 1253 80 Python BugsC++ 2021 209 4297 42 C++ GitBug-Java 2023 199 257 26 Java SWEBenchLite 2023 300 3148 1954 Python New Java Repos 2024 - 132 >100 Java New Python Repos 2024 - 65 >100 Python II. M ETHODOLOGY Figure 2 overviews our methodology, which comprises three major components: (1) data collection (Section II-A, (2) model selection (Section II-B), and (3) evaluation (Section III). A. Data Collection & Filtering. We select widely used bug benchmarks across common pro- gramming languages, gathering ground-truth files containing reference solutions for each bug fix. To provide a likely not- leaked datasets for comparison, we also curated a set of recent open-source repositories from GitHub . To collect benchmarks of interest, we reviewed program-repair.org and selected highly starred benchmarks across three programming languages: BugsCpp [11], Defects4J [4], and BugsInPy [5]. We next included two recent datasets to serve as reference points: GitBug-Java [12], which was recently published to address the leakage issue, and SWEBench-Lite [6], due to its rising popularity for code-related tasks [13], [14], [15]. Table I shows details, including release year, bug count, lines of code, stars, and language. We choose SWEBench-Lite instead of SWEBench due to computational constraints. We collected the ground truth files for each patched bug. To reduce computational costs and prevent bias towards particular files, we removed files with more than 85% overlap to produce a unique sample set. Duplicate files appear when multiple bugs in a dataset occur in the same file (e.g., Defects4J in project Lang , Bugs 1 and 3 affect the same file). We kept the oldest file for consistency. We collected only fixed files since these correspond to the solutions a model may have memorized. We collected a new dataset of 3,214 GitHub repositories written in Java (1,183) and Python (2,031), targeting reposi- tories from 2024 to reduce the likelihood that state-of-the-art LLMs have seen them. To focus on high-quality repositories, we applied a 100-star minimum threshold as a proxy for community interest. To ensure novelty, we used MinHash [16] with Locality Sensitive Hashing (LSH) [17] to filter repos- itories overlapping with existing training data. Due to code duplication, some files in 2024 repositories might overlap with older repositories. Therefore, we included repositories from 2022\u20132023 (>100 stars) to exclude 2024 repositories containing duplicated data. Finally, we randomly sampled 250 files per language to manage computational resources for evaluation. Most used benchmarks from  program-repair .org\/benchmarksDATA COLLECTION DEDUPLICA TIONNEW & UNSEEN  GITHUB REPOSIT ORIES MODEL  SELECTION CodeGen  (6B) CodeLLama (7B) LLama  3.1 (8B) StarCoder 2  (7B) CodeGemma  (7B) Gemma 2  (2B) Mistral (7B)LLama 3.1  (70B) Gemma 2  (27B)LEAKAGE DETECTION 5-GRAM ACC NLL MEMBERSHIP Github repositories written in  Python and Java DEDUPED BENCHMARKS DEDUPED BENCHMARKS intmain (SKY intmain ( int HOW SURPRISED A MODEL  IS WHEN PREDICTING THE NEXT  WORDHOW OFTEN A MODEL ACCURA TELY PREDICTS A SEQUENCE OF N TOKENSFig. 2: Overview of our methodology for detecting leakage. We collected bug benchmarks and unseen repositories from 2024. We evaluated NLLandN-gram accuracy on base models, and analyzed membership of the benchmarks in TheStack . TABLE II: Models used for evaluation, including their training budget in trillions of tokens, number of layers, and cutoff year. Model Tokens (T) Cutoff Year Codegen Multi (6B) [18] 1 2022 CodeLlama (7B) [19] 2.5 2023 LlaMa 3.1 (8B \/ 70B) [20] 15.0 2024 StarCoder 2 (7B) [21] 3.5 2024 Gemma 2 (2B \/ 27B) [22] 2.0 \/ 13.0 2024 CodeGemma (7B) [23] 6.5 2024 Mistral (7B) [24] - - B. Model Selection. We select a combination of models used for fault localiza- tion [25], program repair [26], and vulnerability detection [27]. Table II shows model information.These models are from fam- ilies of well-known code-related models, including codegen- multi ,LLaMa 3.1 ,Gemma 2 ,StarCoder , and Mistral . Note that we focus on open-source base models because our method requires computing the negative log-likelihood (NLL) on sequences, which is generally not possible with closed-source models. We exclude instruction-tuned models and concentrate solely on pretrained models before fine-tuning. Since instruction-tuned models are optimized for conversa- tional formats, n-gram accuracy may be a less suitable metric for measuring memorization in these models. C. Leakage Detection We follow strategies from prior work [9], [10] to evaluate models for potential data leakage. Membership operates at the repository level; Negative Log Likelihood andN-gram accu- racy, at the file level (i.e., the compute model familiarity with a given file). In the bug datasets, these are the fixed (patched) files; in our novel dataset, they are randomly sampled files (Section II-A). Membership : If a repository is included in a widely used pretraining dataset, many models have probably seen that repository\u2019s code. We do not have direct access to, nor knowledge of, the training datasets for all evaluation models. However, we have partial information about the use of pre- training datasets, such as for open-source models, and closed- source models are likely to use them as well. We therefore assess membership via whether a benchmark\u2019s repositories arepresent in TheStack [28], a dataset of permissibly licensed source code in 358 programming languages intended for training and fine-tuning code-based models. Given its size and popularity, several models report having trained on it, such as StarCoder 2 [21]; other closed-source models are likely to also use it. We used the Am I in the Stack tool1on each benchmark repository, across the several versions of TheStack . Negative Log Likelihood ( NLL):NLL evaluates how closely an input sequence aligns with patterns the model has learned during training in terms of how \u201cnatural\u201d the sequence appears to the model. If the model has seen a data point during training, we expect it to have a lower NLL on that point compared to unseen data. If the model has encountered a data point many times during training, NLLis expected to be particularly low (i.e., close to zero) compared to arbitrary code. To compute NLL, we use the reference implementation publicly available on HuggingFace.2Calculating the exact NLL for lengthy sequences is usually impractical because LLMs are trained on the limited context (moreover, we cannot fit an en- tire sequence in memory). Therefore, we split lengthy solution files into overlapping chunks, processed them individually, and combined them using a striding technique. We use strides of 512 tokens when a sequence does not fit into the model\u2019s context window. N-gram accuracy :N-gram accuracy measures the extent to which a model\u2019s output exactly matches a reference sequence at the level of n-grams (i.e., contiguous sequences of nto- kens). High n-gram accuracy indicates that the model\u2019s output closely resembles the reference text, suggesting memorization. N-gram accuracy of 1.0 indicates the model can produce a sequence verbatim. We follow prior work [9] and use 5-grams ( 5-grams strike a balance between compute efficiency and metric accuracy). Since most files cannot fit the context window, we use striding to cover the entire sequence. Following Xu et al. [9], we compute 5-grams from five uniformly distributed starting points per stride. For each starting point, we provide the model with the preceding context, and check whether the predicted string matches ground truth. 1https:\/\/huggingface.co\/spaces\/bigcode\/in-the-stack 2https:\/\/huggingface.co\/docs\/transformers\/en\/perplexity Fig. 3: NLLby model and dataset. NLLis not comparable across models in different families, only across benchmarks within a family. NLLfor other models are consistent with the results displayed. TABLE III: Percentage of repositories in each benchmark leaked in TheStack versions 1.0, 2.0 and 2.1. Benchmark v1.0 (%) v2.0 (%) v2.1 (%) GitBug-Java 61.1 42.6 38.9 BugsInPy 94.1 64.7 64.7 BugsC++ 60.9 60.9 65.2 Defects4J 80.0 80.0 80.0 SWEBench-Lite 83.3 91.7 83.3 III. R ESULTS This section presents results assessing possible leakage of bug benchmarks in base models, using the metrics described in Section II-C: membership in TheStack (Section III-A), Negative Log Likelihood (Section III-B), and 5-gram accu- racy (Section III-C). We also perform a regression analysis of model characteristics, NLL, and 5-gram accuracy (Sec- tion III-D) to better understand characteristics of models that influence data leakage. (Note that we subsequently discuss implications in Section IV, and limitations and threats to the validity of our experiments in Section V.) A. Membership Table III shows benchmark membership in three versions ofTheStack .3The table excludes our new Java and Python data from 2024, as TheStack only includes data to 2023. Of all repositories, the new GitBug-Java benchmark has the lowest membership. TheStack contains high proportions of Defects4J andSWEBench-Lite . While membership does not necessarily mean a model trained on TheStack has seen a specific fixed file (e.g., if a bug- fixing patch was applied after the dataset\u2019s cut-off date), the model may still be familiar with a project\u2019s source code. This familiarity could lead to higher-quality patches or results. This is not inherently problematic but is a critical factor to consider when assessing the model\u2019s potential for generalization. B. Negative Log Likelihood Figure 3 shows NLL values for families of open-source models, allowing us to examine trends in familiarity across 3V1.0 is the initial 3TB of permissively licensed code, 2.0 expands to 15TB of code, and V2.1 eliminates \u201copt-out\" data.benchmarks.Note that Negative Log Likelihood ( NLL) depends on tokenization and architecture. This means we can only directly compare NLLvalues within model families. Figure 3 shows that Defects4J consistently has the lowest NLL across all models. This strongly suggests potential data leakage. This is particularly evident with codegen-multi , which has very low NLL(0.15) for Defects4J . This matches our observations in Figure 1 and suggests that codegen- multi has memorized the Defects4J solutions. We observe comparably low NLL (0.38) on the Gemma 2 27B model for Defects4J relative to other benchmarks and repositories. Interestingly, codegen-multi 6B exhibits low NLL(0.38) on SWEBench-Lite compared to other benchmarks and new data, despite being the oldest model in our evaluation, trained on older data, and the fact that SWEBench-Lite was published recently. This is because the projects in the benchmark existed prior to benchmark publication, as we also saw in the member- ship analysis (Table III). Moreover, although SWEBench-Lite is a new benchmark, the bug fixes date as early as 2017. For all other models, the NLL values are fairly consistent across non- Defects4J benchmarks. As expected, the new repositories we collected exhibit higher NLLcompared to De- fects4J ,BugsCpp ,BugsInPy , and SWEBench-Lite . Evaluation benchmarks are derived from prominent projects and may have been seen at a pretraining time multiple times, unlike our new repositories, which likely were not. Note, however, a potential confound, which is that our new repositories may be different in distribution compared to the models\u2019 training data, which may contribute to higher NLL. Figure 4 visualizes how each benchmark compares against other benchmarks and repositories for each model family, demonstrating how much more \u2018familiar\u2019 a particular model is with a benchmark compared to the others. For example, codegen-multi \u2019sNLLonDefects4J is5.63\u00d7lower than on new Java repositories, and 3.82\u00d7lower than on GitBug- Java . This ratio highlights a significant level of familiarity with Defects4J compared to new repositories. Newer models, particularly LLaMa 3.1 , exhibit relatively consistent NLLvalues across all benchmarks. For example, the NLL ratio between Defects4J andGitBug-Java forLLaMa 3.1 70B is only 1.27, indicating that LLaMa 3.1 perceives Defects4J patches as Defects4J (v1.5)BugsInPy Bugs C++Gitbug JavaSWEBench LiteNew JavaNew PythonDefects4J(v1.5) BugsInPy BugsC++ GitbugJava SWEBenchLite1.00 2.55 3.18 3.83 2.51 5.63 4.86 0.39 1.00 1.25 1.50 0.98 2.21 1.91 0.31 0.80 1.00 1.20 0.79 1.77 1.53 0.26 0.67 0.83 1.00 0.65 1.47 1.27 0.40 1.02 1.27 1.53 1.00 2.25 1.94NLL Ratios for Codegen 6B Multi Defects4J (v1.5)BugsInPy Bugs C++Gitbug JavaSWEBench LiteNew JavaNew PythonDefects4J(v1.5) BugsInPy BugsC++ GitbugJava SWEBenchLite1.00 1.22 1.45 1.27 1.18 1.95 1.28 0.82 1.00 1.19 1.04 0.97 1.59 1.04 0.69 0.84 1.00 0.88 0.82 1.34 0.88 0.79 0.96 1.14 1.00 0.93 1.54 1.01 0.84 1.03 1.23 1.07 1.00 1.65 1.08NLL Ratios for LLama 3.1 70B Defects4J (v1.5)BugsInPy Bugs C++Gitbug JavaSWEBench LiteNew JavaNew PythonDefects4J(v1.5) BugsInPy BugsC++ GitbugJava SWEBenchLite1.00 1.53 1.25 1.47 1.51 2.36 1.53 0.65 1.00 0.82 0.96 0.99 1.54 1.00 0.80 1.22 1.00 1.18 1.21 1.88 1.22 0.68 1.04 0.85 1.00 1.02 1.60 1.04 0.66 1.01 0.83 0.98 1.00 1.56 1.02NLL Ratios for Gemma 2 27B 101 100101Fig. 4: Heatmap illustrating the relative NLLratios across datasets for the codegen-multi ,LLaMa 3.1 , and Gemma 2 . Each cell represents the ratio of the NLLfor the dataset in the column to that of the dataset in the row. For example, the NLLfor new Java repos is 5.63\u00d7higher than that for Defects4J . Darker colors correspond to higher ratios. only slightly more predictable than GitBug-Java . This is expected, as the newer LLaMa 3.1 family of models was trained on significantly more data and is thus less prone to data memorization. Specifically, the LLaMa 3.1 family was trained on30\u00d7more tokens than codegen-multi . C. 5-Gram Accuracy Figure 5 shows 5-gram accuracy results, a complementary assessment of potential model memorization (full tables, in- cluding other models are in the appendix). Note that, due to differences in model vocabularies and tokenization, interpre- tation of 5-gram accuracy can differ across models. Defects4J consistently exhibits the highest 5-gram accu- racy across all model families. Conversely, 5-gram accuracy onGitBug-Java is relatively similar to that of new repositories for all models, which aligns with the expectation that most repositories in the GitBug-Java benchmark were not included in pretraining data (as detailed in Table I). For example, both codegen-multi and Gemma 2 show significantly higher 5-gram match differences between Defects4J and new repos- itories (i.e., 34percentage points and 14percentage points, respectively). Moreover, codegen-multi achieves 82% 5-gram accuracy onDefects4J , strongly suggesting that it has likely memorized much of the benchmark\u2019s solutions. As expected, new repositories generally exhibit lower 5- gram accuracy across all models, with averages of 47% and 48% for Java and Python , respectively. These values happen likely due to the presence of common coding patterns [29]. Conversely, and in line with our NLL findings, codegen- multi shows a notably high 5-gram accuracy onSWEBench- Lite , even though it is a recently published dataset, as it incorporates older data. When it comes to LLaMa 3.1 family, both LLaMa 3.1 70B and 8B exhibit consistent 5-gram accuracy across bench- marks, which could suggest that these models are less prone to memorization due to their exposure to substantially larger datasets during pretraining. However, it is important to notice that 5-gram accuracy alone may not conclusively prove an absence of memorization (contrary to high n-gram accuracy which reliably indicates strong pattern retention). For exam- ple, Figure 6b shows an example where LLaMa 3.1 70B isprompted to predict a patch from BugsInPy (fastapi :Bug 12). On this file, one of the expected 5-grams is \u201c __init__(self, \" , and here LLaMa 3.1 predicts \u201c __init__(\\nself, \" instead. This causes the 5-grams not to match, even though the content is the same. Therefore, it is crucial to evaluate models by considering NLL,5-gram accuracy , and membership as whole, as no single metric provides a complete picture of data leakage. D. How Do Model Characteristics Influence Risk of Leakage? To gain deeper insights, and using the data collected during the study, we estimate regression models for the average values of NLL and 5-gram accuracy . The regressions allow us to explore relationships that metrics alone cannot reveal. Using regression analysis, we can identify factors such as training parameters and budget as significant influences on these metrics. Therefore, we use a simple mixed-effects linear model to predict the average NLLand 5-gram accuracy . As predictors, we include the models\u2019 number of parameters and number of tokens used during pretraining (i.e., training budget ), which serves as a proxy for the unique token count. We acknowledge that reporting the exact number of unique tokens would be a more precise metric, but such data is often unavailable. We also account for potential variability caused by differences in the datasets and tokenizers by including dataset andtokenizer type as random effect. Table IV shows results. In a linear regression, the intercept represents the predicted value of the dependent variable (averages of NLLand 5-gram accuracy ) when all predictors are at their reference values. To make the interpretation easier, we centered the predictors around codegen-multi values. Therefore, the reference value ofparameters is 6B and the reference value of training budget is 1T tokens. The regression coefficients represent the change in the dependent variable for a one-unit increase in the predictor, holding other predictors constant. ForNLL, the predicted average is 0.744 when at reference level, i.e., parameters = 6B and training budget = 1T. For every 1B increase in number of parameters (above 6B), the predicted NLLaverage decreases by 0.002units (while holding training budget constant, at 1T). Similarly, for every 1T tokens increase in training budget, predicted NLLdecreases by 0.014units. For Fig. 5: 5-gram accuracy by model and dataset. Due to space constraints, we selected a sample of the most relevant models. 5-gram accuracy for other models are consistent with the results displayed. TABLE IV: Summary of regressions tests for negative log like- lihood ( NLL) and 5-gram accuracy . We report the coefficient estimates with their standard errors in parentheses. NLL 5-gram Intercept 0.744 (0.094) *** 0.465 (0.049) *** Parameters -0.002 (0.001) * 0.001 (4.0e\u22124) * Training budget -0.014 (0.004) ** 0.006 (2.4e\u22123) * Note: *** p-value < 0.001, ** p-value < 0.01, * p-value < 0.05 5-gram accuracy , the predicted average is 0.465 when predic- tors are at reference level. For every 1B increase in number of parameters, the predicted 5-gram accuracy increases by 0.001units. Similarly, a 1-unit increase in the training budget leads to an increase of 0.006in5-gram accuracy . All reported coefficients have statistical significance. Regression results reveal consistent trends across model families. For example, both LLaMa 3.1 8B and 70B were trained on the same data with a training budget of 15T tokens. However, the 70B model is approximately ten times larger than the 8B model, leading to an overall increase in 5-gram accuracy across all benchmarks (as shown in Figure 5) and decrease in NLL. Similarly, within the Gemma 2 family, the 2B model was trained on 3T tokens, while the 27B was trained on 12T. Here, we observe the same trend: average 5-gram accuracy increases in the 27B model and NLLdecreases. Regression results also imply that models with more pa- rameters tend to exhibit higher n-gram accuracy and, con- sequently, memorize more. For example, Figure 6b shows that LLaMa 3.1 70B, which has the same training budget as LLaMa 3.1 8B, accurately predicts strings, class names, and if-statements in contexts where these predictions might not be immediately apparent, which may indicate memorization. IV. D ISCUSSION Our evaluation provides compelling evidence that data leak- age is an especially significant issue for Defects4J (V1.5). This is evident from the lower NLLvalues and higher 5-gram accuracy . In particular, codegen-multi achieves 82% 5-gram accuracy onDefects4J , while both CodeLlama 7B andGemma- 2 27B attain 64%. Moreover, given that Defects4J is incorpo-rated into the widely-used pretraining dataset ( TheStack ), with 80% membership, eliminating this leakage in future models is likely nearly impossible. We also observe similarly low NLL values in SWEBench-Lite forcodegen-multi . Newer benchmarks BugsInPy and BugsCpp exhibit lower leakage risk in almost all models. A smaller percentage of their repositories are indexed in the latest version of TheS- tack . While BugsInPy andBugsCpp exhibit slightly lower NLL compared to new repositories, their 5-gram accuracy andNLL values are not significantly different from newer, more-likely- unseen benchmarks like GitBug-Java . This suggests that, for now, researchers can use these benchmarks with a relatively low risk of data leakage. However, as pretraining datasets continue to evolve, we recommend that researchers regularly assess TheStack membership, 5-gram accuracy andNLLval- ues of these benchmarks, especially compared to more recent data, to monitor and mitigate potential data contamination. We also observe that the LLaMa 3.1 family seems to exhibit lower memorization of benchmark solutions. Nonetheless, we still observed cases where LLaMa 3.1 70B outputs solution files despite little context (e.g., Figure 6). We suggest researchers consider supplementing their eval- uations with more recent benchmarks such as GitBug-Java . Benchmarks like GitBug-Java , which focus on recent bugs and patches, are less likely to have been included in pretrain- ing datasets compared to established benchmarks. Leveraging these newer benchmarks can provide more reliable evaluations for assessing model\u2019s capabilities. V. L IMITATIONS AND THREATS Data Collection: Despite efforts to filter out older GitHub repositories when collecting new data, we anecdotally ob- served instances where files appeared to be adaptations of existing files (e.g., from 2018) Our filtering process may not have perfectly excluded legacy code. We moreover cannot guarantee that the new repositories are identically distributed compared to those in the benchmarks. To mitigate this issue, given that the repositories in the benchmarks tend to be highly recognizable projects, we applied a >100-star filter to argue that the selected projects are of comparable quality. 1 * * omitted for brevity * * 2 if scheme.lower() != \"bearer\": 3 if self.auto_error: 4 raise HTTPException( 5 status_code=HTTP_403_FORBIDDEN, 6 detail=\"Invalid authentication 7 credentials scheme\") 8 else: 9 return None 10 return HTTPAuthorizationCredentials(scheme=scheme, 11credentials=credentials) self.model.parse(credentials) 12 13class HTTP Digest(HTTPBase) BearerModel(BaseModel):: 14 def __init__(self, *, bearerFormat: str = None, 15 * * omitted for brevity * * Prompt input to LLaMa 3.1 8B. Extra code generated. Ground truth not generated.Matching Ground Truth & Predicted by LLaMa 3.1 8B. (a) Example of generation by LLaMa 3.1 8B .1 * * omitted for brevity * * 2 if scheme.lower() != \"bearer\": 3 if self.auto_error: 4 raise HTTPException( 5 status_code=HTTP_403_FORBIDDEN, \\n 6 detail=\"Invalid authentication credentials\" \\n 7 ) 8 else: 9 return None 10 return HTTPAuthorizationCredentials(scheme=scheme, 11 credentials=credentials) 12 13class HTTPDigest(HTTPBase): 14 def __init__( \\nself, \\n*, \\nqop: str = None \\n, 15 * * omitted for brevity * * Prompt input to LLaMa 3.1 70B . Extra code generated. Ground truth not generated.Matching Ground Truth & Predicted by LLaMa 3.1 70B. (b) Example of generation by LLaMa 3.1 70B. Fig. 6: Example patch of a bug from BugsInPy ( fastapi :Bug 12). We prompt each model with the 30 lines prior to the patch. We highlight the extra code generated not in the ground truth and ground truth code not generated by the model. The remaining lines of the examples are omitted for brevity.patch patch Train + Test Splits : For benchmarks like Defects4J (V1.5) andBugsInPy , the patch files and buggy files are very similar (typically a patch involves only changing a small number of lines). LLMs may have only seen the train split of these benchmarks at pretraining time. This would result in high 5- gram accuracy and low NLLon patch files, even if only buggy files were leaked. Nonetheless, we observed multiple cases where models output patched files verbatim. We mitigate this by looking at trends in NLL and 5-gram accuracy rather than absolute numbers in our analysis. Forgetting : The findings presented in this paper primarily address leakage in the context of base models. Empirical results show that models can \u201cforget\u201d portions of their pre- training data during fine-tuning [30]. That said, while full- scale model fine-tuning may reduce leakage risks, more recent fine-tuning strategies\u2014such as the addition of adapter layers\u2014 often \u201cfreeze\u201d pretrained weights. This practice can inadver- tently increase the likelihood of data leakage.  [Text truncated due to context length limits]\nDocument 5\nThe Sustainability Face of Automated Program Repair Tools MATIAS MARTINEZ, Universitat Polit\u00e8cnica de Catalunya, Spain SILVERIO MART\u00cdNEZ-FERN\u00c1NDEZ, Universitat Polit\u00e8cnica de Catalunya, Spain XAVIER FRANCH, Universitat Polit\u00e8cnica de Catalunya, Spain Automated program repair (APR) aims to automatize the process of repairing software bugs in order to reduce the cost of maintaining software programs. While APR accuracy has significantly improved in recent years, its energy impact remains unstudied. The field of green software research aims to measure the energy consumption required to develop, maintain, and use software products. Our main goal is to define the foundation for measuring the energy consumption of the APR activity. We state that an environmentally sustainable (or green) APR tool achieves a good balance between the ability to correctly repair bugs and the amount of energy consumed during such process. We measure the energy consumption of ten traditional APR tools for Java and eleven fine-tuned Large-Language Models (LLM) trying to repair real bugs from Defects4J. The results of this study show the existing trade-off between energy consumption and repairability. In particular, APR tools such as TBar and RepairLlama repair more bugs than other approaches at the expense of a higher energy consumption. Other tools, such as SimFix and the LLM CodeT5-Large, provide a good trade-off between energy consumption and repairability. We also present guidelines consisting of a set of recommendations for developing greener APR. CCS Concepts: \u2022 Hardware !Power and energy ; \u2022Software and its engineering !Softwaremaintenance tools . Additional Key Words and Phrases: Automated Program Repair, Software Sustainability, Energy Consumption of Software Tools, Green Computing 1 INTRODUCTION In the last decade, following current societal needs, software sustainability has emerged as research field [ 1\u20133]. It consists of different dimensions, including environmental sustainability, human sustainability, and economic sus- tainability. In this paper, we focus on environmental sustainability, defined as \u201chow software product development, maintenance, and use affect energy consumption and the consumption of other natural resources. [\u2026] This dimension is also known as Green Software\u201d [4]. The study of environmental sustainability is of paramount importance in the realm of automated software engineering, i.e. the application of computation to software engineering activities with the objective of partially or fully automating these activities, thereby significantly increasing both quality and productivity [ 5]. The reason is that such computations demand high computational resources, especially nowadays: the advent of Big Data, Internet of Things, Machine Learning, Generative AI and other contemporary methods and technologies, supported by high-performance cloud-based infrastructure, allows researchers to design and implement complex, sophisticated automated solutions that were not feasible in the past, which ultimately implies high levels of energy consumption. Authors\u2019 Contact Information: Matias Martinez, Universitat Polit\u00e8cnica de Catalunya, Barcelona, Spain, matias.martinez@upc.edu; Silverio Mart\u00ednez-Fern\u00e1ndez,UniversitatPolit\u00e8cnicadeCatalunya,Barcelona,Spain,silverio.martinez@upc.edu;XavierFranch,UniversitatPolit\u00e8cnica de Catalunya, Barcelona, Spain, xavier.franch@upc.edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and\/or a fee. Request permissions from permissions@acm.org. \u00a9 2025 Copyright held by the owner\/author(s). ACM 1557-7392\/2025\/6-ART https:\/\/doi.org\/10.1145\/3744900 ACM Trans. Softw. Eng. Methodol.    2 \u2022 M. Martinez et al. One of these automation-prone activities is bug fixing through automated program repair (APR) tools. APR is becoming popular due to the high economic cost that fixing bugs entails [ 6,7]. APR tools take as input a buggy program and deliver one or more patches that repair the bugs, when they are found. In recent years, numerous APR tools have been capable of automatically repairing real bugs (e.g., [ 8\u201310]), mainly from open-source libraries [11\u201313]. Furthermore, APR has started to be adopted in industry [14\u201316]. To evaluate APR tools, researchers usually report two metrics [ 17]:a)Total number of bugs that are repaired with aplausible patch (that is, a patch valid w.r.t. the correctness oracle), and b)Total number of bugs repaired with at least one correctpatch. However, to our knowledge, thereisnopreviousworkintheAPRareathatfocusesonthestudyof theenergyconsumptionofAPRtools . The APR research community has been oriented towards accuracy, with a strong focus on obtaining state-of-the-art results, as happened in the AI community [ 18]. According to current societal demands and the computational demanding nature of state-of-the-art techniques, we argue thatAPR researchers should also consider energy consumption as a main driver when designing and implementing APR tools , carefully analyzing whether a minor gain in accuracy justifies high levels of energy consumption. The absence of methodologies for measuring energy consumption and benchmarkings in APR makes it challenging to evaluate how innovations in APR impact the overall energy consumption of the repair process. This paper aims at addressing this gap. The inclusion of energy as a driver of APR research is important for several reasons. First, APR tools use expensive correctness oracles, executed each time a candidate patch is synthesized. Test suites are examples of them. As APR tools synthesize a considerable amount of patches before finding the correct one [ 19], the amount of energy up to that point could be considered wasteful. Second, even with the impressive progress of the field, state-of-the-art APRs do not achieve high accuracy and are complementary (as shown, for example, in Fig. 2 from [20]). This means that, in order to achieve high accuracy, a practitioner aiming to deploy APR would need to consider not just one tool, but several, increasing the energy required to repair a bug. Third, APR tools could be triggered regularly; for example, a setup that connects APRs with CI\/CD (e.g., via a bot [ 21]) would require to invoke APRs tools for each failed build caused by test cases with failure or error. For example, the study by Moritz et al. [22] on Travis CI builds reports a median of 10.3% for builds with failed tests in Java projects. Triggering APRs for each of these failed builds would require a large amount of repair attempt execution. Unfortunately, the energy consumed by these repair attempts is unknown. To achieve green software development, we believe that it is essential to understand the energy consumption of APR activity. This work is a first step in that direction. The ultimate goal of our research, expressed following GQM [ 23] is to: Analyze APR tools with the purpose of measurement and analysis with respect to energy consumption from the point of view of software developers in the context of bug repair . To achieve this goal, we first measure the energy consumed by APR tools to find plausible and correct patches. We focus on two families of APRs: a) Traditional tools (i.e., search-, constraint- and template-based), and b) Large- Language Model (LLM)-based tools. We also focus on the correctness of generated patches, in order to quantify the energy \u2018wasted\u2019 on generating overfitting (incorrect) patches. Then, we study the relationship between energy consumed and twometrics studied in previous work as proxies ofrepair efficiency , which is one aspect of the APR performance [ 19]:a)Repair time, studied by [ 24] in the context of APR, and b)Number of candidate patches (NCP), defined by [ 25], and studied by Liu et al. [ 19] in the context of APR. With this study, we want to know whether these two metrics can be used as a proxy for energy consumption. In addition, we study whether repairing hard and important bugs requires more energy. For this, we use the data provided by Motwani et al. [26]. We evaluate on the Defects4J benchmark [ 11] the energy consumption of ten publicly available traditional repair tools (including TBar [ 27], Prapr [ 24] and SimFix [ 28]), ten fine-tuned large language models (including InCoder [ 29] and CodeGen [ 30]) used by Jiang et al. [ 31], and RepairLlama [ 32], a repair approach based on ACM Trans. Softw. Eng. Methodol.    The Sustainability Face of Automated Program Repair Tools \u2022 3 CodeLlama [ 33], which introduces LoRA [ 34], a state-of-the-art fine-tuning technique named PEFT (parameter- efficient fine-tuning), for program repair. The results of this experiment show a moderate positive correlation between energy consumed and number of bugs correctly repaired. The traditional approach TBar, based on a collection of existing templates, is the most energy-consuming approach to repair Defects4J bugs (118 kWh). In total, 28 bugs were correctly fixed by TBar, more than any other traditional APR tool. Among LLM-based tools, the largest models (those with \u00196-7 billion parameters) consume more energy due to their hardware requirements (they require several GPUs). In particular, RepairLlama consumes 1,263 Wh to repair 61 bugs correctly. Tools based on LLM with similar size (6 billons parameters) such as InCoder and CodeGen, consumes much less energy than RepairLLama (both \u0019820) but also repair less bugs, 37. Smaller models such as CodeT5Large and PLBARTLarge show a good balance between energy (608 Wh and 567 Wh, respectively) and repairability (31 and 30 repaired bugs, respectively). We also focus on the impact of overfitting patches on the overall energy consumed by the repair tools. In traditional tools, approximately 6% to 40% of the energy is consumed by repair attempts that yield only overfitting patches. In contrast, for LLM-based tools, the energy spent on repair attempts that produce solely overfitting patches ranges from approximately 5% to 17%. Based on the insights from our research, we propose a set of guidelines with recommendations for developing greener APR. These recommendations cover various types of APR, including LLM-based approaches, and address multiple stages of the repair process, such as patch generation, model training, and patch validation. The paper continues as follows. Section 2 presents a background on APR and energy consumption. Section 3 presents the methodology to answer the research questions. Section 4 responds to the RQs. Section 5 presents a discussion on this study. Section 6 presents guidelines with recommendations for greener APR. Section 7 presents the threats to validity. Section 8 discusses the limitations of this work. Section 9 concludes the paper and highlights directions for future research. This is an extended version of a two-page article that introduces the problem of APR and energy consump- tion [35]. Data availability statement. All data generated in this experiment are available as an appendix at https: \/\/github.com\/UPCBarcelonaTech\/green_apr. 2 BACKGROUND 2.1 Automated program repair 2.1.1 Approaches and tools. During recent years, several APR approaches and tools have been proposed with the goal of automatically fixing bugs in software applications [ 36,37]. As a recent controlled experiment with 40 developers shows, proposing correct patches to developers helps successfully repair defects and speeds up debugging time [38]. AfamilyofAPRapproachesis test-suite based program repair approaches[ 8],whichusetestcasesasaproxyfor the program specification. This family is further subdivided into several types. On the one hand, the traditional1 family includes: (a) Search-based APRs, such as GenProg [ 8], PAR [40], SPR [9] and CapGen [ 41]. These APRs navigate the search space composed of candidate patches to find the solution, i.e. the correct patch. Some of them use information previously extracted from other programs (e.g., [ 42,43]) to speed up the search [ 44] or even in trained models (e.g., [ 45]) to filter out incorrect candidates; (b) Semantic-based APRs, such as SemFix [ 46], Nopol [47] and Angelix [10], which use symbolic execution and constraint solving. On the other hand, there exists the Neural network-based APR family, which includes, for instance, Se- quenceR [ 48], Coconut [ 49], Recoder [ 50], SelfAPR [ 51], KNOD [ 52] and RewardRepair [ 20]. These APRs rely on 1We adopt the term traditional for these approaches, as done by two recent surveys: [39] and [37]. ACM Trans. Softw. Eng. Methodol.    4 \u2022 M. Martinez et al. machine learning to generate patches (e.g., using sequence-to-sequence learning). More recently, researchershave proposed LLM-based approaches that, when pre-trained in source code, show good results in fixing bugs [ 31,53]. Also, conversational chatbots, such as ChatGPT, are capable of proposing a fixed version of small buggy pro- grams [54]. 2.1.2 Adoption of APR. APR has also gained traction in the industry. Practitioners have conducted experiments and implemented APR tools in real-world industrial contexts. Some examples are Meta\u2019s tool, Getafix [ 15], and Bloomberg\u2019s tools, Fixie [ 14] and B-Assist [ 55]. For instance, an experiment with Bloomberg\u2019s software engineers demonstrated that suggestions provided by B-Assist were accepted 74.56% of the time, either directly or after modification, through the GitHub user interface [ 55]. A recent survey conducted with 337 software practitioners [ 56] reveals that the majority are either aware of or actively use APR tools. Among the tools mentionedbyrespondents,themostfrequentlycitedwasAstor[ 57],whichincludesapproachessuchasjGenProg and jMutRepair, both studied in our paper. However, the survey also highlighted several disadvantages of APR tools. The most significant concern, reported by 196 respondents, was that these APR tools may overlook rare or uncommon behavior. Additionally, 177 respondents noted that the maintenance cost of APR tools can be high. 2.1.3 Cost and Efficiency of APR. The cost of APR has been studied previously in a few previous works. For example, Le Goues et al. [ 58] studied the monetary cost of repairing C bugs using GenProg [ 8]. Other works have studied the efficiency of program repair. For example, Liu et al. [ 19] use the Number of Candidate Patches (NCP), metric originally used by to measure the efficiency of repair approaches [ 25]. Nevertheless, beyond the progress made in the field [ 37,39], to our knowledge, there are no previous works focusing on the energy consumption of APR. Moreover, even there are different studies on the energy consumption of LLMs, in both training and inference phases, to our knowledge, there is no previous work on the energy profile of LLMs used on program repair (for example, energy of patch inference using LLMs). 2.1.4 Patch Overfitting. Preliminary experiments with APR, such as Smith et al. [ 59], Martinez et al. [ 60], Qi et al. [61] and Le et al. [ 62] have detected one of the initial limitation of APR: a considerable portion of generated patchesareincorrect. Thesepatchesareknownas Overfitting patches.Just Correctpatchesarethosethat correctly repair a bug. Overfitting patches are not accepted by developers. APR tools rely on test cases from project under repair to identify bugs and validate fixes. Unfortunately, a candidate patch passing all test cases is Plausible , but that does not guarantee correctness: it could also be Incorrect . This can happen in scenarios not covered by the tests. For example, an overfitting patch may produce an unexpected output for an input not included in the test cases, that is, it overfitsthe available tests and breaks untested but desired functionality [59]. To show the real effectiveness of an APR, researchers introducing new repair approaches have reported the amount of both plausible and correct generated patches. To illustrate the magnitude of the problem, the study by Liu et al. [ 19] shows that TBar, one of the most effective APR tools, produced 24 correct patches out of 72 plausible ones: 66.6% of the generated patches are incorrect. Initial analyses of overfitting were performed by humans by annotating patches as either overfitting or correct (e.g [19,60]). Additionally, another type of analysis generates additional test cases not included in the buggy program, thus not used for validating candidate patches (e.g. [ 59,63]). Then, different patch overfitting detection systems have emerged, such as PatchSim [ 64] and DiffTGen [ 65], xTestCluster [ 66], all of these based on test case generation. The main idea of these approaches is to generate new inputs that allow to differentiate the behaviors of overfitting patches from correct ones. Wang et al. [ 67] conducted a preliminary empirical study on the effectiveness of automated patch overfitting detection, while Petke et al. [ 68] studied the magnitude of the overfitting problem. ACM Trans. Softw. Eng. Methodol.    The Sustainability Face of Automated Program Repair Tools \u2022 5 Then, researchers have trained ML models using patches labeled from previews work. These ML-based ap- proaches, given a plausible patch as input, produce a binary decision (correct or overfitting) and\/or return an overfitting score. Some of these ML-based approaches are ODS [ 69], Shibboleth [ 70], CACHE [ 71], and Tian et al. [72]. At the same time, repair techniques have incorporated mechanisms to avoid overfitting patches, such as Prophet [ 45], ObjSim [ 73] and CapGen [ 41] which incorporate patch ranking module (in the case of Prophet trained from labeled patches) to rank correct patches at high positions before incorrect plausible ones. In this paper, we study the impact of overfitting patches on the energy consumed by an APR tool. 2.2 Energy consumption of software Energy consumption has been investigated in the scope of different types of systems, such as mobile apps [ 74], software product lines [ 75] and more recently, ML-based systems [ 76]. As a matter of example, Strubell et al. quantified the approximate environmental costs of training a variety of successful neural network models [ 77]. They estimated that a human life per average implies 11,023 CO2emissions in one whole year whereas training one big Transformer model with neural architecture search emits 626,155 CO2. In other work, Schwartz et al. reported that the cost of training ML models (in terms of the amount of computing resources used) as required by state-of-the-art techniques has increased by a factor of 300.000x in only 6 years, doubling every few months [ 18]. This increase is primarily due to modeling and algorithmic choices, over hardware considerations [ 18], which has motivated several researchers to focus on software when searching for energy consumption reduction [76, 78]. Hort et al. presented a literature study on the energy use of language models for source code [ 79]. Among different software engineering tasks, they studied papers on program repair and report that just four works on that field ([ 80], [48], [81] and [82]) provide details about a)hardware use for training the model, and b)time and energy spend for training. More recently, Shi et al. presented a roadmap on LLMs for software engineering tasks [83]. However, none of them focus on the energy consumption of the repair process (i.e., the inference and the validation of patches), as we do in this paper. Moving towards the ecological transition, software development cannot only target accuracy, but energy efficiency as well. In this context, both traditional and state-of-the-art neural network-based (including LLM- based)APRtoolsshallconsidertheoptimizationsoftheirparametersandtheamountsofenergyandtheresources used. However, this is not easy because software developers lack critical information and knowledge about software energy consumption [84, 85]. 3 METHODOLOGY In this paper, the type of investigation we choose to carry out the empirical study is Technology-oriented Experi- ment[86]. This presents the method applied to carry out the experiment that allows us to answer the research questions, presented in Section 3.1.1. Figure 1 shows an overview of the experiment process. Each gray box corresponds to an activity explained in the remaining subsections. 3.1 Experiment Scope 3.1.1 Research Questions. The research goal defined in Section 1 breaks down in the following research questions (RQs) about energy consumption of ARP tools: (1)What is the energy consumption of APR tools on Defects4J? . This RQ aims to obtain the energy profile of APR tools in the task of repairing real bugs. (2)Which APR tool achieves the best energy efficiency in correctly fixing bugs This RQ aims to investigate the trade-off between energy consumption and the repairability of each tool. ACM Trans. Softw. Eng. Methodol.    6 \u2022 M. Martinez et al. Bug APR  toolRepair  Attempt  Wattmeter  Patch  Assessment  (Section 3.5) Energy  Computation  (Section 3.6) Experiment Execution  (Section 3.3) Tools and Bugs  Selection  (Section 3.2) Analysis and  Interpretation  (Section 3.7)  Power + Energy  (Wh)  Data Validity  Evaluation  (Section 3.4) + Energy  (Wh)  Defects4J  TBar  PrAPR  RepairLlama Incoder6B \u2026Patches  Experiment  Scoping  (Section 3.1) RQs Answers to  the RQ  (Section 4)  Fig. 1. Experiment process for measuring energy consumption of APR tools. (3)What is the impact of patch overfitting on energy consumption of APR? This RQ aims to measure the amount of energy that is destined for a successful repair and the amount that is wasted in unsuccessful repairs with overfitting patches. (4)To what extent does the energy consumed to find a patch correlate with: a) the repair time?, b) difficulty to fix a bug, c) the number of patch candidates (NPC) and d) project under repair? This RQ checks whether there is a correlation between energy and other metrics that represent the repair time, efficiency, difficulty to repair and project under repair. 3.1.2 Variables. Our experiment has two independent variables: 1)Bug, and 2)APR tool; and the following dependent variables: a)number of bugs with plausible patches, b)number of bugs with correct patches, c)time and energy spent to find the first plausible patch, d)time and energy spent to find the first correct patch, e)energy spent during a repair attempt. 3.2 APR Tools and Bugs Selection In this section we define the criteria to select the APR tools and bugs of our experiment. We decide to study the energy consumption of tools capable of repairing Java bugs. For this reason, we choose the benchmark named Defects4J [ 11], which is a benchmark of Java written buggy programs, commonly used to evaluate the repairability of Java bugs (e.g., [ 19,87]. We focus on the version 1.2 a benchmark composed of 395 Java bugs. Each of these bugs is exposed by at least one test case. A repair attempt is the execution of a repair tool (e.g., TBar) on a particular bug from Defect4J (e.g., Math-70) under configuration given to the tool as input. With respect to the APR tools, we focus on two families of tools that have produced state-of-the art results over the last 10 years: 1) traditional repair tools (inc. search-based, constraint-based and template-based), and 2) LLM based repair tools . We now describe the selection process for the tools of each family. Traditional repair tools. We apply the same criteria as Durieux et al. [ 87] and Liu et al. [ 19], which include: 1)availability of the tool, 2)availability of the source code of the tool (e.g., on a code platform such as Github or Gitlab), 3)the tool receives as input the buggy program to be repaired, but does not receive any additional information about the bug. Following the above criteria, we select in this study the following traditional tools: Avatar [ 88], FixMiner [ 43], Nopol [47], SimFix [ 28], TBar [ 27], Dynamoth [ 89], jGenProg [ 57] (implementation of GenProg [ 8] for Java bugs), jMutRepair [ 60], and kPAR [ 40] (implementation by [ 90]). As suggested by their authors, SimFix [ 28] is used instead of the ACS tool [ 91]. We also add PRarp [ 24], which was not considered in [ 19,87]. We discard repair tools ACM Trans. Softw. Eng. Methodol.    The Sustainability Face of Automated Program Repair Tools \u2022 7 implemented in the ARJA framework (which include ARJA, KALI-A, GenProg-A and RSRepair, among others) because, after inspecting their patches from [ 87], we found a large amount of non-plausible ones. Furthermore, given the limited computational resources we have to execute our large-scale experiment (see Section 3.3.3), we discard Java implementations of the Kali approach [ 61] and Cardumen [ 92] because, as observed by [ 19,87], they have a lower ability to find correct patches. Large language model (LLM)-based repair tools. A recent new family of APR approaches is based on neural networks, whose ability to repair Java bugs has been shown in a number of studies (e.g., [ 20,48\u201352,93,94]). The pioneer approaches of this family trained the networks from scratch, using a large dataset of real bug fixing (e.g., [48]). More recently, researchers proposed new approaches based on pre-trained LLMs (e.g., [ 31,53]) or chatbots (e.g., [54, 95]). In this study, we represent the family of neural network-based approaches with the tools proposed by two works. First, we choose the work from Jiang et al. [31] for the following reasons: \u000fProvides usable tools that allow to reproduce their experiment. \u000fUses ten open-source code LLMs from four families of large models: InCoder from Meta [ 29], CodeGen and CodeT5 [96] from SalesForce [30], and PLBART [97]. \u000fFine-tunes these 10 models using a dataset of 143,666 bug fixes, provided by Zhu et al. [ 50]. These fine-tuned models are also publicly available. \u000fThefine-tunedmodelsachievestate-of-the-artresults,improvingtheperformanceofotherneuralnetwork- based approaches, including RewardRepair [20], CURE [94], RECODER [50] and KNOD [52]. The ten LLMs used by Jiang et al. are: 1)CodeGen-350m (350 millions parameters), 2)CodeGen-2B (2 billions), 3)CodeGen-6B (6 billions), 4)CodeT5-small (60 millions), 5)CodeT5-base (220 millions), 6)CodeT5-large (770 millions), 7)InCoder-1B (6.7 billions), 8)InCoder-6B (1.3 billions), 9)PLBART-Base (140 millions), 10)PLBART- Large (400 millions). Secondly, we use RepairLlama from Silva et al. [32] for the following reasons: \u000fRepairLlama is built on top of CodeLlama [ 33] a state of the art LLM for code related task. CodeLlama is not included in the work from Jiang et al. as it was released after their work. \u000fApplies the state-of-the-art fine-tuning technique named PEFT (parameter-efficient fine-tuning) for program repair. Instead of fine-tuning the entire network as done by Jiang et al., PEFT means that Silva et al. just fine-tune a small portion of the network, the adapters. In this work, we use the RepairLlama version IR4 x OR2 which is the one that achieves the highest number of correct patches according to [32], and is built on top CodeLlama7B (7 billion parameters). 3.3 Experiment Execution 3.3.1 Tools preparation. Before launching the experiment, we need to modify some of the tools. By default, publicly available implementations of TBar, Avatar, FixMiner, and kPar, stop when the first plausible patch is foundoraftertryingafixednumberofcandidatepatches(e.g.,10.000forTBar).Asweareinterestedincomputing the energy to repair correctly a bug, and the first patch found can be plausible but incorrect, we modified the source code of these tools with the goal of continuing the search after the first patch found, until the execution reaches a timeout. For the other traditional tools (e.g., Nopol, jGenProg), we indicate via the provided input arguments to continue the search. We also modify all traditional and LLM-based tools considered in this study to register the timestamp corre- sponding to the instant a plausible patch is found. This moment corresponds to the end of the validation of a patch. This timestamp allows us to compute the energy used from the beginning of the repair attempt to the ACM Trans. Softw. Eng. Methodol.    8 \u2022 M. Martinez et al. moment the patch is found and validated. We also modify the code to register the number of candidate patches synthesized (e.g., those that do not compile) in order to compute the metric NPC [25].2 3.3.2 Experiment setup. In order to execute LLM-based tools, we use the scripts provided by Jiang et al. and from Silva et al. to repair Defect4J bugs using the fine-tuned version of these models, also provided by the authors3. We do not change any parameter or hyperparameter. By default, their script queries the LLMs models to generate ten candidate patches per buggy line. For each of these patches, the script compiles it, applies it to the buggy project, then executes the initially failing test cases, and finally executes the test suite provided by the buggy program. We modify their scripts to register the timestamps corresponding to the start and end of each of these phases. Experiments on traditional APRs (e.g., [ 19]) usually have different setups than experiments on LLM-based APRs (including Jiang et al. [ 31] and Silva et al.[ 32]). Two of the main differences are the following. First, the considered LLM-based tools do not execute the fault localization phase; they give as input to the model only the buggy location. That is known as perfect fault localization [19]. In the case of Jiang et al. the input is the buggy line (i.e., the line where the human-written patch is located) and for Silva et al. is the buggy function. In contrast, the traditional tools studied in this paper do not require the buggy line(s) as input. Consequently, they apply a fault localization phase in order to discover potential locations on the code (aka suspicious code) where candidate patches could be applied. This setup is known as Normal Fault Localization [19] Second, Jiang et al. focus on a subset of bugs, namely single-hunk bugs, because, as they mention, the best deep learning-based APR techniques are all specially designed for Java single-hunk [ 31]. Note that RepairLlama from Silva et al. is able to repair multi-hunk bugs, but these hunks must be located in the same function. For these reasons, to avoid unfair comparison, we analyze separately the energy consumption of traditional approaches and LLM-based and traditional approaches, not aiming to compare the results between these two families of approaches. 3.3.3 Executionplatform. Weperformrepairattemptsonahigh-performancecomputing(HPC)clustercomposed of two families of identical nodes: 1)nodes with CPU (Intel Xeon Gold 5220, 96 GB RAM); 2)nodes with CPU and GPU (Intel Xeon E5-2698 v4, 512GB RAM, 8 GPUs Tesla V100-SXM2-32GB). Given the execution scripts from [31], GPU-equipped nodes are required to run the selected LLM models. Each repair attempt is executed on a dedicated node. No other user has access to the nodes assigned to this study, and we do not execute any other task in parallel. Before the execution of a repair attempt, the cluster deploys an image with the default standard environment (OS + libraries). This image is the same for every repair attempt. It is important to note that the environment does not execute any automated update after the image deployment (e.g. library update from the internet). All of this ensures that all repair attempts are executed under the same conditions. Each node is individually monitored by an OmegaWatt vendor wattmeter4, which provides the electrical power consumed each 20 milliseconds with a precision of 0.1 watt. An experiment based on energy measurement should be performed several times in order to remove energy measurements that are potentially outliers [ 98]. For this reason, we execute each repair attempt five times and then compute the median of the metrics obtained on each repair attempt. In a repair attempt, before and after a repair tool execution, we execute the command sleepwith 30 second as parameter, in order to capture the power of the machine in idle state. That helps us to detect eventual anomalies, as we explain later. For the execution of traditional APR tools, we set a timeout of 5 hours for each repair attempt, which is larger than those used in previous Java repair experiments [ 60,87]. The repair attempts are executed on the nodes having just CPUs, as GPU is not required by traditional APR tools. 2We need to compute NPC ourselves because in Liu et al. [19], the work that uses NCP as a proxy of efficiency, the NPC per repair attempt (e.g., tool-bug pair) is not shown, they only report a summary per tool. 3Replicability packages from [31]: https:\/\/github.com\/lin-tan\/clm and from [32]: https:\/\/github.com\/ASSERT-KTH\/repairllama 4https:\/\/mv.omegawatt.  [Text truncated due to context length limits]\nDocument 6\n2366 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 51, NO. 8, AUGUST 2025 RepairLLaMA: Ef\ufb01cient Representations and Fine-Tuned Adapters for Program Repair Andr\u00e9 Silva ,S e nF a n g , and Martin Monperrus Abstract \u2014Automated Program Repair (APR) has evolved sig- ni\ufb01cantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue ofresearch, with many dimensions which have not been explored.Existing work mostly \ufb01ne-tune LLMs with naive code represen-tations and does not scale to frontier models. To address thisproblem, we propose RepairLLaMA, a novel program repairapproach that 1) identi\ufb01es optimal code representations forAPR with \ufb01ne-tuned models, and 2 ) pioneers state-of-the-art parameter-ef\ufb01cient \ufb01ne-tuni ng technique (PEFT) for program repair. This results in RepairLLaMA producing a highly ef-fective \u2018program repair adapter\u2019 for \ufb01xing bugs with AI. Ourexperiments demonstrate the validity of both concepts. First,\ufb01ne-tuning adapters with program repair speci\ufb01c code repre-sentations enables the model to u se meaningful repair signals and produce better patches. Seco nd, parameter-ef\ufb01cient \ufb01ne- tuning helps \ufb01ne-tuning to converge and clearly contributes tothe effectiveness of RepairLLaMA in \ufb01xing bugs outside the\ufb01ne-tuning data distribution. Overall, RepairLLaMA correctly\ufb01xes 144 Defects4J v2, 109 HumanEval-Java, and 20 GitBug-Java bugs, outperforming all baselines. Index Terms \u2014Automated program repair, large language mod- els, code representations, parameter-ef\ufb01cient \ufb01ne-tuning. I. INTRODUCTION AUTOMATED program repair (APR) [1],[2]aims at au- tomatically \ufb01xing a software bug without human inter- vention. Learning-based repair [3],[4],[5],[6],[7],[8],[9], [10] has become the mainstream solution to this problem due to the powerful ability of deep neural networks to learn com- plex bug \ufb01x patterns. Large language models (LLMs), pre- trained on vast amounts of data, have pushed learning-basedrepair to the next frontier [8],[11]. There are two main re- search lines in applying LLMs to program repair: (1) \ufb01ne-tuning techniques to adapt pre-trained LLMs and create specializedrepair models [5],[8],[10],[12], and (2) prompt engineering Received 7 June 2024; revised 30 May 2025; accepted 2 June 2025. Date of publication 18 J une 2025; date of current version 15 August 2025. This work was supported in part by Wallenberg AI, Autonomous Systems and SoftwareProgram (WASP) funded by the Knut and Alice Wallenberg Foundation.Recommended for acceptance by K. Tantithamthavorn. (Andr\u00e9 Silva and Sen Fang contributed equally to this work.) (Corresponding author: Andr\u00e9 Silva.) Andr\u00e9 Silva and Martin Monperrus are with KTH Royal Institute of Tech- nology, 114 28 Stockholm, Sweden (e -mail: andreans@kth.se; monperrus@ kth.se). Sen Fang is with NC State University, Raleigh, NC 27695 USA (e-mail: sfang9@ncsu.edu). Digital Object Identi\ufb01 er 10.1109\/TS E.2025.3581062and agent-based appr oaches that lever age LLMs\u2019 capabilities through carefully designed inputs and interaction patterns[13],[14],[15],[16],[17]. Fine-tuning has the potential to learn domain-speci\ufb01c rep- resentations and leverage signals in order to generate high-quality patches. The drawback is t hat it requires substantial computational resour ces and high-quality trai ning data. Prompt engineering and agent-based approaches mostly consist of adhoc human-developed prompts and work\ufb02ows to instruct mod- els for speci\ufb01c tasks and are bounded by the limited size of con- text windows. While both approaches offer different trade-offs,we argue that \ufb01ne-tuni ng is crucial for hi gh-quality automated program repair because: (1) it allows the model to internalize repair patterns from a supervised corpus of bug \ufb01xes, (2) it en-ables learning of task-speci\ufb01c repair representations rather than relying on manual prompt engineering steps, and (3) it provides opportunities to incorpor ate repair-speci\ufb01c information during training. Our paper is a key contribution in the space of \ufb01ne- tuning LLMs for program repair. Fine-tuning LLMs for program repair is complex. Early work simply re\ufb01nes the network we ights based on additional \ufb01ne- tuning data. However, this kind of \ufb01ne-tuning is rather primitiveand suffers from two signi\ufb01cant drawbacks. First, \ufb01ne-tuning is also known to be able to adapt the input\/output representa- tions of the data under study [18]. In the context of program repair, there is an opportunity to \ufb01ne-tune with code repre- sentations that maximize downstream task performance, that is, repair performance. In particular, previous work overlooksthe realistic representation of fault localization in the input. Second, previous work considered the most basic \ufb01ne-tuning technique, which is full-parameter \ufb01ne-tuning. As LLMs in-crease in size [19], full-parameter \ufb01ne-tuning poses important over\ufb01tting problems when \ufb01ne-tuni ng data is limited, which is typically the case in program repair. In this paper, we addressthe problem of devising ef\ufb01cient \ufb01ne-tuning techniques [20] for program repair, with a focus on code representations and adapters. We propose RepairLLaMA, a new program repair approach that leverages parameter-ef\ufb01cient \ufb01ne-tuning to adapt LLMsto handle repair-speci\ufb01c code representations. First, Repair- LLaMA\u2019s code representations incorporate fault localization signals and are designed to s upport multi-loca tion bugs. Second, RepairLLaMA utilizes Low-Rank Adaption (LoRA), a state- of-the-art parameter-ef\ufb01cient \ufb01ne-tuning technique, to train a much smaller repair adapter (when compared to the full LLM) that adapts the LLM for program repair while helping prevent \u00a9 2025 The Authors. This work is licensed under a Creative Co mmons Attribution 4.0 Licen se. For more information, see https:\/\/creativecommons.org\/licenses\/by\/4.0\/ SILV A et al.: R EPAIR LLAMA: EFFICIENT REPRESENTATIONS AND FINE-TUNED ADAPTERS 2367 over\ufb01tting [21]. As we will demonstrate in this paper, the con- cept of repair adapter is novel and potent. Our experimental results validate RepairLLaMA\u2019s core de- sign. First, RepairLLaMA achieves state-of-the-art \ufb01ne-tuning performance in three benchmarks, correctly \ufb01xing 144 De- fects4J v2 [22] bugs, and 109 and 20 bugs, respectively, on recently proposed benchmarks HumanEval-Java [8]and GitBug-Java [23], which boosts internal and external valid- ity. The experiments show that the devised code representa-tions with repair signals allow the LLM to synthesize patches more effectively than the naive code-only representations. Also, RepairLLaMA clearly outperforms non-\ufb01ne-tuned baselines,incl. GPT-4. Moreover, our results also show the effective- ness of parameter-ef\ufb01cient \ufb01ne-tuning: RepairLLaMA\u2019s repair adapters, with only 4M parameters, are 1600x smaller than the initial pre-trained LLM, CodeLLama-7B [24]. To sum up, the ef\ufb01cient representations and repair adapters of RepairLLaMAoutperform recent results on \ufb01ne-tuning for program repair [8], [10],[25] as well as world-class models such as GPT-3.5 and GPT-4. Overall, we make the following contributions: \u2022We design RepairLLaMA, an original \ufb01ne-tuning pipeline for automated program repair with LLMs. RepairLLaMA\u2019srepresentations maximize knowledge from the program repair domain, while keeping strong alignment with pre- training. \u2022We systematically evaluate different code representations for program repair \ufb01ne-tuning. Our results clearly show that the best code representation leverages the task-speci\ufb01csignals of fault localization and original buggy code. \u2022We demonstrate that parameter-ef\ufb01cient \ufb01ne-tuning per- forms competitively with full- parameter \ufb01ne-t uning in the context of program repair. The \u201crepair adapters\u201d of Repair- LLaMA are training-ef\ufb01cient, and achieve state-of-the-artrepair performance across three benchmarks, Defects4J, HumanEval-Java, and GitBug-Java, outperforming even GPT-4. \u2022For the sake of open science, we publish our source code, models, and artifacts at https:\/\/github.com\/ASSERT-KTH\/ repairllama. II. R EPAIR LLAMA: E FFICIENT FINE-TUNING FORPROGRAM REPAIR A. Overview Fig. 1illustrates the pipeline o f RepairLLaMA for APR, which is divided into three consecutive stages. The core nov- elties of this pipeline are: 1 ) the APR speci\ufb01c code represen- tations, and 2) the end-to-end use of a parameter-ef\ufb01cient \ufb01ne- tuning technique. The core of RepairLLaMA is a repair adapter .Arepair adapter is a plug-and-play extension of the model parame- ters that modi\ufb01es the behavior of the LLM in order to maxi- mize performance on the repair task, for a given programminglanguage. The adapter is responsible for transforming a rich, tailored input representation of the buggy code into the \ufb01t output representation of the patch.In the \ufb01rst stage of RepairLLaMA, the core choices are made, namely: 1) the initial pre-t rained model (subsection II-C); 2) the input code representation and output code representation(subsection II-D); and 3) the \ufb01ne-tuning dataset (subsection II-E). These choices are all important and are further discussed in the remainder of this section. In the second stage, a repair adapter is trained. The repair adapter is a much smaller (i.e., approx. 4M parameters) plug- and-play adapter of the initial LLM while remaining competi-tive on the task of program repair. Finally, in the third stage, the repair adapter is employed to \ufb01x real-world bugs. B. Target Bugs The \ufb01rst consideration when designing a \ufb01ne-tuning pipeline for program repair is the bugs we aim to \ufb01x. This relates to 1)the programming language, 2) the type of bugs (syntax errors, runtime errors, functional errors, etc), and 3) the dif\ufb01culty of bugs, which can be proxied by the code span to modify in orderto \ufb01x the bug. In this work, we focus on 1) Java bugs, 2) that are functional, and come with at least a failing test case, 3) that are intra-procedural, i.e. \ufb01xed with changes to a single function (called hereafter single-function bugs ). We do not make any assump- tion on the length of the met hod under repair, and 4) explicitly support bugs that r equire changes in mu ltiple locations in the function [26], beyond single-line or single-chunk bugs. C. Choice of the Initial LLM Choosing the suitable initial LLM for \ufb01ne-tuni ng is crucial. For example, when \ufb01ne-tuning for code-related tasks, an LLM pre-trained on large-scale code corpora is more effective thanone pre-trained on pure natural language data. To effectively \ufb01ne-tune an LLM for APR, we curate three criteria to choose the initial model. First, the LLM should be publicly available and open-source. Fine-tuning a closed-source LLM on the task-speci\ufb01c dataset is not a valid option. Although some companies like OpenAIdo provide an API for \ufb01ne-tuning their LLMs, it is expensive, and the ownership of the \ufb01nal model (incl. weights) does not meet open-science reproduction criteria. Open-source models,such as LLaMA [27] or StarCoder [28], publish model weights online, allowing anyone to modify and deploy them. Second, the LLM should be pre-trained with large-scale code data. As observed by related work [24],[28], LLMs pre- trained on massive code data achieve better performance in code-related tasks. Thus, we consider only LLMs specialized on code. Third, the initial LLM shoul d have been trained with an in\ufb01lling objective [29] during pre-training. As observed by related work [8], in\ufb01lling is a natural and effective learning objective for the program repair task, since it allows the modelto synthesize code according to both the context appearing before and after. It should also be supported by an off-the-shelf parameter-ef\ufb01cient \ufb01ne-tuning library. 2368 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 51, NO. 8, AUGUST 2025 Fig. 1. Overview of RepairLLaMA. The core novelties of RepairLLaMA ar e the APR speci\ufb01c code representations and the engineering of an effective program repair adapter that is plugged into the underlying LLM. In subsection III-B we instantiate those criteria in the context of functional program repair for Java. D. Choice of Code Representations Source code repres entation is a critical aspect that signif- icantly impacts the effectiveness of the model [30].I nt h i s section, we discuss key characteristics of the source code rep- resentation design space. We introduce, motivate, and elaborateon input and output code representations speci\ufb01c to the program repair task. 1) Representation of Fault Localization: Virtually all the APR literature a ssumes line-based faul t localization, with a single line given as input to the repair algorithm. This is not appropriate to \ufb01x multi-location bugs [26],[31]. Consider Fig. 3 (OR4), which shows the canonical patch for the multi-location bug Chart-5 from Defects4J. In this case, fault localization must identify a location where an entirely new if block shouldbe synthesized and inserted as well as another pre-existing if condition, appearing later in th e code. To our knowledge, there is no fault localization technique able to predict tuples of blocksto be repaired together. In this paper, we propose a novel way to represent fault localization information: our core idea is to represent fault localization not as a single line, but as a region. In Re- pairLLaMA, we encode fault localization as a span rang-ing from the beginning of the suspicious region to its end. This encoding is realistic because 1) identifying a buggy method is within reach of existing fault localization methods,and 2) exhaustively listing all suspicious code regions of a buggy method is worst-case O(n 2)in the number of method lines.2) Input Representation Space: In APR, the design space of the input representation relates to what is shown from the buggy code and to the presence of additional information. For exam- ple, fault localization signals can be useful in scoping downwhere the code should be modi\ufb01ed. However, such information might not be seen at the pre-training stage. For the LLM to utilize it, one must represent it in a way that it can learn during\ufb01ne-tuning. To study the input representation space, we design four input representations tailored to APR (Fig. 2): IR1: Buggy function This naive representation describes the code in the standard format as i t is written, simply as text. Fig.2(IR1) shows the buggy function of the multi-location bug Chart-5, a Defects4J bug. The advantage of IR1 is that it is thesame representation LLMs observe during pre-training. When using this representation, the main limitation is that the model has no access to fault localization information and, thus, needs to determine where to change the code, which can be considered as implicit anomaly detection. IR2: Buggy function w\/ FL comments This representation adds two comments signaling the start and end of the buggy chunk of code. For example, in Fig. 2(IR2), the three lines be- tween the start and end of the suspicious region are surrounded by comments signaling the beginning and end of the buggy chunk. By providing fault localization information, the modelcan scope its changes to the buggy section. IR3: Buggy function w\/ in\ufb01lling mask This representation uses the in\ufb01lling scheme some LLMs are trained for duringpre-training [29]. The buggy chunk is replaced by the in\ufb01lling token, which prompts the model to \ufb01ll it. For example, in Fig. 2 (IR3), the three lines between the start and end of the suspiciousregion are replaced by the <FILL_ME >token. This representa- tion yields shorter inputs and requires less \ufb01ne-tuning since the SILV A et al.: R EPAIR LLAMA: EFFICIENT REPRESENTATIONS AND FINE-TUNED ADAPTERS 2369 Fig. 2. Buggy code of the multi-location bug chart-5 represented in our four different input representations. in\ufb01lling objective has been used dur ing pre-training. However, by masking the buggy portion of code, this representation incursinformation loss that can be useful to generate a \ufb01x. IR4: Buggy function w\/ in\ufb01lling mask and buggy code This representatio n combines the buggy code with the in\ufb01lling scheme. The buggy code is shown in a comment at the end of the pre\ufb01x portion. For example, in Fig. 2(IR4), the buggy lines are kept in comments, and the <FILL_ME >token is placed immediately afterward. This representation is different from the one learned during pre-training and requires \ufb01ne-tuning. Code found in the wild would typically not include buggy code ascomments, which is considered bad practice. Yet, with \ufb01ne- tuning, this representation might add valuable information to the in\ufb01lling scheme. 3) Output Representation Space: Output representations in APR correspond to the representation of the synthesized\ufb01xed code. A natural output representation is a diff over the buggy code, aka a patch. As discussed in subsubsection II-D2 , \ufb01ne-tuning is required to adapt an LLM to generate such task-speci\ufb01c outputs. To study the output representa- tion space, we design four output representations tailored to APR (Fig. 3):Fig. 3. Patch for multi-location bug chart- 5 represented in our four different output representations. OR1: Fixed function The naive output is the full \ufb01xed function. It is not a diff. Fig. 3(OR1) shows the \ufb01xed function of the multi-location bug Char t-5. The major drawback of OR1 is that such output may be much larger than the actual code changes for \ufb01xing, and LLMs are known to be more effective at generating short sequences over long sequences. OR2: Fixed chunk In this representation, the output is com- posed of the \ufb01xed chunk of code to replace the buggy chunk of code. The advantage is that the \ufb01xed chunk is typically shorterthan the full function, i.e. shorter than OR1. For example, in Fig. 3(OR2), only 6 \ufb01xed lines are outputted. OR2 requires an input representation that includes fault localization (i.e. IR2,IR3, IR4) since the output contains no information regarding what to replace. OR3: Three-line context-diff The output is a typical con- textual diff with a three-line context, aka a uni\ufb01ed diff. For example, in Fig. 3(OR3), a uni\ufb01ed diff of the statement change is outputted. The main challenge of this representation is that the model needs to learn to locate the bug locations during \ufb01ne- tuning, which is dif\ufb01cult. Additionally, this representation isalso lengthier than generating a \ufb01xed chunk (OR2) only. OR4: One-line context-diff The output is a contextual diff with a shorter, one-line context. OR4 uses a one-line diff 2370 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 51, NO. 8, AUGUST 2025 TABLE I POSSIBLE CODE REPRESENTATION PAIRS FOR FINE-TUNING LLM S FOR AUTOMATED PROGRAM REPAIR .THEY EXPLOIT THE CHARACTERISTICS OF THE APR T ASK,INCL.THE PRESENCE OF FAULT LOCALIZATION SIGNALS AND THE NOTION OF \u2018\u2018BUGGY CODE\u2019\u2019 Code Representations FL Aligned w\/ PT Buggy Code IR1 x OR1 \u2717 \u2714\/\u2717 \u2714 IR1 x OR3 \u2717 \u2714\/\u2717 \u2714 IR1 x OR4 \u2717 \u2714\/\u2717 \u2714 IR2 x OR2 \u2714 \u2717\/\u2714 \u2714 IR3 x OR2 \u2714 \u2714\/\u2714 \u2717 IR4 x OR2 \u2714 \u2717\/\u2714 \u2714 context, making it shorter than OR3. For example, in Fig. 3 (OR4), there are \ufb01ve source code lines less when compared with OR3. Despite this, it is still lengt hier than OR2 and also requires the model to learn where to apply the patch. 4) Input\/Output Representation Pairs: To utilize an LLM for APR, input and output representations must be carefullypaired. This is because all input representations cannot be paired with all output representations. For instance, IR1 cannot pair with OR2 since one cannot apply a \ufb01xed chunk to the buggyfunction without the fault localization information. Table Ipro- vides the list of the code representation pairs that are studied in this paper. Each row corresponds to a code representationpair. Column FLindicates whether the pair includes or not fault localization information. Column Aligned w\/ PT provides a relative assessment of the alignment of the representationw.r.t. the pre-training data\/objective. A red cross means that the code representation is not aligned with the pre-training data and objective. The left side shows the input and the right the outputrepresentations. Column Buggy Code indicates whether the pair includes or not the original buggy code. The \ufb01rst three rows (i.e., IR1xOR1, IR1xOR3, IR1xOR4) include code representation pairs that do not contain fault lo- calization signals. The input is the same across all pairs (IR1),whereas the output can either be the full \ufb01xed function (OR1) or a diff (OR3, OR4). The key difference between the pairs is the output length and format. The latter three rows (i.e., IR2xOR2, IR3xOR2, IR4xOR2) include code representation pairs that contain fault localiza- tion information, either as tokens or as in\ufb01lling, which is spe-ci\ufb01c to program repair. The most aligned representation with pre-training is IR3xOR2 since the pre-trained model has sup- port for in\ufb01lling. IR2 represents the in\ufb01lling objective withnever-before-seen comments, whereas IR4 keeps the buggy code as comments. The natural output representation to pair with these is OR2 since it only includes the new code to re-place the already localized buggy chunk, minimizing output length. Note that we have empirically tested other combina- tions in a pilot experiment, and the ones not listed in Table I underperform. E. Choice of Fine-Tuning Dataset After choosing an initial m odel and appropria te code repre- sentations, the next step is to curate a \ufb01ne-tuning dataset. First, the dataset must be relevant to the task at hand. In the APRtask, a relevant dataset usually includes pairs of buggy and \ufb01xed code samples. Second, the type of samples included should be similar to the target bugs. Third, the size of the dataset shouldbe considered. A larger dataset generally leads to better model performance as it provides more examples for the model to \ufb01ne-tune from. However, it is important to balance size withquality - a smaller, high-quality dataset may b e more bene\ufb01cial than a larger, low-quality one. Four th, the diversity of the dataset is important. A diverse dataset that covers a wide range of exam-ples can help the model generalize better to unseen data. Lastly, the legality and ethics of the dataset should b e considered, in particular regarding privacy and copyright. F . Program Repair Adapters for LLMs With the recent release of various LLMs, the scale of pa- rameters has signi\ufb01cantly increased. For instance, state-of-the- art models such as LLaMA [19] and CodeLLaMA [24] range from 7B to 70B parameters. Fine-tuning these LLMs often requires substantial GPU resources. As an example, Lv et al. [32] report that \ufb01ne-tuning the full parameters of LLaMA-7B on an RTX 3090 consumes 126.08 GB at peak GPU memory usage, with the batch size and sequence length set to 1 and 1024 respectively. Fine-tuning current LLMs with limited resourcesis a challenge. RepairLLaMA uses LoRA [20], a state-of-the-art parameter- ef\ufb01cient \ufb01ne-tuning method that reduces memory requirementswhile maintaining model performance. Instead of \ufb01ne-tuning all model parameters, LoRA freezes the pre-trained LLM weights and injects trainable low-rank matrices into speci\ufb01cattention layers. In addition to reducing memory requirements, by reducing the number of trainable parameters LoRA also acts as a regularizer [21] helping prevent over\ufb01tting during \ufb01ne-tuning. In our implementation, we apply LoRA to the query (q_proj ) and value ( v_proj ) projection matrices in each transformer\u2019s self-attention layer. These projection matrices transform the input embeddings into query and value vectorsused in the attention. For each target weight matrix A\u2208R d\u00d7k, LoRA decomposes the update into a product of two lower-rank matricesB\u2208Rd\u00d7randC\u2208Rr\u00d7k, whereris the rank. During inference, the effective weight matrix is W=W0+BA, where W0is the frozen pre-trained weight. The trained matrices can be interpreted as a repair adapter that is much smaller than the original LLMs. For example, with rank r =8, our adapter only requires training 0.39%1of the parameters compared to full \ufb01ne-tuning. This parameter-ef\ufb01cient approach allows RepairLLaMA to achieve strong pro- gram repair performance while being trainable on a single GPU. G. Inference Time The \ufb01nal step is to deploy the repair adapter. The tar- get buggy program is fed to a fault localization algorithm 1Take CodeLLaMA-7B as an example, its hidden size is 4096, so the original parameter of a matrix is 4096 \u00d74096. As for the LoRA adapter, this value is 4096 \u00d78 + 4096 \u00d78. SILV A et al.: R EPAIR LLAMA: EFFICIENT REPRESENTATIONS AND FINE-TUNED ADAPTERS 2371 and processed to generate an APR-speci\ufb01c code represen- tation. Then, the code represent ation is fed to the initial model combined with the LoRA repair adapter to generate alist of candidate patches for the buggy program. Patches are then checked for plausibility and co rrectness per of f-the-shelf techniques. III. E XPERIMENTAL METHODOLOGY A. Research Questions In this work, we focus on the following research questions: \u2022RQ1 (Code Representations for Fine-Tuning) : What is the best code representation to \ufb01ne-tune an LLM forprogram repair? \u2022RQ2 (Parameter-Ef\ufb01cient Fine-Tuning vs. Full Fine- Tuning) : How does parameter-ef\ufb01cient \ufb01ne-tuning com- pare against full-parameter \ufb01ne-tuning for program repair? \u2022RQ3 (RepairLLaMA vs. ChatGPT-based APR) : How does RepairLLaMA compare against state-of-the-artChatGPT-based program repair? B. Implementation Model to Fine-Tune Per the criteria of subsection II-C,w e choose CodeLlama-7b [24] as our initial LLM. CodeLLaMA is a publicly available LLM released in 2023 and is trained on 500B code tokens. Per the experiments reported in [24], CodeLLaMA outperforms GPT-3.5 on two code generation benchmarks. Fine-tuning Dataset We choose Megadiff [33] as the \ufb01ne- tuning dataset, and process all samples into the different code representations. First, the function pairs \u2013 each com- prising a buggy version and its \ufb01xed counterpart \u2013 are ex- tracted along with their corresponding diff identi\ufb01ers. Subse- quently, we eliminate pairs that do not change single func-tions, and remove duplicate pairs through textual compar- ison. After that, we compute our custom code representa- tions. We keep only samples whose total length (input plusoutput) is shorter than 1024 tokens measured by the LLM tokenizer. Consequently, the \ufb01ne-tuning datasets range from 30,000 to 50,000 \ufb01ne-tuning pairs (see our appendix repositoryat https:\/\/github.com\/A SSERT-KTH\/repairllama). Evaluation Benchmark We select three Java benchmarks for our evaluation: Defects4J [22], HumanEval-Java [8], and GitBug-Java [23]. Following recent related work [11],[14], [34], we scope our evaluation to single-function bugs, as de\ufb01ned in subsection II-B. Defects4J comprises 835 real-world bugs from 17 open-source Java projects, from which we identify 488 single-function bugs. HumanEval-Java is a bug bench- mark containing arti\ufb01cial bugs inserted in HumanEval [35] Java programs. HumanEval-Java contains 162 single-function bugs. GitBug-Java is a bug benchmark of recent bugs, collectedfrom the 2023 commit history of 55 open-source repositories, comprising 90 single-function bugs. Contrary to Defects4J, GitBug-Java suffers from less data leakage in the pre-trainingdata since all bugs are much more recent than Defects4J. No- tably, GitBug-Java exclusively contains bugs from after thetraining data cutoff date of all models used in our experiments: CodeLlama-7b (September 2022) [24],gpt-4-0613 (September 2021) 2, and gpt-3.5-turbo-0613 (September 2021)2. Fine-Tuning Hyperparameters We \ufb01ne-tune CodeLLaMA with LoRA for each of our curated code representations with the same hyper-parameter setti ngs: we set the learning rate to 5e-4 with cosine decay, max input length to 10243,t r a i n - ing epoch to 2, and batch size to 16 per GPU, and we use Adam_W as the optimizer. For LoRA, we use a rank of 8, alphaof 16, dropout of 0.05, and inject the adaptation matrices in theq_proj andv_proj layers. Using the same hyper-parameter settings for each code representation ensures fair comparison.Each \ufb01ne-tuning run is executed on a server with 4xA100 40GB GPUs. Inference Setup In inference, we employ beam search as our decoding strategy with a beam size of 10 per previous research [8]. Hence, for each bug, we generate 10 candidate patches. We use the HuggingFace transformers library to implement all \ufb01ne- tuning and inference experiments. Inference is run on a single A100 40GB GPU. C. Patch Assessment Patch assessment is a notoriously hard task. In our evaluation, we compute the best metrics for that task [8],[11],[26]:1 ) Aplausible patch is de\ufb01ned as one that successfully passes all test cases. 2) An exact-match patch is textually identical to a developer-provided reference patch, incl. spaces and format- ting. 3) An AST-match patch has an AST which is equivalent to the AST of the developer-provided reference patch. 4) Asemantic-match patch is a patch deemed equivalent after man- ual assessment by an expert. Plausible and exact-match patches are straightforward. Let us dwell on the two other kinds. The major advantage of an AST-match patch is to compute performance regardless of form atting and indentation changes. It is also more scalable than manually checking patches for correctness without expertise in the programs under repair. TheAST-match process involves converting plausible and reference patches into abstract syntax trees [36] and subsequently utiliz- ing AST differencing [37] to compare their ASTs for discrep- ancies. Asemantic-match patch is the most costly assessment to get. In this paper, to assess semantic equivalence, the two \ufb01rstauthors independently label all plausible but not AST\/Exact match patches in a \ufb01rst round. For the patches the two \ufb01rst authors disagree upon, the third author breaks the tie. For all four metrics, the higher the metric, the better the per- formance. We validate the candidate patches on a workstation with an 18-core Intel Core i9-10980XE CPU and 128 GB ofRAM, operating under Ubuntu 22.04.3 LTS. 2https:\/\/platform.ope nai.com\/docs\/models 3Although longer input lengths enable the model to process more context and, potentially, \ufb01x more bugs, about four times more GPU memory is required when doublin g the input length. 2372 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 51, NO. 8, AUGUST 2025 D. Methodology for RQ1 The objective of RQ1 is to investigate the most effective code representations for \ufb01ne-tuning an LLM for program repair. While existing research has delved into the utility of LLMs for program repair, the impact of the code representations, such as their realism, has been overlooked. It is known that variations in code representations may yield substantial differences inperformance for \ufb01ne-tuned LLMs [25]. Consequently, in RQ1, we empirically evaluate 6 realistic code representation pairs presented in II-D and measure their performance. We \ufb01ne-tune an LLM as described in III-B .W ep r o m p tt h e model to generate 10 patches for each bug using beam search decoding. We then evaluate the generated patches as outlined in subsection III-C , to measure the effectiveness of each code representation. We prompt the non \ufb01ne-tuned CodeLLaMA-7B as a baseline. We assess the statistical signi\ufb01cance of performance differ- ence by employing the McNemar test for each pairwise com- bination of representations. In our context, each statistical testlooks at the binary outcomes of two representations (or models) evaluated on the same set of benchmark examples, according to semantical match. The null hypothesis ( H 0) for the McNemar\u2019s tests is that the distributions are indistinguishable, i.e. the row and column marginal frequencies in the 2x2 contingency ta- ble are equal, i.e., H0:P(A=1,B=0)=P(A=0,B=1) , where A and B represent whether A and B, respectively, \ufb01xed a given bug. E. Methodology for RQ2 The objective of RQ2 is to evaluate the respective effec- tiveness of parameter-ef\ufb01cient and full-parameter \ufb01ne-tuning. Generally, parameter-ef\ufb01cient \ufb01ne-tuning methods represent a trade-off between computational cost and model performance,in order to train LLMs with limited computational resources. While traditional full-parameter \ufb01ne-tuning approaches often yield better results, it comes at the expense of signi\ufb01cantlyhigher memory requirements and a larger-scale \ufb01ne-tuning dataset. In other words, fully \ufb01ne-tuning an LLM on a small \ufb01ne-tuning dat aset may result in over\ufb01 tting. In this experiment, we explore and compare the effectiveness of parameter-ef\ufb01cient and full-parameter \ufb01ne-tuning in the speci\ufb01c context of program repair, which has never been done to the best of our knowledge. Baseline. We consider two baseline base models in RQ2: CodeLLaMA-7B and deepseek-coder-6.7b-base [38]. To study the difference between parameter-ef\ufb01cient \ufb01ne-tuning and full-parameter \ufb01ne-tuning, we \ufb01ne-tune both models with both ap- proaches. Here, we use the same hyper-parameters as in LoRA \ufb01ne-tuning described in RQ1. We use a learning rate of 2e-5 for full-parameter-\ufb01ne-tuning. We employ the same statistical sig- ni\ufb01cance test as described in subsection III-D when comparing RepairLlama against these baselines. Additionally, we benchmark our approach against the highest-performing model documented by Jiang et al. [34]\u2014 speci\ufb01cally, the \ufb01ne-tuned variant of Incoder-6B [39].  [Text truncated due to context length limits]\nDocument 7\nhttps:\/\/doi.org\/10.1007\/s10664-025-10658-6 Do LLMs consider security? an empirical study on responses to programming questions Amirali Sajadi1\u00b7Binh Le1\u00b7Anh Nguyen1\u00b7Kostadin Damevski2\u00b7 Preetha Chatterjee1 \u00a9 The Author(s) 2025 Abstract The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT\u2019s potential in volunteering context-speci\ufb01c information to the developers, promoting safe coding practices. Motivated by this \ufb01nding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Over\ufb02ow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential \ufb01xes of the vulnerability, to help raise users\u2019 awareness. Our \ufb01ndings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of \ufb01le names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and \ufb01xes of vulnerabilities compared to Stack Over\ufb02ow responses. Finally, we provide an in-depth discussion on the implications of our \ufb01ndings, and demonstrated a CLI-based prompting tool that can be used to produce more secure LLM responses. Communicated by: Xin Xia B Amirali Sajadi amirali.sajadi@drexel.edu Binh Le bql23@drexel.edu Anh Nguyen adn56@drexel.edu Kostadin Damevski kdamevski@vcu.edu Preetha Chatterjee preetha.chatterjee@drexel.edu 1College of Computing and Informatics, Drexel University, Philadelphia, PA, USA 2College of Engineering, Virginia Commonwealth University, Richmond, V A, USA 0123456789().: V,-vol 123Empirical Software Engineering (2025) 30:101 Accepted: 3 April 2025 \/ Published online: 16 April 2025 Keywords Security evaluation \u00b7Vulnerability awareness \u00b7Large language models 1 Introduction Large language Models (LLMs) have become deeply integrated into software engineer- ing work\ufb02ows, performing tasks such as code generation, summarization, debugging, and addressing queries related to programming (Liu et al. 2023a ; Hou et al. 2023 ; Zheng et al. 2023 ; Belzner et al. 2023 ). In particular, LLM chatbots or conversational LLMs ,s u c ha s OpenAI\u2019s GPT (OpenAI 2023 ), Anthropic\u2019s Claude (Anthropic 2024 ), and Meta\u2019s Llama (Meta 2024 ), have signi\ufb01cantly impacted problem-solving activities by enabling interactive Q&As (Suad Mohamed 2024 ; Das et al. 2024 ; Da Silva et al. 2024 ). Developers use them to describe symptoms, provide contextual information, and seek guidance on solutions (Hou et al. 2023 ). According to a 2023 survey, 92% of U.S.-based developers are using various generative models to perform or to automate some of their daily tasks (Shani 2024 ). However, the rapid adoption of LLMs by software developers has raised many concerns regarding the security implications of using LLMs. A recent study found that participants using AI assistants produced code with signi\ufb01cantly more vulnerabilities (Perry et al. 2023 ). Alarmingly, these participants were also more con\ufb01dent in the security of their code, suggest- ing that AI code assistants can foster a false sense of security, increasing the risk of introducing vulnerabilities into real-world software. Another study found that 32.8% of Python and 24.5% of JavaScript code produced by GitHub Copilot are vulnerable (Fu et al. 2023 ). These vulner- abilities, if exploited, can lead to severe consequences, such as the Log4Shell vulnerability (Kosinski 2023 ). In 2024 alone, over 34,000 vulnerabilities were reported (CVE Program 2024 ), highlighting the increasing frequency and severity of cybersecurity threats that endan- ger the safety, security, and reliability of software systems. Beyond generating vulnerable code, using LLMs can impact software security in more intricate and subtle ways. For instance, novice developers may unknowingly input insecure code (copied from Q&A forums) and ask LLMs to refactor and adapt it to their problem context. Similarly, during debugging, a developer might provide a block of code containing vulnerabilities, such as unsanitized user input in an SQL query, without being aware of its potential security implications. If LLMs fail to identify and address these vulnerabilities, developers may integrate \ufb02awed code into their projects, relying on the model without rec- ognizing the potential security risks themselves. To better understand this phenomenon, let us consider the following examples. In a SO question, a developer asks for help to resolve an issue with writing to a \ufb01le: I have a text file with one URL per line, like: https:\/\/www.google.com https:\/\/www.facebook.com ... The problem is, when I write the resulting URL to a file, I get anadditional %0A at the end of each line. Can you please explain to me why is this happening? I am using this script to fetch them: 123101 Page 2 of 29 EmpiricalSoftwareEngineering(2025)30 :101 add = open (\"manual_list.txt\" ,\"r\") for ainadd: response = requests.get(a, timeout=(2, 5), verify= False) fout = open (\"mylist.txt\" ,\"a\") fout.write( response.url+ \"\\n\" ) fout.close() Here, the developer is focused on removing the extra %0A characters, which could be resolved using the strip() function i.e., requests.get(a.strip(), timeout= (2, 5), verify=False)) . However, they fail to notice a signi\ufb01cant security risk posed by the verify=False parameter. Disabling SSL certi\ufb01cate veri\ufb01cation is gener- ally considered poor security practice and exposes the application to serious risks, including man-in-the-middle attacks, where an attacker could intercept or manipulate the data sent over HTTPS. If prompted with this question, GPT-4, Claude 3, and Llama 3 all correctly explain the issue with %0A and suggested using the strip() function to \ufb01x it. However, none of the LLMs mentioned the security implications of the verify=False parameter, leaving the developer unaware of the potential vulnerability. By failing to inform the user of this vulnerability, the LLM responses can indirectly reinforce the faulty implementation and, in many cases, further build upon it, perpetuating insecure coding practices. Several studies have explored the potential risks associated with the use of LLMs and examined concerns regarding the generation of insecure code (Pearce et al. 2022 ; Siddiq and Santos 2022 ; Khoury et al. 2023 ; Siddiq et al. 2024b ), inaccuracies in vulnerability detection (Ullah et al. 2024 ; Akuthota et al. 2023 ; Purba et al. 2023 ; Zhou et al. 2024b ), and poten- tial misuse for offensive applications, ranging from hardware to user-level attacks (Happe and Cito 2023 ; Falade 2023 ). However, most studies have focused on the security of LLM- generated code, often neglecting the natural language generated by LLMs that plays a critical role in interactive learning and problem-solving (Eastman 2023 ; Xia and Zhang 2023 ;H a o et al. 2024 ). Additionally, unlike prior research that focuses on detecting vulnerabilities in LLM-generated code or evaluating the capabilities of LLMs when explicitly tasked to detect vulnerabilities, our work investigates the ability of LLMs to proactively identify vulnera- bilities in user-supplied code. This re\ufb02ects real-world use cases where developers that rely on LLMs for various tasks inevitably prompt LLMs with vulnerable code. Our study is the \ufb01rst to assess the security awareness of LLMs using the textual information provided by the LLMs alongside or independent of code. We assess not only LLMs\u2019 ability to proactively detect vulnerabilities but also their effectiveness in communicating critical information \u2013 such as causes, exploits, and \ufb01xes \u2013 to enhance developer understanding and prevent the use of exploitable and insecure code. We begin with a motivational study using an existing dataset of developer conversations with ChatGPT, investigating the frequency and speci\ufb01city of vulnerability-related warnings issued by ChatGPT. Mainly, our motivational study aims to understand the way ChatGPT engages with security topics. Initial \ufb01ndings suggest that ChatGPT can offer valuable, context-speci\ufb01c security guidance that encourages safer practices. More importantly, we found instances where the model voluntarily pointed out the security risks. These \ufb01ndings were signi\ufb01cant, as they demonstrated ChatGPT\u2019s potential to proactively raise developers\u2019 security awareness during development. This observation motivated us to dig deeper and per- form a systematic examination of the security awareness across three popular LLMs: Claude 3, GPT-4, and Llama 3. We curated a dataset of 300 Stack Over\ufb02ow questions that contain vulnerable code. In half of these questions, security vulnerabilities were explicitly noted by 123EmpiricalSoftwareEngineering(2025)30:101 Page3of29101 the SO participants (Mentions-Dataset); in the other half, vulnerabilities were not identi\ufb01ed (Transformed-Dataset). We used these questions as prompts to the LLMs and analyzed their responses, focusing on whether they proactively recognized and addressed security concerns in both code and text-based guidance. Speci\ufb01cally, we investigate the following research questions: RQ1: Given insecure code, do LLMs warn developers of the potential security \ufb02aws or required security modi\ufb01cations? Through RQ1, we aim to investigate the degree to which LLMs exhibit security aware- ness. We conduct a qualitative analysis to examine whether LLMs simply provide answers to questions or if they also warn users about the security \ufb02aws and suggest potential mod- i\ufb01cations to improve code security. Our results indicate that LLMs seldom issue security warnings about vulnerabilities unless explicitly prompted to do so. RQ2: In instances where users are reminded of security concerns, are they informed about the causes, potential exploits, and possible \ufb01xes of the vulnerabilities? Through RQ2, we aim to determine the types of information included in the LLM security warnings. We qualitatively analyze the information LLMs provided for each vulnerability, speci\ufb01cally checking if they detailed the causes , potential exploits , and possible \ufb01xes.W e then perform the same analysis on user-provided SO responses containing warnings about insecure code and compare these to the LLM responses across the two datasets. According to our \ufb01ndings, in cases where vulnerabilities are pointed out, LLMs generally offer more information about the causes, exploits, and \ufb01xes of the insecurities compared to SO responses. We discuss the implications of our \ufb01ndings for improving LLM security awareness in Software Engineering (SE) across three key areas: a) Prompt Engineering for SE, b) Inte- grating LLMs with SE tools, and c) Designing LLMs for SE. By sampling 50 questions from our dataset, we explored various prompting techniques to increase the likelihood of LLMs issuing security warnings. We observed that adding short phrases such as \u201cAddress secu- rity vulnerabilities\u201d to the prompts showed some effectiveness, although limitations persist. Additionally, we develop a pipeline for integrating outputs from static analysis tools like CodeQL into LLM prompts and \ufb01nd the potential of this method for enhancing security awareness of LLMs. Beyond these immediate solutions, our \ufb01ndings highlight the need for targeted design improvements to address the substantial gaps in LLMs\u2019 security awareness, not only in the code they generate but also in the explanations and recommendations they provide. These insights point to key areas for future research, tool development, and LLM evaluations aimed at creating more security-conscious LLMs as programming assistance. Overall, this paper presents the \ufb01rst study on the security awareness of three popular LLMs in answering programming-related questions. We \ufb01nd that all three LLMs we studied rarely warn developers about security vulnerabilities. We fruther assess how well LLMs can raise developer awareness and encourage the adoption of secure coding practices. We notice that when LLMs do issue security warnings, they often provide more information about the causes, potential exploits, and \ufb01xes of the vulnerability, compared to typical SO responses. Observations from this study will inform future designs of tools and evaluation methodologies that aim to make LLM-driven programming more secure. More speci\ufb01cally our paper makes the following contributions: \u0081A motivational study that examines the naturally occurring security-related conversa- tions between developers and ChatGPT and outlines the ability of LLMs for proactively warning users about security. \u0081A benchmark for evaluating: (a) LLMs\u2019 capabilities in proactively detecting vul- nerabilities in real-world user-supplied code, and (b) issuing warnings with critical 123101 Page 4 of 29 Empirical Software Engineering (2025) 30 :101 information\u2014such as causes ,exploits ,a n d \ufb01xes\u2014to enhance developer understanding and mitigate integration of insecure code in existing code-bases. \u0081Preliminary results to demonstrate the opportunities for adapting simple and practical prompt engineering techniques to enhance the security of the LLM responses to pro- gramming questions. \u0081A CLI-based tool that analyzes developer queries and integrates CodeQL outputs to generate prompts that result in signi\ufb01cantly safer LLM responses. 2 Motivational Study The 2023 JetBrains survey, based on responses from 26k developers across 196 countries, reveals that 77% (i.e., approximately three in four developers) use ChatGPT (JetBrains 2023 ). Given its widespread adoption, we conducted a exploratory study to explore whether security considerations naturally emerge in developer conversations with ChatGPT. This motivational study serves as an initial gauge to determine if LLMs engage with security topics at all, help- ing us assess the feasibility of a larger, more comprehensive study. By examining existing ChatGPT conversations with explicit security mentions, we aimed to understand the fre- quency and depth of security-related advice offered by the model. This approach allowed us to examine patterns in ChatGPT\u2019s security guidance, including the initiating party of these discussions and the speci\ufb01c topics addressed. Figure 1illustrates part of a conversation in which ChatGPT issues a security warning, highlighting the security implications of the suggested code. Dataset In May 2023, OpenAI introduced a feature that allows users to share their conver- sations with ChatGPT through dedicated links (OpenAI 2023 ). Using this feature, Xiao et al. (2023 ) collected all the 3,794 developer-ChatGPT conversations publicly shared on GitHub and Hacker News until October 2023, forming the DevGPT dataset. To identify conversa- tions that mention security, we performed a keyword search on the text of these ChatGPT conversations. As keywords, we used the SO tags related to security proposed by Yang et al. (2016 ), such as \u201csecurity\u201d, \u201cweb-security\u201d, \u201csql-injection\u201d, and \u201cxss\u201d. To ensure comprehen- siveness of our data selection, we used both hyphened and non-hyphened variations of the keywords, such as \u201cweb-security\u201d and \u201cweb security\u201d, where applicable. This \ufb01ltering step resulted in a subset of the DevGPT dataset containing 233 conversations. We excluded 13 out of these 233 conversations that included security-related keywords but are predominantly in languages other than English, resulting in 220 conversations. Fig. 1 DevGPT: ChatGPT Reminding User about the Security of the Code 123Empirical Software Engineering (2025) 30 :101 Page 5 of 29 101 Although DevGPT primarily contains developer-ChatGPT interactions, not all conver- sations are guaranteed to relate to software engineering (e.g., programming concepts, debugging, or code generation). Therefore, we conducted a manual check to \ufb01lter out such non-SE conversations. In addition, as part of this manual step, two authors validated the per- formance of our keyword search by distinguishing conversations that discuss security from those that merely included security-related keywords without actually addressing security topics, i.e., false positives. The Cohen\u2019s kappa agreement McHugh ( 2012 ) for classifying con- versations as security-related or non-security-related achieved the strong score of 0.799, after which, the authors resolved all con\ufb02icts and \ufb01nalized the data through discussion. Finally, we identi\ufb01ed 102 technical conversations with ChatGPT mentioning software security, shared across various data sources, including code \ufb01les (45), issue threads (33), Hacker News (11), pull requests (9), and GitHub commits (4). Procedure We conducted a qualitative analysis to investigate the role of security in the 102 developer-LLM conversations with mentions of security. We identi\ufb01ed which party (Devel- oper or ChatGPT) brings up the security considerations, examined the types of available information, and identi\ufb01ed the type of vulnerability mentioned in each conversation. Two authors of this paper manually annotated the dataset (annotation instructions with examples are included in our replication package). The analysis was performed in an iterative approach consisting of multiple sessions. To calculate inter-rater agreement, we used Cohen\u2019s Kappa coef\ufb01cient. This process resulted in substantial agreement values (i.e., >0.6): 0.88 for types of information ,0 . 6 6f o r types of vulnerability , and 0.62 for initiating party . Types of Information To examine the types of information in the ChatGPT conversations, we used the Pan et al. ( 2021 ) taxonomy for for categorizing information in developer chats. Developer chats (e.g., Slack, Discord) closely align with developer-LLM conversations since both support dynamic, rapid, and iterative information exchange (Chatterjee et al. 2019 ). Con- sequently, we believe this taxonomy is well-suited for our study. The information categories in the taxonomy are listed in Table 1. Types of Vulnerability The understand the types of vulnerabilities that were discussed, we used a taxonomy proposed by Russo et al. ( 2019 ) and manually identi\ufb01ed the relevant category for each conversation. We chose Russo et al.\u2019s taxonomy because it provided a predetermined set of vulnerability types, allowing us to present an overview without needing to determine the level of granularity in CWEs for each vulnerability. Russo et al. categorize vulnerabilities into ten distinct types, providing a broad yet concise overview suitable for our analysis. Additionally, we took note of whether ChatGPT makes broad mentions of security, or if it offers detailed information about security. We de\ufb01ne a speci\ufb01c mention as a mention that contains any form of implementation details. An example of speci\ufb01c mention could be a conversation in which ChatGPT warns the user about the possibility of an SQL injection Table 1 Distribution of Types of Information in DevGPT Conversations with Security MentionsInformation type Instances Technical Discussion 49 Programming Information 31 Programming Problems 10 General Information 5 Documentation Information 4 Library Problems 3 123101 Page 6 of 29 Empirical Software Engineering (2025) 30 :101 based on the prompt\u2019s code and offers ways to preventing it. On the other hand, we de\ufb01ne a broad mention as a mention that lacks any implementation details, such as the sentence \u201c It\u2019s important to consider additional security measures when dealing with \ufb01le uploads... \u201d. Initiating Party We also determined whether it is ChatGPT or the user who \ufb01rst introduces the topic of security into the conversation. Speci\ufb01cally, we identi\ufb01ed instances where Chat- GPT provided security-related information or where the user directly asked about security aspects of development. In cases where the question inherently involves security but lacks explicit security mentions in the user\u2019s prompt, we do not attribute the initiation of the security discussion to either party. Findings Table 1illustrates the distribution of the types of information available in all 102 conversations in our dataset. The most common type of information is Technical Dis- cussion , which relates to conceptual conversations about software engineering. The frequent occurrences of security mentions in Technical Discussions points to the fact that security is more often included when conversations revolve around higher level issues related to soft- ware development. In many of these instances, ChatGPT informs the users about the good practices with regard to security, making statements such as: \u201c Always ensure that you\u2019re following the correct steps as mentioned in the WordPress.com OAuth2 documentation, and handle the tokens securely, keeping them out of URLs whenever possible to maintain secu- rity\u201d.Programming Information , i.e., conversations in which the user is mostly trying to get the LLM to generate code or to explain programming concepts, and Programming Problems , i.e., conversations in which the user is mostly trying to resolve programming problems, are the second and third most common types of conversations that lead to notions of security. In these cases, we often see ChatGPT trying to inform the user about the security concerns in their code or even the code generated by ChatGPT itself as well as the potential \ufb01xes for these concerns. Security, however, is rarely brought up in conversations about the user\u2019s problems with speci\ufb01c libraries or when the users are attempting to retrieve any general or documenta- tion related information. General Information , i.e., discussions that are not closely related to the project itself, such as the best choice of IDE or job-hunting experiences, Documentation Information ,a n d Library Problems are the least frequently observed types of information in conversations. Table 2provides an overview of the types of vulnerabilities .N o t a b l y ,\u201c Authentication bypass or Improper Authorization \u201da n d\u201c Cross-Site Scripting or HTML Injection \u201da r et h e most frequently discussed types of vulnerability, while others have much fewer mentions. Further, some types of vulnerabilities such as \u201c Buffer\/Stack\/Heap\/Integer Over\ufb02ow, Format String and Off-by-One \u201d were not present in any conversation. We also observed that 54 conversations include broad mentions of security and the other 48 conversations include speci\ufb01c mentions. These results indicate that in many instances ChatGPT makes statements about security without pointing out a speci\ufb01c vulnerability. Our \ufb01ndings about the initiating party show that in 69 out of the 102 cases, ChatGPT is the one who \ufb01rst mentioned security. In 14 out of those 69 conversations, the user followed up on this mention of security and further discussed the topic, while in 49 conversations the users did not directly follow up on the mention of security and in 6 instances the conversation did not continue by the user. Further, out of the 69 instances where ChatGPT \ufb01rst brought up the topic of security, 42 mentions were broad, while 24 were speci\ufb01c. In the remaining 33 of the 102 conversations, it was the user who \ufb01rst mentioned security. For example, one 123Empirical Software Engineering (2025) 30 :101 Page 7 of 29 101 Table 2 Summary of Vulnerability Categories Mentioned by ChatGPT Throughout Conversations Vulnerability Cate- goryDescription Instances Example Authentication bypass or Improper Autho- rizationAllowing attacker to bypass required authentication or not performing required authentication checks18 authorization code should be unique and tem- porary for each user session...each code can only be exchanged once for security reasons. Cross-Site Scripting or HTML InjectionAllowing attacker to execute arbitrary code in the web browser and stealing cookie-based credentials.16 This means that the cookie can only be accessed via HTTP requests and not through client-side scripts, which is a good security measure to prevent XSS attacks. SQL Injection Not properly sanitiz- ing user input before using them in SQL queries.5 Prepared statements are ef\ufb01cient and safe against SQL injection attacks. Information Disclo- sure and\/or Arbitrary File ReadAllowing attacker to get access to informa- tion and \ufb01les.3 Avoid hardcoding them in your Python script. Instead, use environment variables or AWS pro\ufb01les. Directory Traversal Allowing attacker to gain read access to arbitrary \ufb01le content.2 Exposing the entire node_modules directory publicly is generally a bad idea, due to the potential security risks and unnecessary expo- sure of dependencies. Remote Code Execu- tionAllowing attacker to execute arbitrary code within the affected application, poten- tially leading to unauthorized access or a privilege escalation.1 The vm2 library is a sandbox that can run untrusted code securely. It\u2019s built on top of the Node.js vm module and adds additional security. user said \u201cI have this class for generating user tokens... <code snippet>... What has better security, my class or using SHA-256?\u201d Overall, through this analysis several key observations have emerged. The existence of 102 instances with mentions of security within DevGPT dataset indicates a degree of empha- sis on security. Further, in 69 of these 102 conversations, ChatGPT volunteered the security information, without a direct request. 49 of these 69 instances were broad mentions e.g., \u201cLastly, always keep the user\u2019s privacy and security in mind. \u201d, while 24 contained imple- mentation details tailored to the user\u2019s speci\ufb01c use-case. In addition, 14 of these 69 mentions led to further inquiries about security by the users. For instance, in one conversation, Chat- GPT reminded the user about the importance of using prepared statements in order to avoid SQL injection, at which point, the user asked ChatGPT to rewrite this code with prepared statement. Given our observations, this empirical exploration of real-world developer conversations with ChatGPT highlights its potential to proactively inform developers about security vul- nerabilities and provide context-speci\ufb01c solutions for writing secure programs. Although infrequent, such proactive contributions promote security awareness, helping developers avoid insecure practices and prevent them from building upon vulnerable code. These \ufb01nd- ings highlighted the importance of systematically studying this behavior. Consequently, we 123101 Page 8 of 29 Empirical Software Engineering (2025) 30 :101 designed our main study to evaluate the extent and consistency of this behavior across three popular LLMs, under controlled scenarios. 3 Methodology Building upon our \ufb01ndings from the motivational study, we designed an experimental study to systematically evaluate the security awareness of three prominent LLMs, Claude 3, GPT-4, and Llama 3. Our goal is to determine whether LLMs can proactively detect vulnerable code in prompt inputs, and how consistently they issue security warnings and relevant vulnerability- related information to the users. To this end, we \ufb01rst collected Stack Over\ufb02ow (SO) questions containing vulnerable code snippets, which we then used as prompts for the LLMs. We qualitatively analyzed the LLM responses to address our research questions. By examining whether an LLM issues a warning about the security of the code in the SO question, we evaluate the LLM\u2019s security awareness. When an LLM not only answers the question but also highlights the security issue in the code, it demonstrates a high level of security awareness. Conversely, if the LLM addresses the question without mentioning security, it demonstrates a low level of security awareness. This premise forms the basis for our assessment of LLMs\u2019 security awareness in answering developers\u2019 questions. Figure 2 provides an overview of our approach, which we discuss in detail next. 3.1 Dataset Collection and Refactoring Data Collection We collect a dataset of SO questions with insecure code but without any explicit mention of security within the questions themselves. Figure 3shows an example of such a question, where the JavaScript code uses the eval function to dynamically create a variable based on user input, introducing a signi\ufb01cant risk of code injection or a cross-site scripting (XSS) attack. The developer posting the question seems unaware of the related security issue; however, one of the respondents points this out in a comment \u201cYou have a pretty nasty XSS vulnerability here\u201d . However, not all SO questions containing vulnerable code receive responses that point out the security implications of the code. In fact, earlier studies highlight the concerns with developers copy-pasting insecure code from SO posts (Fischer et al. 2017 ). Therefore, we aim to create two datasets for our study: (a) Mentions-Dataset: a set of questions that received responses (either answers or comments) mentioning and\/or addressing the security concerns Fig. 2 Overview of our study methodology 123Empirical Software Engineering (2025) 30 :101 Page 9 of 29 101 Fig. 3 Example of a SO Question Containing Vulnerable Code (Highlighted Red), and a User Response (Comment), Pointing Out the Vulnerability in the question\u2019s code (as shown in Fig. 3), (b) Transformed-Dataset: a set of questions that despite containing vulnerable code, do not receive such responses. The code in this dataset has been transformed (Later detailed in this section) to ensure that it is not easily recognizable for the LLMs. The Mentions-Dataset represents a best-case scenario where the vulnerability has already been highlighted by the community, and the code appears exactly as it was originally posted on SO. This allows us to observe how LLMs respond to questions that both exist in their training data and include explicit security warnings. In contrast, the Transformed-Dataset intentionally excludes any mention of security in the responses and applies code refactoring to reduce the chance of data leakage from the LLM\u2019s training data. By comparing these two datasets, we can examine LLM behavior across a spectrum of possible real-world situations: from code that has been explicitly marked as vulnerable to code that has not been associated with any vulnerabilities by the SO responses. This contrast helps us assess not only how well LLMs leverage familiar content (Mentions-Dataset) but also their ability to generalize to insecure code they may not have associated with vulnerabilities before (Transformed- Dataset). To collect these datasets, we used the SO data dump (Stack Exchange 2023 ) from March 2015 to March 2024, which contains over 10 million SO questions. First, we selected ques- tions that contain tags \u201cpython\u201d or \u201cjavascript\u201d to focus on these widely-used programming languages. According to the Stack Over\ufb02ow ( 2024 ) Developer Survey, both JavaScript and Python are listed in the top 3 most commonly used languages, making our results more representative of common real-world scenarios. We excluded questions that did not have 123101 Page 10 of 29 Empirical Software Engineering (2025) 30 :101 accepted answers, resulting in a total of 4.89 million questions. To ensure that there were no mentions of security in the natural language section of the questions, we performed keyword searches using the same keywords from Section 2. We discarded questions containing any of those keywords, leaving us with 4.77 million questions. Next, we discarded the questions that did not contain code snippets and analyzed the code in remaining questions using CodeQL (GitHub 2022 ), a static analysis tool often used to identify security vulnerabilities (Hamer et al. 2024 ;S i d d i qe ta l . 2024b ; Pearce et al. 2022 ). As CodeQL does not require access to the entire codebase, it effectively detected vulnerabilities directly within the code snippets. This step resulted in the detection of 4935 questions with insecure code snippets. We also made sure that each question in Mentions-Dataset has received at least one answer or comment containing a security-related keyword. Similarly, we ensured that Transformed- Dataset only consists of questions that have no security-related keywords in their answers or comments. At each step of the \ufb01ltering process, three authors manually checked the datasets to ensure their integrity. By inspecting all the instances in the subset of Mentions-Dataset, we identi\ufb01ed 150 unique SO questions i.e., 75 Python and 75 JavaScript. Next, we selected the same number of questions form the Transformed-Dataset, resulting in a total of 300 questions.  [Text truncated due to context length limits]\n","topics":""}
